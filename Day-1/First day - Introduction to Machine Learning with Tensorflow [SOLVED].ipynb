{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summer School of Data Science - Split '17\n",
    "\n",
    "# 1. Introduction to Machine Learning with TensorFlow\n",
    "\n",
    "This hands-on session serves as an introductory course for essential TensorFlow usage and basic machine learning with TensorFlow. This notebook is partly based on and follow the approach of chapter 6 of the book \"Deep Learning\" by Ian Goodfellow, Yoshua Bengio and Aaron Courville, available at: http://www.deeplearningbook.org/.\n",
    "\n",
    "Other useful tutorials exist in the form of Jupyter notebooks, some of which are:\n",
    "- https://github.com/udacity/deep-learning\n",
    "- https://github.com/DataScienceUB/DeepLearningfromScratch\n",
    "\n",
    "This notebook covers basic TensorFlow usage concepts, which are then applied to elementary machine learning models like linear and logistic regression, and finally a simple multilayer perceptron is built and trained using the established TensorFlow concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic TensorFlow concepts\n",
    "\n",
    "TensorFlow is an open source Python library which provides multiple APIs for buidling and evaluating computational graphs. These graphs can be used to represent any machine learning model, and TensorFlow provides methods for efficient optimization and evaluation of the models. The programmer's guide for TensorFlow can be found at https://www.tensorflow.org/programmers_guide/, and the full documentation is availale at https://www.tensorflow.org/api_docs/python/.\n",
    "\n",
    "The import statement for TensorFlow programs is: `import tensorflow as tf`. This provides access to all TensorFlow APIs, classes, methods and symbols. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor\n",
    "\n",
    "The basic concept behind TensorFlow is the tensor - an n-dimensional array of a base datatype. In TensorFlow it is represented by the `tf.Tensor` object which will produce a value when evaluated. A `tf.Tensor` object has a shape (which defines the structure of the elements) and a data type, shared by all the elements in the Tensor. The main types of tensors are:\n",
    "- Constant\n",
    "- Variable\n",
    "- Placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`tf.constant()`](https://www.tensorflow.org/api_docs/python/tf/constant) method creates a constant tensor, populated with values of a data type, specified by arguments `value`, `shape` (optional), [`dtype`](https://www.tensorflow.org/api_docs/python/tf/DType) (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# create a TensorFlow constant tensor\n",
    "t = tf.constant(5)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"const_tensor:0\", shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# create a TensorFlow constant of a specific data type and shape\n",
    "t = tf.constant(7,shape=[2,3],dtype=tf.float32,name=\"const_tensor\")\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, any Tensor is only evaluated within a [`Session`](https://www.tensorflow.org/api_docs/python/tf/Session), which is the environment in which all tensors and operations are executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.  7.  7.]\n",
      " [ 7.  7.  7.]]\n"
     ]
    }
   ],
   "source": [
    "# create a TensorFlow session and evaluate the created constant\n",
    "sess=tf.Session()\n",
    "print(sess.run(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other very common and useful methods for creating tensors of constant value are [`tf.zeros()`](https://www.tensorflow.org/api_docs/python/tf/zeros) and [`tf.ones()`](https://www.tensorflow.org/api_docs/python/tf/ones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.  0.]\n",
      "  [ 0.  0.]]\n",
      "\n",
      " [[ 0.  0.]\n",
      "  [ 0.  0.]]\n",
      "\n",
      " [[ 0.  0.]\n",
      "  [ 0.  0.]]]\n"
     ]
    }
   ],
   "source": [
    "# create a tensor of any shape populated with zeros and check within the session\n",
    "t = tf.zeros([3,2,2])\n",
    "print(sess.run(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# create a tensor of any shape populated with ones and check within the session\n",
    "t = tf.ones([2,4])\n",
    "print(sess.run(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors containing random values from various distribution can be created using a number of methods, with the most commonly used being [`tf.random_uniform()`](https://www.tensorflow.org/api_docs/python/tf/random_uniform) and [`tf.random_normal()`](https://www.tensorflow.org/api_docs/python/tf/random_normal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"random_uniform:0\", shape=(3, 4, 2), dtype=float32)\n",
      "[[[ 16.83423424  12.7082653 ]\n",
      "  [ 17.87173653  12.05112267]\n",
      "  [ 10.16734123  17.5232811 ]\n",
      "  [ 11.0253067   17.1642189 ]]\n",
      "\n",
      " [[ 16.27947998  19.20880699]\n",
      "  [ 13.15979576  17.601614  ]\n",
      "  [ 14.10945988  18.1970787 ]\n",
      "  [ 15.78850937  17.66868401]]\n",
      "\n",
      " [[ 13.94125748  15.40588951]\n",
      "  [ 15.38523674  17.21222687]\n",
      "  [ 17.30607224  11.45663071]\n",
      "  [ 18.22287369  16.17138481]]]\n"
     ]
    }
   ],
   "source": [
    "# create a random tensor containing values from a uniform distribution between 10 and 20\n",
    "t = tf.random_uniform([3,4,2],minval=10,maxval=20)\n",
    "print(t)\n",
    "print(sess.run(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple algebraic operations such as `+`,`-`,`/`,and `*` can be used with tensors in this form, or by calling [`tf.add()`](https://www.tensorflow.org/api_docs/python/tf/add), [`tf.subtract()`](https://www.tensorflow.org/api_docs/python/tf/subtract), [`tf.divide()`](https://www.tensorflow.org/api_docs/python/tf/divide), or [`tf.multiply()`](https://www.tensorflow.org/api_docs/python/tf/multiply). These are all element-wise, and defined for tensors of equal shapes and data-types. Tensors can be cast into a specific data type by calling [`tf.cast()`](https://www.tensorflow.org/api_docs/python/tf/cast)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.,  4.],\n",
       "       [ 4.,  4.],\n",
       "       [ 4.,  4.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add a scalar to a tensor\n",
    "a = tf.ones([3,2])\n",
    "sess.run(a+3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_1:0\", shape=(2, 3), dtype=float32)\n",
      "Tensor(\"random_normal:0\", shape=(2, 3), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 3.76239157,  5.0288825 ,  2.72073221],\n",
       "       [ 4.68344164,  3.4789114 ,  2.83210611]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subtract two tensors\n",
    "a = tf.constant(4.,shape=[2,3])\n",
    "print(a)\n",
    "b = tf.random_normal(shape=[2,3])\n",
    "print(b)\n",
    "sess.run(a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"truediv:0\", shape=(2, 3), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# divide two integer tensors\n",
    "a = tf.constant(4,shape=[2,3])\n",
    "b = tf.constant(7,shape=[2,3])\n",
    "print(a/b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other very useful operations include:\n",
    "- Absolute value (modulus) - [`tf.abs()`](https://www.tensorflow.org/api_docs/python/tf/abs)\n",
    "- Exponentiation with $e$ - [`tf.exp()`](https://www.tensorflow.org/api_docs/python/tf/exp)\n",
    "- Square and other powers - [`tf.square()`](https://www.tensorflow.org/api_docs/python/tf/square) and [`tf.pow()`](https://www.tensorflow.org/api_docs/python/tf/pow)\n",
    "- Matrix multiplication - [`tf.matmul()`](https://www.tensorflow.org/api_docs/python/tf/matmul)\n",
    "- Transpose - [`tf.transpose()`](https://www.tensorflow.org/api_docs/python/tf/transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.53116924  2.12819934]\n",
      " [ 4.25643158  3.49265838]\n",
      " [ 1.98903203  0.74123877]]\n",
      "[[  0.4170498    1.57493806   5.78904486]\n",
      " [  1.57493806   6.16942215  23.66168785]\n",
      " [  5.78904486  23.66168785  94.96226501]]\n"
     ]
    }
   ],
   "source": [
    "# try out varied mathematical operations with various tensors\n",
    "a = tf.exp(tf.random_normal(shape=[3,2]))\n",
    "print(sess.run(a))\n",
    "b = tf.matmul(a,tf.transpose(a))\n",
    "print(sess.run(b))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders and Variables\n",
    "\n",
    "Placeholders and Vairables are special kinds of tensors which are the essential building blocks of more complex data and computation streams. These are the most commonly used types of tensors in TensorFlow. \n",
    "\n",
    "A [Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) is a tensor which acts like a \"promise\" to provide a value at the evaluation of the computational graph. Placeholders are mostly used as input points in the computational graph where data will be provided. It will produce an error when evaluated, unless the value is fed to the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Exp_1:0\", dtype=float32)\n",
      "148.413\n",
      "[[  7.  10.]\n",
      " [ 15.  22.]]\n"
     ]
    }
   ],
   "source": [
    "# create a placeholder and feed it a value in a session\n",
    "a = tf.placeholder(dtype=tf.float32)\n",
    "b = tf.exp(a)\n",
    "print(b)\n",
    "print(sess.run(b,feed_dict={a:5}))\n",
    "\n",
    "# create two placeholders and a tensor implementing matrix multiplication \n",
    "x1 = tf.placeholder(dtype=tf.float32)\n",
    "x2 = tf.placeholder(dtype=tf.float32)\n",
    "y = tf.matmul(x1,x2)\n",
    "print(sess.run(y,{x2:[[1,2],[3,4]],x1:[[1,2],[3,4]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [Variable](https://www.tensorflow.org/api_docs/python/tf/Variable) is a tensor which allows the addition of trainable parameters to the computational graph. Constants are intialized when created, as opposed to variables, which need to be initialized within the session (and the initialization procedure must be defined). Variables can be \"manually\" assigned a new value using [`tf.assign`](https://www.tensorflow.org/api_docs/python/tf/assign), and their state is kept within the session object. This is mostly used for model training, during which variables are changed within the optimization process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=int32_ref>\n",
      "6\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# create a variable, initialize it, and assign a new value within a session\n",
    "sess = tf.Session()\n",
    "a = tf.Variable(5)\n",
    "print(a)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(a)\n",
    "sess.run(tf.assign(a,6))\n",
    "print(sess.run(a))\n",
    "\n",
    "sess.close()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(sess.run(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression in TensorFlow\n",
    "\n",
    "Linear regression is one of the simplest and most commonly used regression models. The multivariate linear regression can be written as: \n",
    "\n",
    "$$y = w^{T}x + b$$\n",
    "\n",
    "where $y \\in \\mathbb{R}$ is the output, $w \\in \\mathbb{R}^{p}$ is a column vector containing $p$ weights for $p$ features in $x \\in \\mathbb{R}^{p}$, and $b \\in \\mathbb{R}$ is the bias. The parameters contained in $w$ and $b$ are also called coefficients and are trained by using a gradient descent algorithm.\n",
    "\n",
    "\n",
    "### Exercise:\n",
    "\n",
    "Let us build a univariate linear regression model for a simple problem, using the previously introduced TensorFlow concepts:\n",
    "- The model input $x$ is a placeholder for data\n",
    "- The trainable model parameters $w$ and $b$ are defined as TensorFlow Variables\n",
    "- The model output $\\hat{y}$ is a Tensor\n",
    "- The obesrved output $y$ is also a placeholder, where data will be provided for training purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#define placeholders for data\n",
    "x = tf.placeholder(dtype=tf.float32,shape=[None])\n",
    "y = tf.placeholder(dtype=tf.float32,shape=[None])\n",
    "\n",
    "#define model parameters as variables\n",
    "w = tf.Variable(tf.random_normal(shape=()))\n",
    "b = tf.Variable(tf.random_normal([]))\n",
    "\n",
    "#create a tensor which calculates the model output\n",
    "y_model = w*x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a model built in TensorFlow, a loss function needs to be defined, most commonly as a [reduction](https://www.tensorflow.org/api_guides/python/math_ops#Reduction) operation. An [optimizer](https://www.tensorflow.org/api_guides/python/train#Optimizers) object needs to be defined, and the [`minimize()`](https://www.tensorflow.org/api_docs/python/tf/train/Optimizer#minimize) method called in order to update the variables defined within the model to minimize the selected loss function. When creating optimizer objects, choices about the learning rate have to be made - these, in combination with the number of training epochs, can greatly influence the model training process. With the approapriate learning rate, the optimization can quickly converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.24455598  0.54034035  0.67284074 -0.31131036 -1.54162209 -0.82222324\n",
      " -0.4128549  -1.02316814 -0.27822906  0.8220521 ] [  3.86083594  12.62098343  13.4825515    8.34129937   2.34952128\n",
      "   5.802013     7.89253573   4.99876662   8.6800274   14.12025928]\n",
      "w: 1.69321 , b: 1.86731 , loss: 55.0208\n",
      "w: 1.59488 , b: 3.25869 , loss: 37.6063\n",
      "w: 1.61127 , b: 4.36472 , loss: 26.5811\n",
      "w: 1.70484 , b: 5.25072 , loss: 19.3754\n",
      "w: 1.8483 , b: 5.96626 , loss: 14.503\n",
      "w: 2.022 , b: 6.54901 , loss: 11.0944\n",
      "w: 2.21188 , b: 7.02772 , loss: 8.63271\n",
      "w: 2.40806 , b: 7.42435 , loss: 6.80452\n",
      "w: 2.6037 , b: 7.75578 , loss: 5.41489\n",
      "w: 2.79419 , b: 8.035 , loss: 4.33898\n",
      "w: 2.97653 , b: 8.27208 , loss: 3.49415\n",
      "w: 3.14889 , b: 8.47488 , loss: 2.82378\n",
      "w: 3.3103 , b: 8.64952 , loss: 2.28779\n",
      "w: 3.46034 , b: 8.80085 , loss: 1.8569\n",
      "w: 3.59903 , b: 8.93271 , loss: 1.50917\n",
      "w: 3.72665 , b: 9.04818 , loss: 1.22777\n",
      "w: 3.84365 , b: 9.14975 , loss: 0.999633\n",
      "w: 3.95062 , b: 9.23942 , loss: 0.814426\n",
      "w: 4.04817 , b: 9.31886 , loss: 0.663935\n",
      "w: 4.13699 , b: 9.38943 , loss: 0.541574\n",
      "w: 4.21771 , b: 9.45227 , loss: 0.442044\n",
      "w: 4.29099 , b: 9.50836 , loss: 0.361058\n",
      "w: 4.35743 , b: 9.55851 , loss: 0.295148\n",
      "w: 4.41764 , b: 9.60341 , loss: 0.241499\n",
      "w: 4.47215 , b: 9.64366 , loss: 0.197827\n",
      "w: 4.52147 , b: 9.67978 , loss: 0.162273\n",
      "w: 4.56609 , b: 9.71224 , loss: 0.133327\n",
      "w: 4.60642 , b: 9.74141 , loss: 0.109761\n",
      "w: 4.64287 , b: 9.76765 , loss: 0.0905736\n",
      "w: 4.67581 , b: 9.79126 , loss: 0.0749513\n",
      "w: 4.70556 , b: 9.81252 , loss: 0.0622314\n",
      "w: 4.73243 , b: 9.83168 , loss: 0.0518748\n",
      "w: 4.75669 , b: 9.84893 , loss: 0.0434425\n",
      "w: 4.7786 , b: 9.86448 , loss: 0.0365766\n",
      "w: 4.79838 , b: 9.8785 , loss: 0.0309864\n",
      "w: 4.81624 , b: 9.89114 , loss: 0.0264345\n",
      "w: 4.83236 , b: 9.90253 , loss: 0.0227283\n",
      "w: 4.8469 , b: 9.91281 , loss: 0.0197107\n",
      "w: 4.86004 , b: 9.92208 , loss: 0.0172537\n",
      "w: 4.87189 , b: 9.93044 , loss: 0.0152531\n",
      "w: 4.88258 , b: 9.93798 , loss: 0.0136242\n",
      "w: 4.89223 , b: 9.94478 , loss: 0.0122979\n",
      "w: 4.90094 , b: 9.95092 , loss: 0.011218\n",
      "w: 4.90881 , b: 9.95645 , loss: 0.0103387\n",
      "w: 4.9159 , b: 9.96145 , loss: 0.0096228\n",
      "w: 4.9223 , b: 9.96595 , loss: 0.00903985\n",
      "w: 4.92808 , b: 9.97002 , loss: 0.00856524\n",
      "w: 4.93329 , b: 9.97369 , loss: 0.0081788\n",
      "w: 4.93799 , b: 9.977 , loss: 0.00786412\n",
      "w: 4.94224 , b: 9.97998 , loss: 0.00760787\n",
      "w: 4.94607 , b: 9.98268 , loss: 0.00739925\n",
      "w: 4.94952 , b: 9.98511 , loss: 0.00722942\n",
      "w: 4.95264 , b: 9.9873 , loss: 0.00709113\n",
      "w: 4.95546 , b: 9.98928 , loss: 0.0069785\n",
      "w: 4.958 , b: 9.99107 , loss: 0.00688684\n",
      "w: 4.96029 , b: 9.99268 , loss: 0.00681217\n",
      "w: 4.96236 , b: 9.99414 , loss: 0.00675137\n",
      "w: 4.96422 , b: 9.99545 , loss: 0.00670192\n",
      "w: 4.96591 , b: 9.99663 , loss: 0.0066616\n",
      "w: 4.96743 , b: 9.9977 , loss: 0.00662878\n",
      "w: 4.9688 , b: 9.99866 , loss: 0.00660207\n",
      "w: 4.97003 , b: 9.99953 , loss: 0.0065803\n",
      "w: 4.97115 , b: 10.0003 , loss: 0.00656261\n",
      "w: 4.97216 , b: 10.001 , loss: 0.00654817\n",
      "w: 4.97307 , b: 10.0017 , loss: 0.00653645\n",
      "w: 4.97389 , b: 10.0022 , loss: 0.00652686\n",
      "w: 4.97463 , b: 10.0028 , loss: 0.00651908\n",
      "w: 4.97529 , b: 10.0032 , loss: 0.00651275\n",
      "w: 4.9759 , b: 10.0037 , loss: 0.00650759\n",
      "w: 4.97644 , b: 10.004 , loss: 0.00650339\n",
      "w: 4.97693 , b: 10.0044 , loss: 0.00649998\n",
      "w: 4.97737 , b: 10.0047 , loss: 0.00649719\n",
      "w: 4.97777 , b: 10.005 , loss: 0.00649494\n",
      "w: 4.97813 , b: 10.0052 , loss: 0.00649309\n",
      "w: 4.97846 , b: 10.0055 , loss: 0.00649156\n",
      "w: 4.97875 , b: 10.0057 , loss: 0.00649033\n",
      "w: 4.97902 , b: 10.0059 , loss: 0.00648933\n",
      "w: 4.97925 , b: 10.006 , loss: 0.00648852\n",
      "w: 4.97947 , b: 10.0062 , loss: 0.00648788\n",
      "w: 4.97966 , b: 10.0063 , loss: 0.00648734\n",
      "w: 4.97984 , b: 10.0064 , loss: 0.00648688\n",
      "w: 4.98 , b: 10.0065 , loss: 0.00648651\n",
      "w: 4.98014 , b: 10.0066 , loss: 0.00648624\n",
      "w: 4.98027 , b: 10.0067 , loss: 0.006486\n",
      "w: 4.98039 , b: 10.0068 , loss: 0.00648583\n",
      "w: 4.98049 , b: 10.0069 , loss: 0.00648565\n",
      "w: 4.98059 , b: 10.007 , loss: 0.00648553\n",
      "w: 4.98067 , b: 10.007 , loss: 0.00648543\n",
      "w: 4.98075 , b: 10.0071 , loss: 0.00648536\n",
      "w: 4.98082 , b: 10.0071 , loss: 0.00648529\n",
      "w: 4.98088 , b: 10.0072 , loss: 0.00648523\n",
      "w: 4.98094 , b: 10.0072 , loss: 0.00648517\n",
      "w: 4.98099 , b: 10.0072 , loss: 0.00648514\n",
      "w: 4.98104 , b: 10.0073 , loss: 0.00648511\n",
      "w: 4.98108 , b: 10.0073 , loss: 0.00648508\n",
      "w: 4.98112 , b: 10.0073 , loss: 0.00648506\n",
      "w: 4.98115 , b: 10.0074 , loss: 0.00648506\n",
      "w: 4.98118 , b: 10.0074 , loss: 0.00648504\n",
      "w: 4.98121 , b: 10.0074 , loss: 0.00648502\n",
      "w: 4.98123 , b: 10.0074 , loss: 0.00648502\n"
     ]
    }
   ],
   "source": [
    "#define the loss function as the mean of all squared errors (MSE)\n",
    "loss = tf.reduce_mean(tf.square(y_model-y))\n",
    "\n",
    "#create a gradient descent optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
    "\n",
    "#create a train operation\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "#generate data to train the regression\n",
    "import numpy as np\n",
    "x_train = np.random.normal(size=10)\n",
    "y_train = 5*x_train + 10 + np.random.normal(size=10)/10\n",
    "print(x_train,y_train)\n",
    "\n",
    "#initialize variables, run 100 epochs of training algorithm\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(100):\n",
    "    sess.run(train,{y:y_train,x:x_train})\n",
    "    print('w:',sess.run(w),', b:',sess.run(b),', loss:',sess.run(loss,{y:y_train,x:x_train}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic regression is a very common and simple linear model for classification purposes, based on linear regression and the logistic function:\n",
    "\n",
    "$$y = \\frac{1}{1+e^{-(w^{T}x + b)}}$$\n",
    "\n",
    "Due to the nature of the logistic function, it produces output values in the range $[0,1]$, thus providing a probability for each class given in the output. Similar to linear regression, the variables defined within the logistic regression model are parameters trainable by various optimization algorithms.\n",
    "\n",
    "Let us build a logistic regression for the well-known XOR problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1210b0ac8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e2c6c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#generate XOR training data\n",
    "import numpy as np\n",
    "x_train = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y_train = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "#import matplotlib for visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#logical indices of data where the outputs are 1 and 0\n",
    "t = np.where(y_train==1)[0]\n",
    "f = np.where(y_train==0)[0]\n",
    "\n",
    "#scatter plot of the data\n",
    "plt.scatter(x_train[t,0],x_train[t,1],c='b',marker='x',s=70)\n",
    "plt.scatter(x_train[f,0],x_train[f,1],c='r',marker='o',s=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "- The model input $x$ is a placeholder for a data\n",
    "- The trainable model parameters $w$ and $b$ are defined as TensorFlow Variables\n",
    "- The model output $\\hat{y}$ is a Tensor\n",
    "- The obesrved output $y$ is also a placeholder, where output data will be provided in order to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [[-1.02229416]\n",
      " [ 0.75775838]] , b: [ 0.93021137] , loss: 0.312112\n",
      "w: [[-1.02493966]\n",
      " [ 0.73862422]] , b: [ 0.90882725] , loss: 0.309364\n",
      "w: [[-1.027354  ]\n",
      " [ 0.71971798]] , b: [ 0.88780278] , loss: 0.306703\n",
      "w: [[-1.02952194]\n",
      " [ 0.70107347]] , b: [ 0.86716753] , loss: 0.304133\n",
      "w: [[-1.03142953]\n",
      " [ 0.68272233]] , b: [ 0.84694862] , loss: 0.301661\n",
      "w: [[-1.03306508]\n",
      " [ 0.66469371]] , b: [ 0.82717061] , loss: 0.29929\n",
      "w: [[-1.03441858]\n",
      " [ 0.64701402]] , b: [ 0.80785525] , loss: 0.297026\n",
      "w: [[-1.03548217]\n",
      " [ 0.62970668]] , b: [ 0.78902119] , loss: 0.294868\n",
      "w: [[-1.03625   ]\n",
      " [ 0.61279207]] , b: [ 0.77068412] , loss: 0.292819\n",
      "w: [[-1.03671849]\n",
      " [ 0.59628731]] , b: [ 0.75285655] , loss: 0.290878\n",
      "w: [[-1.03688562]\n",
      " [ 0.58020645]] , b: [ 0.7355479] , loss: 0.289044\n",
      "w: [[-1.03675163]\n",
      " [ 0.56456041]] , b: [ 0.71876472] , loss: 0.287315\n",
      "w: [[-1.03631818]\n",
      " [ 0.54935712]] , b: [ 0.70251071] , loss: 0.285688\n",
      "w: [[-1.03558874]\n",
      " [ 0.53460163]] , b: [ 0.68678683] , loss: 0.284161\n",
      "w: [[-1.03456795]\n",
      " [ 0.52029645]] , b: [ 0.67159158] , loss: 0.282729\n",
      "w: [[-1.0332619 ]\n",
      " [ 0.50644159]] , b: [ 0.65692121] , loss: 0.281389\n",
      "w: [[-1.0316776]\n",
      " [ 0.4930349]] , b: [ 0.64276999] , loss: 0.280135\n",
      "w: [[-1.02982306]\n",
      " [ 0.4800722 ]] , b: [ 0.62913042] , loss: 0.278963\n",
      "w: [[-1.02770698]\n",
      " [ 0.46754766]] , b: [ 0.61599338] , loss: 0.277869\n",
      "w: [[-1.02533853]\n",
      " [ 0.45545387]] , b: [ 0.60334849] , loss: 0.276847\n",
      "w: [[-1.02272749]\n",
      " [ 0.44378221]] , b: [ 0.5911842] , loss: 0.275894\n",
      "w: [[-1.01988387]\n",
      " [ 0.43252292]] , b: [ 0.57948804] , loss: 0.275004\n",
      "w: [[-1.01681793]\n",
      " [ 0.4216654 ]] , b: [ 0.56824684] , loss: 0.274172\n",
      "w: [[-1.01353991]\n",
      " [ 0.41119835]] , b: [ 0.55744684] , loss: 0.273396\n",
      "w: [[-1.01006007]\n",
      " [ 0.4011099 ]] , b: [ 0.5470739] , loss: 0.272669\n",
      "w: [[-1.00638878]\n",
      " [ 0.39138788]] , b: [ 0.53711361] , loss: 0.27199\n",
      "w: [[-1.00253618]\n",
      " [ 0.38201976]] , b: [ 0.52755135] , loss: 0.271353\n",
      "w: [[-0.99851215]\n",
      " [ 0.37299293]] , b: [ 0.5183726] , loss: 0.270757\n",
      "w: [[-0.99432647]\n",
      " [ 0.36429468]] , b: [ 0.50956267] , loss: 0.270196\n",
      "w: [[-0.98998868]\n",
      " [ 0.35591239]] , b: [ 0.50110716] , loss: 0.269669\n",
      "w: [[-0.98550802]\n",
      " [ 0.34783348]] , b: [ 0.49299178] , loss: 0.269172\n",
      "w: [[-0.98089343]\n",
      " [ 0.34004563]] , b: [ 0.48520249] , loss: 0.268703\n",
      "w: [[-0.97615355]\n",
      " [ 0.3325367 ]] , b: [ 0.4777256] , loss: 0.26826\n",
      "w: [[-0.97129667]\n",
      " [ 0.32529482]] , b: [ 0.47054768] , loss: 0.267841\n",
      "w: [[-0.96633077]\n",
      " [ 0.31830841]] , b: [ 0.46365568] , loss: 0.267443\n",
      "w: [[-0.9612636 ]\n",
      " [ 0.31156623]] , b: [ 0.45703691] , loss: 0.267064\n",
      "w: [[-0.95610243]\n",
      " [ 0.30505738]] , b: [ 0.45067912] , loss: 0.266704\n",
      "w: [[-0.95085442]\n",
      " [ 0.29877129]] , b: [ 0.44457039] , loss: 0.26636\n",
      "w: [[-0.94552624]\n",
      " [ 0.29269782]] , b: [ 0.43869928] , loss: 0.266031\n",
      "w: [[-0.94012433]\n",
      " [ 0.28682715]] , b: [ 0.43305472] , loss: 0.265716\n",
      "w: [[-0.93465483]\n",
      " [ 0.28114986]] , b: [ 0.42762607] , loss: 0.265413\n",
      "w: [[-0.92912358]\n",
      " [ 0.27565691]] , b: [ 0.42240313] , loss: 0.265123\n",
      "w: [[-0.92353618]\n",
      " [ 0.27033964]] , b: [ 0.41737604] , loss: 0.264843\n",
      "w: [[-0.91789788]\n",
      " [ 0.26518974]] , b: [ 0.41253543] , loss: 0.264573\n",
      "w: [[-0.9122138 ]\n",
      " [ 0.26019931]] , b: [ 0.40787223] , loss: 0.264312\n",
      "w: [[-0.90648872]\n",
      " [ 0.25536075]] , b: [ 0.40337783] , loss: 0.264059\n",
      "w: [[-0.90072715]\n",
      " [ 0.25066683]] , b: [ 0.39904398] , loss: 0.263814\n",
      "w: [[-0.89493346]\n",
      " [ 0.24611066]] , b: [ 0.39486274] , loss: 0.263577\n",
      "w: [[-0.88911176]\n",
      " [ 0.24168567]] , b: [ 0.39082664] , loss: 0.263346\n",
      "w: [[-0.88326591]\n",
      " [ 0.23738563]] , b: [ 0.38692844] , loss: 0.263121\n",
      "w: [[-0.87739962]\n",
      " [ 0.23320459]] , b: [ 0.38316128] , loss: 0.262902\n",
      "w: [[-0.87151641]\n",
      " [ 0.2291369 ]] , b: [ 0.3795186] , loss: 0.262689\n",
      "w: [[-0.86561954]\n",
      " [ 0.22517718]] , b: [ 0.37599418] , loss: 0.26248\n",
      "w: [[-0.85971224]\n",
      " [ 0.22132036]] , b: [ 0.37258205] , loss: 0.262277\n",
      "w: [[-0.85379744]\n",
      " [ 0.2175616 ]] , b: [ 0.36927658] , loss: 0.262078\n",
      "w: [[-0.84787792]\n",
      " [ 0.21389632]] , b: [ 0.36607239] , loss: 0.261883\n",
      "w: [[-0.84195638]\n",
      " [ 0.21032016]] , b: [ 0.36296433] , loss: 0.261692\n",
      "w: [[-0.83603537]\n",
      " [ 0.20682901]] , b: [ 0.35994756] , loss: 0.261505\n",
      "w: [[-0.83011723]\n",
      " [ 0.20341899]] , b: [ 0.35701743] , loss: 0.261322\n",
      "w: [[-0.82420421]\n",
      " [ 0.20008639]] , b: [ 0.35416955] , loss: 0.261142\n",
      "w: [[-0.8182984 ]\n",
      " [ 0.19682771]] , b: [ 0.35139972] , loss: 0.260966\n",
      "w: [[-0.81240183]\n",
      " [ 0.19363967]] , b: [ 0.34870398] , loss: 0.260793\n",
      "w: [[-0.80651641]\n",
      " [ 0.19051912]] , b: [ 0.34607857] , loss: 0.260623\n",
      "w: [[-0.80064392]\n",
      " [ 0.18746312]] , b: [ 0.34351993] , loss: 0.260455\n",
      "w: [[-0.79478598]\n",
      " [ 0.18446888]] , b: [ 0.34102464] , loss: 0.260291\n",
      "w: [[-0.78894418]\n",
      " [ 0.18153375]] , b: [ 0.33858949] , loss: 0.26013\n",
      "w: [[-0.78312004]\n",
      " [ 0.17865524]] , b: [ 0.33621144] , loss: 0.259971\n",
      "w: [[-0.7773149 ]\n",
      " [ 0.17583099]] , b: [ 0.33388758] , loss: 0.259814\n",
      "w: [[-0.77153009]\n",
      " [ 0.17305878]] , b: [ 0.33161515] , loss: 0.259661\n",
      "w: [[-0.76576686]\n",
      " [ 0.17033651]] , b: [ 0.32939157] , loss: 0.259509\n",
      "w: [[-0.76002634]\n",
      " [ 0.1676622 ]] , b: [ 0.32721439] , loss: 0.25936\n",
      "w: [[-0.75430959]\n",
      " [ 0.16503398]] , b: [ 0.32508126] , loss: 0.259214\n",
      "w: [[-0.74861765]\n",
      " [ 0.16245009]] , b: [ 0.32298997] , loss: 0.25907\n",
      "w: [[-0.74295145]\n",
      " [ 0.15990885]] , b: [ 0.32093844] , loss: 0.258928\n",
      "w: [[-0.73731184]\n",
      " [ 0.15740868]] , b: [ 0.3189247] , loss: 0.258788\n",
      "w: [[-0.73169965]\n",
      " [ 0.15494813]] , b: [ 0.31694683] , loss: 0.25865\n",
      "w: [[-0.72611564]\n",
      " [ 0.15252578]] , b: [ 0.31500313] , loss: 0.258515\n",
      "w: [[-0.72056055]\n",
      " [ 0.15014032]] , b: [ 0.31309187] , loss: 0.258381\n",
      "w: [[-0.71503496]\n",
      " [ 0.14779049]] , b: [ 0.3112115] , loss: 0.25825\n",
      "w: [[-0.70953953]\n",
      " [ 0.14547512]] , b: [ 0.30936047] , loss: 0.25812\n",
      "w: [[-0.7040748 ]\n",
      " [ 0.14319311]] , b: [ 0.30753741] , loss: 0.257993\n",
      "w: [[-0.6986413 ]\n",
      " [ 0.14094342]] , b: [ 0.30574095] , loss: 0.257867\n",
      "w: [[-0.69323951]\n",
      " [ 0.13872506]] , b: [ 0.30396983] , loss: 0.257743\n",
      "w: [[-0.68786985]\n",
      " [ 0.13653709]] , b: [ 0.30222288] , loss: 0.257622\n",
      "w: [[-0.68253273]\n",
      " [ 0.13437864]] , b: [ 0.30049893] , loss: 0.257502\n",
      "w: [[-0.67722845]\n",
      " [ 0.13224889]] , b: [ 0.29879695] , loss: 0.257384\n",
      "w: [[-0.67195743]\n",
      " [ 0.13014705]] , b: [ 0.29711592] , loss: 0.257267\n",
      "w: [[-0.66671991]\n",
      " [ 0.1280724 ]] , b: [ 0.29545489] , loss: 0.257153\n",
      "w: [[-0.66151613]\n",
      " [ 0.12602422]] , b: [ 0.29381296] , loss: 0.25704\n",
      "w: [[-0.65634638]\n",
      " [ 0.12400185]] , b: [ 0.2921893] , loss: 0.256929\n",
      "w: [[-0.65121084]\n",
      " [ 0.1220047 ]] , b: [ 0.2905831] , loss: 0.256819\n",
      "w: [[-0.6461097 ]\n",
      " [ 0.12003215]] , b: [ 0.2889936] , loss: 0.256712\n",
      "w: [[-0.64104307]\n",
      " [ 0.11808366]] , b: [ 0.28742009] , loss: 0.256606\n",
      "w: [[-0.63601112]\n",
      " [ 0.11615871]] , b: [ 0.28586191] , loss: 0.256501\n",
      "w: [[-0.63101399]\n",
      " [ 0.11425679]] , b: [ 0.28431842] , loss: 0.256398\n",
      "w: [[-0.62605172]\n",
      " [ 0.11237744]] , b: [ 0.28278905] , loss: 0.256297\n",
      "w: [[-0.62112439]\n",
      " [ 0.11052021]] , b: [ 0.28127322] , loss: 0.256197\n",
      "w: [[-0.61623204]\n",
      " [ 0.10868468]] , b: [ 0.2797704] , loss: 0.256099\n",
      "w: [[-0.61137474]\n",
      " [ 0.10687045]] , b: [ 0.27828011] , loss: 0.256003\n",
      "w: [[-0.60655248]\n",
      " [ 0.10507713]] , b: [ 0.27680185] , loss: 0.255908\n",
      "w: [[-0.60176528]\n",
      " [ 0.10330438]] , b: [ 0.27533522] , loss: 0.255814\n",
      "w: [[-0.59701312]\n",
      " [ 0.10155183]] , b: [ 0.27387977] , loss: 0.255722\n",
      "w: [[-0.59229594]\n",
      " [ 0.09981917]] , b: [ 0.27243513] , loss: 0.255631\n",
      "w: [[-0.58761376]\n",
      " [ 0.09810608]] , b: [ 0.27100089] , loss: 0.255541\n",
      "w: [[-0.58296651]\n",
      " [ 0.09641226]] , b: [ 0.26957676] , loss: 0.255453\n",
      "w: [[-0.57835412]\n",
      " [ 0.09473746]] , b: [ 0.2681624] , loss: 0.255367\n",
      "w: [[-0.57377648]\n",
      " [ 0.09308136]] , b: [ 0.26675749] , loss: 0.255282\n",
      "w: [[-0.56923354]\n",
      " [ 0.09144375]] , b: [ 0.26536176] , loss: 0.255198\n",
      "w: [[-0.56472522]\n",
      " [ 0.08982435]] , b: [ 0.26397491] , loss: 0.255115\n",
      "w: [[-0.56025136]\n",
      " [ 0.08822294]] , b: [ 0.2625967] , loss: 0.255034\n",
      "w: [[-0.55581188]\n",
      " [ 0.08663929]] , b: [ 0.26122689] , loss: 0.254954\n",
      "w: [[-0.55140668]\n",
      " [ 0.08507317]] , b: [ 0.25986528] , loss: 0.254875\n",
      "w: [[-0.54703557]\n",
      " [ 0.08352439]] , b: [ 0.25851163] , loss: 0.254798\n",
      "w: [[-0.54269844]\n",
      " [ 0.08199275]] , b: [ 0.25716576] , loss: 0.254721\n",
      "w: [[-0.53839517]\n",
      " [ 0.08047803]] , b: [ 0.25582749] , loss: 0.254646\n",
      "w: [[-0.53412563]\n",
      " [ 0.07898007]] , b: [ 0.2544966] , loss: 0.254573\n",
      "w: [[-0.52988958]\n",
      " [ 0.07749867]] , b: [ 0.25317299] , loss: 0.2545\n",
      "w: [[-0.52568692]\n",
      " [ 0.07603367]] , b: [ 0.25185645] , loss: 0.254428\n",
      "w: [[-0.52151752]\n",
      " [ 0.07458489]] , b: [ 0.25054687] , loss: 0.254358\n",
      "w: [[-0.51738113]\n",
      " [ 0.07315217]] , b: [ 0.24924409] , loss: 0.254289\n",
      "w: [[-0.51327759]\n",
      " [ 0.07173534]] , b: [ 0.24794801] , loss: 0.254221\n",
      "w: [[-0.50920677]\n",
      " [ 0.07033426]] , b: [ 0.24665847] , loss: 0.254154\n",
      "w: [[-0.50516844]\n",
      " [ 0.06894878]] , b: [ 0.24537539] , loss: 0.254088\n",
      "w: [[-0.50116241]\n",
      " [ 0.06757873]] , b: [ 0.24409866] , loss: 0.254023\n",
      "w: [[-0.49718854]\n",
      " [ 0.06622399]] , b: [ 0.24282818] , loss: 0.253959\n",
      "w: [[-0.49324661]\n",
      " [ 0.06488441]] , b: [ 0.24156384] , loss: 0.253896\n",
      "w: [[-0.48933643]\n",
      " [ 0.06355985]] , b: [ 0.24030557] , loss: 0.253834\n",
      "w: [[-0.48545781]\n",
      " [ 0.06225019]] , b: [ 0.23905329] , loss: 0.253773\n",
      "w: [[-0.48161054]\n",
      " [ 0.06095528]] , b: [ 0.23780692] , loss: 0.253713\n",
      "w: [[-0.47779441]\n",
      " [ 0.05967499]] , b: [ 0.23656638] , loss: 0.253655\n",
      "w: [[-0.47400922]\n",
      " [ 0.0584092 ]] , b: [ 0.23533161] , loss: 0.253597\n",
      "w: [[-0.47025478]\n",
      " [ 0.05715779]] , b: [ 0.23410255] , loss: 0.25354\n",
      "w: [[-0.46653086]\n",
      " [ 0.05592063]] , b: [ 0.23287913] , loss: 0.253483\n",
      "w: [[-0.46283728]\n",
      " [ 0.0546976 ]] , b: [ 0.23166129] , loss: 0.253428\n",
      "w: [[-0.45917383]\n",
      " [ 0.05348857]] , b: [ 0.23044899] , loss: 0.253374\n",
      "w: [[-0.45554027]\n",
      " [ 0.05229344]] , b: [ 0.22924218] , loss: 0.253321\n",
      "w: [[-0.45193642]\n",
      " [ 0.05111208]] , b: [ 0.2280408] , loss: 0.253268\n",
      "w: [[-0.44836205]\n",
      " [ 0.04994437]] , b: [ 0.22684482] , loss: 0.253216\n",
      "w: [[-0.44481698]\n",
      " [ 0.04879022]] , b: [ 0.2256542] , loss: 0.253165\n",
      "w: [[-0.44130096]\n",
      " [ 0.0476495 ]] , b: [ 0.2244689] , loss: 0.253115\n",
      "w: [[-0.43781379]\n",
      " [ 0.04652209]] , b: [ 0.22328888] , loss: 0.253066\n",
      "w: [[-0.43435526]\n",
      " [ 0.04540789]] , b: [ 0.22211409] , loss: 0.253018\n",
      "w: [[-0.43092516]\n",
      " [ 0.04430679]] , b: [ 0.22094451] , loss: 0.25297\n",
      "w: [[-0.42752329]\n",
      " [ 0.04321868]] , b: [ 0.21978012] , loss: 0.252923\n",
      "w: [[-0.42414942]\n",
      " [ 0.04214346]] , b: [ 0.21862088] , loss: 0.252877\n",
      "w: [[-0.42080334]\n",
      " [ 0.041081  ]] , b: [ 0.21746676] , loss: 0.252832\n",
      "w: [[-0.41748485]\n",
      " [ 0.04003122]] , b: [ 0.21631774] , loss: 0.252787\n",
      "w: [[-0.41419372]\n",
      " [ 0.03899401]] , b: [ 0.21517381] , loss: 0.252743\n",
      "w: [[-0.41092974]\n",
      " [ 0.03796924]] , b: [ 0.21403493] , loss: 0.2527\n",
      "w: [[-0.4076927 ]\n",
      " [ 0.03695684]] , b: [ 0.21290107] , loss: 0.252658\n",
      "w: [[-0.40448242]\n",
      " [ 0.03595668]] , b: [ 0.21177222] , loss: 0.252616\n",
      "w: [[-0.40129867]\n",
      " [ 0.03496866]] , b: [ 0.21064836] , loss: 0.252575\n",
      "w: [[-0.39814124]\n",
      " [ 0.03399269]] , b: [ 0.20952947] , loss: 0.252535\n",
      "w: [[-0.39500991]\n",
      " [ 0.03302865]] , b: [ 0.20841554] , loss: 0.252495\n",
      "w: [[-0.39190447]\n",
      " [ 0.03207646]] , b: [ 0.20730653] , loss: 0.252456\n",
      "w: [[-0.38882473]\n",
      " [ 0.031136  ]] , b: [ 0.20620245] , loss: 0.252417\n",
      "w: [[-0.38577047]\n",
      " [ 0.03020719]] , b: [ 0.20510326] , loss: 0.252379\n",
      "w: [[-0.38274151]\n",
      " [ 0.0292899 ]] , b: [ 0.20400895] , loss: 0.252342\n",
      "w: [[-0.37973762]\n",
      " [ 0.02838406]] , b: [ 0.20291951] , loss: 0.252306\n",
      "w: [[-0.37675861]\n",
      " [ 0.02748955]] , b: [ 0.20183493] , loss: 0.25227\n",
      "w: [[-0.37380427]\n",
      " [ 0.02660629]] , b: [ 0.20075519] , loss: 0.252234\n",
      "w: [[-0.3708744 ]\n",
      " [ 0.02573416]] , b: [ 0.19968028] , loss: 0.252199\n",
      "w: [[-0.3679688 ]\n",
      " [ 0.02487307]] , b: [ 0.19861019] , loss: 0.252165\n",
      "w: [[-0.36508727]\n",
      " [ 0.02402294]] , b: [ 0.19754489] , loss: 0.252131\n",
      "w: [[-0.36222962]\n",
      " [ 0.02318365]] , b: [ 0.19648437] , loss: 0.252098\n",
      "w: [[-0.35939565]\n",
      " [ 0.02235511]] , b: [ 0.19542864] , loss: 0.252065\n",
      "w: [[-0.35658514]\n",
      " [ 0.02153722]] , b: [ 0.19437768] , loss: 0.252033\n",
      "w: [[-0.35379794]\n",
      " [ 0.0207299 ]] , b: [ 0.19333147] , loss: 0.252002\n",
      "w: [[-0.35103384]\n",
      " [ 0.01993304]] , b: [ 0.19228999] , loss: 0.251971\n",
      "w: [[-0.34829262]\n",
      " [ 0.01914655]] , b: [ 0.19125324] , loss: 0.25194\n",
      "w: [[-0.34557411]\n",
      " [ 0.01837033]] , b: [ 0.19022122] , loss: 0.25191\n",
      "w: [[-0.3428781 ]\n",
      " [ 0.01760429]] , b: [ 0.1891939] , loss: 0.25188\n",
      "w: [[-0.34020445]\n",
      " [ 0.01684834]] , b: [ 0.18817128] , loss: 0.251851\n",
      "w: [[-0.33755293]\n",
      " [ 0.01610238]] , b: [ 0.18715334] , loss: 0.251823\n",
      "w: [[-0.33492336]\n",
      " [ 0.01536633]] , b: [ 0.18614008] , loss: 0.251795\n",
      "w: [[-0.33231556]\n",
      " [ 0.01464008]] , b: [ 0.18513148] , loss: 0.251767\n",
      "w: [[-0.32972935]\n",
      " [ 0.01392356]] , b: [ 0.18412752] , loss: 0.25174\n",
      "w: [[-0.32716453]\n",
      " [ 0.01321666]] , b: [ 0.18312822] , loss: 0.251713\n",
      "w: [[-0.32462093]\n",
      " [ 0.01251929]] , b: [ 0.18213354] , loss: 0.251686\n",
      "w: [[-0.3220984 ]\n",
      " [ 0.01183136]] , b: [ 0.18114346] , loss: 0.25166\n",
      "w: [[-0.31959674]\n",
      " [ 0.01115278]] , b: [ 0.180158] , loss: 0.251635\n",
      "w: [[-0.31711575]\n",
      " [ 0.01048348]] , b: [ 0.17917714] , loss: 0.25161\n",
      "w: [[-0.31465527]\n",
      " [ 0.00982334]] , b: [ 0.17820086] , loss: 0.251585\n",
      "w: [[-0.31221512]\n",
      " [ 0.0091723 ]] , b: [ 0.17722915] , loss: 0.251561\n",
      "w: [[-0.30979514]\n",
      " [ 0.00853025]] , b: [ 0.17626201] , loss: 0.251537\n",
      "w: [[-0.30739513]\n",
      " [ 0.00789711]] , b: [ 0.17529942] , loss: 0.251513\n",
      "w: [[-0.30501494]\n",
      " [ 0.0072728 ]] , b: [ 0.17434138] , loss: 0.25149\n",
      "w: [[-0.30265442]\n",
      " [ 0.00665721]] , b: [ 0.17338786] , loss: 0.251467\n",
      "w: [[-0.30031335]\n",
      " [ 0.00605028]] , b: [ 0.17243886] , loss: 0.251445\n",
      "w: [[-0.2979916 ]\n",
      " [ 0.00545191]] , b: [ 0.17149436] , loss: 0.251423\n",
      "w: [[-0.29568902]\n",
      " [ 0.00486202]] , b: [ 0.17055437] , loss: 0.251401\n",
      "w: [[-0.29340541]\n",
      " [ 0.00428053]] , b: [ 0.16961886] , loss: 0.25138\n",
      "w: [[-0.29114065]\n",
      " [ 0.00370735]] , b: [ 0.16868782] , loss: 0.251359\n",
      "w: [[-0.2888945 ]\n",
      " [ 0.00314239]] , b: [ 0.16776125] , loss: 0.251338\n",
      "w: [[-0.28666687]\n",
      " [ 0.00258558]] , b: [ 0.16683912] , loss: 0.251318\n",
      "w: [[-0.28445759]\n",
      " [ 0.00203682]] , b: [ 0.16592143] , loss: 0.251298\n",
      "w: [[-0.2822665 ]\n",
      " [ 0.00149604]] , b: [ 0.16500817] , loss: 0.251278\n",
      "w: [[-0.28009343]\n",
      " [ 0.00096316]] , b: [ 0.16409932] , loss: 0.251259\n",
      "w: [[-0.27793822]\n",
      " [ 0.0004381 ]] , b: [ 0.16319488] , loss: 0.25124\n",
      "w: [[ -2.75800705e-01]\n",
      " [ -7.92315695e-05]] , b: [ 0.16229482] , loss: 0.251221\n",
      "w: [[-0.27368078]\n",
      " [-0.0005889 ]] , b: [ 0.16139914] , loss: 0.251203\n",
      "w: [[-0.27157825]\n",
      " [-0.00109101]] , b: [ 0.16050783] , loss: 0.251185\n",
      "w: [[-0.26949298]\n",
      " [-0.00158561]] , b: [ 0.15962087] , loss: 0.251167\n",
      "w: [[-0.26742482]\n",
      " [-0.00207279]] , b: [ 0.15873826] , loss: 0.251149\n",
      "w: [[-0.26537362]\n",
      " [-0.00255264]] , b: [ 0.15785997] , loss: 0.251132\n",
      "w: [[-0.26333925]\n",
      " [-0.00302522]] , b: [ 0.15698598] , loss: 0.251115\n",
      "w: [[-0.26132154]\n",
      " [-0.00349062]] , b: [ 0.15611631] , loss: 0.251098\n",
      "w: [[-0.25932038]\n",
      " [-0.0039489 ]] , b: [ 0.15525092] , loss: 0.251082\n",
      "w: [[-0.2573356 ]\n",
      " [-0.00440016]] , b: [ 0.15438981] , loss: 0.251065\n",
      "w: [[-0.25536704]\n",
      " [-0.00484445]] , b: [ 0.15353297] , loss: 0.251049\n",
      "w: [[-0.2534146 ]\n",
      " [-0.00528186]] , b: [ 0.15268038] , loss: 0.251034\n",
      "w: [[-0.25147811]\n",
      " [-0.00571247]] , b: [ 0.15183203] , loss: 0.251018\n",
      "w: [[-0.24955745]\n",
      " [-0.00613633]] , b: [ 0.15098789] , loss: 0.251003\n",
      "w: [[-0.24765249]\n",
      " [-0.00655354]] , b: [ 0.15014797] , loss: 0.250988\n",
      "w: [[-0.24576306]\n",
      " [-0.00696415]] , b: [ 0.14931226] , loss: 0.250973\n",
      "w: [[-0.24388906]\n",
      " [-0.00736824]] , b: [ 0.14848073] , loss: 0.250959\n",
      "w: [[-0.24203035]\n",
      " [-0.00776588]] , b: [ 0.14765337] , loss: 0.250945\n",
      "w: [[-0.2401868 ]\n",
      " [-0.00815715]] , b: [ 0.14683016] , loss: 0.25093\n",
      "w: [[-0.23835826]\n",
      " [-0.00854211]] , b: [ 0.14601108] , loss: 0.250917\n",
      "w: [[-0.23654461]\n",
      " [-0.00892084]] , b: [ 0.14519615] , loss: 0.250903\n",
      "w: [[-0.23474573]\n",
      " [-0.00929339]] , b: [ 0.14438534] , loss: 0.25089\n",
      "w: [[-0.23296148]\n",
      " [-0.00965985]] , b: [ 0.14357862] , loss: 0.250877\n",
      "w: [[-0.23119174]\n",
      " [-0.01002027]] , b: [ 0.142776] , loss: 0.250864\n",
      "w: [[-0.22943638]\n",
      " [-0.01037474]] , b: [ 0.14197744] , loss: 0.250851\n",
      "w: [[-0.22769529]\n",
      " [-0.0107233 ]] , b: [ 0.14118296] , loss: 0.250838\n",
      "w: [[-0.22596833]\n",
      " [-0.01106604]] , b: [ 0.14039251] , loss: 0.250826\n",
      "w: [[-0.22425538]\n",
      " [-0.01140301]] , b: [ 0.1396061] , loss: 0.250814\n",
      "w: [[-0.22255632]\n",
      " [-0.01173428]] , b: [ 0.1388237] , loss: 0.250802\n",
      "w: [[-0.22087105]\n",
      " [-0.01205991]] , b: [ 0.13804531] , loss: 0.25079\n",
      "w: [[-0.21919942]\n",
      " [-0.01237997]] , b: [ 0.13727091] , loss: 0.250778\n",
      "w: [[-0.21754132]\n",
      " [-0.01269452]] , b: [ 0.13650048] , loss: 0.250767\n",
      "w: [[-0.21589665]\n",
      " [-0.01300364]] , b: [ 0.13573401] , loss: 0.250756\n",
      "w: [[-0.21426529]\n",
      " [-0.01330737]] , b: [ 0.13497147] , loss: 0.250745\n",
      "w: [[-0.21264711]\n",
      " [-0.01360578]] , b: [ 0.13421287] , loss: 0.250734\n",
      "w: [[-0.211042  ]\n",
      " [-0.01389892]] , b: [ 0.1334582] , loss: 0.250723\n",
      "w: [[-0.20944986]\n",
      " [-0.01418687]] , b: [ 0.13270742] , loss: 0.250712\n",
      "w: [[-0.20787057]\n",
      " [-0.01446969]] , b: [ 0.13196053] , loss: 0.250702\n",
      "w: [[-0.20630403]\n",
      " [-0.01474743]] , b: [ 0.13121751] , loss: 0.250692\n",
      "w: [[-0.20475011]\n",
      " [-0.01502015]] , b: [ 0.13047835] , loss: 0.250682\n",
      "w: [[-0.20320871]\n",
      " [-0.0152879 ]] , b: [ 0.12974304] , loss: 0.250672\n",
      "w: [[-0.20167972]\n",
      " [-0.01555076]] , b: [ 0.12901156] , loss: 0.250662\n",
      "w: [[-0.20016305]\n",
      " [-0.01580878]] , b: [ 0.12828387] , loss: 0.250652\n",
      "w: [[-0.19865857]\n",
      " [-0.016062  ]] , b: [ 0.12756] , loss: 0.250643\n",
      "w: [[-0.19716619]\n",
      " [-0.0163105 ]] , b: [ 0.12683992] , loss: 0.250634\n",
      "w: [[-0.1956858 ]\n",
      " [-0.01655432]] , b: [ 0.12612359] , loss: 0.250624\n",
      "w: [[-0.19421731]\n",
      " [-0.01679353]] , b: [ 0.12541103] , loss: 0.250615\n",
      "w: [[-0.1927606 ]\n",
      " [-0.01702817]] , b: [ 0.12470221] , loss: 0.250606\n",
      "w: [[-0.19131558]\n",
      " [-0.0172583 ]] , b: [ 0.12399711] , loss: 0.250598\n",
      "w: [[-0.18988213]\n",
      " [-0.01748398]] , b: [ 0.12329571] , loss: 0.250589\n",
      "w: [[-0.18846017]\n",
      " [-0.01770525]] , b: [ 0.12259801] , loss: 0.250581\n",
      "w: [[-0.1870496 ]\n",
      " [-0.01792218]] , b: [ 0.12190399] , loss: 0.250572\n",
      "w: [[-0.18565032]\n",
      " [-0.01813481]] , b: [ 0.12121364] , loss: 0.250564\n",
      "w: [[-0.18426223]\n",
      " [-0.01834321]] , b: [ 0.12052692] , loss: 0.250556\n",
      "w: [[-0.18288524]\n",
      " [-0.01854741]] , b: [ 0.11984384] , loss: 0.250548\n",
      "w: [[-0.18151926]\n",
      " [-0.01874747]] , b: [ 0.11916438] , loss: 0.25054\n",
      "w: [[-0.18016419]\n",
      " [-0.01894344]] , b: [ 0.11848853] , loss: 0.250532\n",
      "w: [[-0.17881992]\n",
      " [-0.01913537]] , b: [ 0.11781627] , loss: 0.250525\n",
      "w: [[-0.17748639]\n",
      " [-0.01932332]] , b: [ 0.11714757] , loss: 0.250517\n",
      "w: [[-0.17616348]\n",
      " [-0.01950732]] , b: [ 0.11648244] , loss: 0.25051\n",
      "w: [[-0.17485112]\n",
      " [-0.01968742]] , b: [ 0.11582085] , loss: 0.250502\n",
      "w: [[-0.17354921]\n",
      " [-0.01986369]] , b: [ 0.11516279] , loss: 0.250495\n",
      "w: [[-0.17225765]\n",
      " [-0.02003615]] , b: [ 0.11450825] , loss: 0.250488\n",
      "w: [[-0.17097637]\n",
      " [-0.02020487]] , b: [ 0.1138572] , loss: 0.250481\n",
      "w: [[-0.16970527]\n",
      " [-0.02036988]] , b: [ 0.11320964] , loss: 0.250474\n",
      "w: [[-0.16844428]\n",
      " [-0.02053123]] , b: [ 0.11256554] , loss: 0.250468\n",
      "w: [[-0.16719329]\n",
      " [-0.02068898]] , b: [ 0.11192489] , loss: 0.250461\n",
      "w: [[-0.16595224]\n",
      " [-0.02084316]] , b: [ 0.11128768] , loss: 0.250454\n",
      "w: [[-0.16472103]\n",
      " [-0.02099383]] , b: [ 0.1106539] , loss: 0.250448\n",
      "w: [[-0.16349958]\n",
      " [-0.02114101]] , b: [ 0.11002353] , loss: 0.250442\n",
      "w: [[-0.1622878 ]\n",
      " [-0.02128476]] , b: [ 0.10939655] , loss: 0.250435\n",
      "w: [[-0.16108562]\n",
      " [-0.02142513]] , b: [ 0.10877294] , loss: 0.250429\n",
      "w: [[-0.15989296]\n",
      " [-0.02156215]] , b: [ 0.1081527] , loss: 0.250423\n",
      "w: [[-0.15870972]\n",
      " [-0.02169587]] , b: [ 0.1075358] , loss: 0.250417\n",
      "w: [[-0.15753584]\n",
      " [-0.02182632]] , b: [ 0.10692224] , loss: 0.250411\n",
      "w: [[-0.15637124]\n",
      " [-0.02195357]] , b: [ 0.10631199] , loss: 0.250406\n",
      "w: [[-0.15521583]\n",
      " [-0.02207763]] , b: [ 0.10570505] , loss: 0.2504\n",
      "w: [[-0.15406954]\n",
      " [-0.02219856]] , b: [ 0.10510139] , loss: 0.250394\n",
      "w: [[-0.15293229]\n",
      " [-0.02231639]] , b: [ 0.10450101] , loss: 0.250389\n",
      "w: [[-0.15180399]\n",
      " [-0.02243117]] , b: [ 0.10390388] , loss: 0.250383\n",
      "w: [[-0.15068458]\n",
      " [-0.02254293]] , b: [ 0.10331] , loss: 0.250378\n",
      "w: [[-0.149574  ]\n",
      " [-0.02265172]] , b: [ 0.10271934] , loss: 0.250373\n",
      "w: [[-0.14847215]\n",
      " [-0.02275757]] , b: [ 0.1021319] , loss: 0.250367\n",
      "w: [[-0.14737895]\n",
      " [-0.02286052]] , b: [ 0.10154767] , loss: 0.250362\n",
      "w: [[-0.14629436]\n",
      " [-0.02296061]] , b: [ 0.10096661] , loss: 0.250357\n",
      "w: [[-0.14521828]\n",
      " [-0.02305789]] , b: [ 0.10038872] , loss: 0.250352\n",
      "w: [[-0.14415064]\n",
      " [-0.02315237]] , b: [ 0.09981399] , loss: 0.250347\n",
      "w: [[-0.1430914]\n",
      " [-0.0232441]] , b: [ 0.09924239] , loss: 0.250342\n",
      "w: [[-0.14204045]\n",
      " [-0.02333312]] , b: [ 0.09867392] , loss: 0.250338\n",
      "w: [[-0.14099772]\n",
      " [-0.02341947]] , b: [ 0.09810856] , loss: 0.250333\n",
      "w: [[-0.13996318]\n",
      " [-0.02350318]] , b: [ 0.09754629] , loss: 0.250328\n",
      "w: [[-0.13893673]\n",
      " [-0.02358428]] , b: [ 0.09698711] , loss: 0.250324\n",
      "w: [[-0.13791831]\n",
      " [-0.02366281]] , b: [ 0.09643099] , loss: 0.250319\n",
      "w: [[-0.13690785]\n",
      " [-0.0237388 ]] , b: [ 0.09587793] , loss: 0.250315\n",
      "w: [[-0.13590528]\n",
      " [-0.0238123 ]] , b: [ 0.09532791] , loss: 0.250311\n",
      "w: [[-0.13491055]\n",
      " [-0.02388333]] , b: [ 0.09478089] , loss: 0.250306\n",
      "w: [[-0.13392359]\n",
      " [-0.02395193]] , b: [ 0.0942369] , loss: 0.250302\n",
      "w: [[-0.13294433]\n",
      " [-0.02401812]] , b: [ 0.09369589] , loss: 0.250298\n",
      "w: [[-0.1319727 ]\n",
      " [-0.02408195]] , b: [ 0.09315786] , loss: 0.250294\n",
      "w: [[-0.13100864]\n",
      " [-0.02414344]] , b: [ 0.0926228] , loss: 0.25029\n",
      "w: [[-0.13005209]\n",
      " [-0.02420264]] , b: [ 0.09209069] , loss: 0.250286\n",
      "w: [[-0.12910298]\n",
      " [-0.02425955]] , b: [ 0.09156152] , loss: 0.250282\n",
      "w: [[-0.12816125]\n",
      " [-0.02431423]] , b: [ 0.09103527] , loss: 0.250278\n",
      "w: [[-0.12722684]\n",
      " [-0.0243667 ]] , b: [ 0.09051193] , loss: 0.250274\n",
      "w: [[-0.12629971]\n",
      " [-0.024417  ]] , b: [ 0.08999147] , loss: 0.25027\n",
      "w: [[-0.12537977]\n",
      " [-0.02446515]] , b: [ 0.0894739] , loss: 0.250267\n",
      "w: [[-0.12446697]\n",
      " [-0.02451118]] , b: [ 0.08895919] , loss: 0.250263\n",
      "w: [[-0.12356126]\n",
      " [-0.02455512]] , b: [ 0.08844733] , loss: 0.250259\n",
      "w: [[-0.12266256]\n",
      " [-0.024597  ]] , b: [ 0.08793832] , loss: 0.250256\n",
      "w: [[-0.12177083]\n",
      " [-0.02463685]] , b: [ 0.08743213] , loss: 0.250252\n",
      "w: [[-0.12088601]\n",
      " [-0.02467471]] , b: [ 0.08692874] , loss: 0.250249\n",
      "w: [[-0.12000804]\n",
      " [-0.0247106 ]] , b: [ 0.08642815] , loss: 0.250245\n",
      "w: [[-0.11913686]\n",
      " [-0.02474453]] , b: [ 0.08593035] , loss: 0.250242\n",
      "w: [[-0.1182724 ]\n",
      " [-0.02477655]] , b: [ 0.08543531] , loss: 0.250239\n",
      "w: [[-0.11741464]\n",
      " [-0.02480669]] , b: [ 0.08494302] , loss: 0.250236\n",
      "w: [[-0.1165635 ]\n",
      " [-0.02483496]] , b: [ 0.08445346] , loss: 0.250232\n",
      "w: [[-0.11571893]\n",
      " [-0.0248614 ]] , b: [ 0.08396664] , loss: 0.250229\n",
      "w: [[-0.11488087]\n",
      " [-0.02488603]] , b: [ 0.08348254] , loss: 0.250226\n",
      "w: [[-0.11404929]\n",
      " [-0.02490887]] , b: [ 0.08300113] , loss: 0.250223\n",
      "w: [[-0.1132241 ]\n",
      " [-0.02492996]] , b: [ 0.08252241] , loss: 0.25022\n",
      "w: [[-0.11240527]\n",
      " [-0.02494933]] , b: [ 0.08204635] , loss: 0.250217\n",
      "w: [[-0.11159275]\n",
      " [-0.02496699]] , b: [ 0.08157296] , loss: 0.250214\n",
      "w: [[-0.11078648]\n",
      " [-0.02498296]] , b: [ 0.08110221] , loss: 0.250211\n",
      "w: [[-0.1099864 ]\n",
      " [-0.02499729]] , b: [ 0.08063409] , loss: 0.250208\n",
      "w: [[-0.10919248]\n",
      " [-0.02500998]] , b: [ 0.08016859] , loss: 0.250205\n",
      "w: [[-0.10840464]\n",
      " [-0.02502107]] , b: [ 0.0797057] , loss: 0.250203\n",
      "w: [[-0.10762286]\n",
      " [-0.02503058]] , b: [ 0.0792454] , loss: 0.2502\n",
      "w: [[-0.10684708]\n",
      " [-0.02503852]] , b: [ 0.07878768] , loss: 0.250197\n",
      "w: [[-0.10607724]\n",
      " [-0.02504493]] , b: [ 0.07833254] , loss: 0.250195\n",
      "w: [[-0.10531331]\n",
      " [-0.02504984]] , b: [ 0.07787993] , loss: 0.250192\n",
      "w: [[-0.10455523]\n",
      " [-0.02505325]] , b: [ 0.07742987] , loss: 0.250189\n",
      "w: [[-0.10380295]\n",
      " [-0.0250552 ]] , b: [ 0.07698233] , loss: 0.250187\n",
      "w: [[-0.10305642]\n",
      " [-0.0250557 ]] , b: [ 0.0765373] , loss: 0.250184\n",
      "w: [[-0.10231562]\n",
      " [-0.02505478]] , b: [ 0.07609478] , loss: 0.250182\n",
      "w: [[-0.10158047]\n",
      " [-0.02505246]] , b: [ 0.07565474] , loss: 0.250179\n",
      "w: [[-0.10085093]\n",
      " [-0.02504877]] , b: [ 0.07521718] , loss: 0.250177\n",
      "w: [[-0.10012697]\n",
      " [-0.02504372]] , b: [ 0.07478207] , loss: 0.250175\n",
      "w: [[-0.09940854]\n",
      " [-0.02503733]] , b: [ 0.07434941] , loss: 0.250172\n",
      "w: [[-0.09869559]\n",
      " [-0.02502963]] , b: [ 0.07391918] , loss: 0.25017\n",
      "w: [[-0.09798807]\n",
      " [-0.02502063]] , b: [ 0.07349139] , loss: 0.250168\n",
      "w: [[-0.09728594]\n",
      " [-0.02501037]] , b: [ 0.073066] , loss: 0.250166\n",
      "w: [[-0.09658916]\n",
      " [-0.02499885]] , b: [ 0.07264302] , loss: 0.250163\n",
      "w: [[-0.09589768]\n",
      " [-0.0249861 ]] , b: [ 0.07222242] , loss: 0.250161\n",
      "w: [[-0.09521146]\n",
      " [-0.02497213]] , b: [ 0.0718042] , loss: 0.250159\n",
      "w: [[-0.09453046]\n",
      " [-0.02495697]] , b: [ 0.07138833] , loss: 0.250157\n",
      "w: [[-0.09385464]\n",
      " [-0.02494063]] , b: [ 0.07097482] , loss: 0.250155\n",
      "w: [[-0.09318394]\n",
      " [-0.02492314]] , b: [ 0.07056364] , loss: 0.250153\n",
      "w: [[-0.09251834]\n",
      " [-0.02490451]] , b: [ 0.07015478] , loss: 0.250151\n",
      "w: [[-0.09185778]\n",
      " [-0.02488476]] , b: [ 0.06974824] , loss: 0.250149\n",
      "w: [[-0.09120223]\n",
      " [-0.02486391]] , b: [ 0.06934399] , loss: 0.250147\n",
      "w: [[-0.09055165]\n",
      " [-0.02484198]] , b: [ 0.06894203] , loss: 0.250145\n",
      "w: [[-0.089906  ]\n",
      " [-0.02481898]] , b: [ 0.06854235] , loss: 0.250143\n",
      "w: [[-0.08926524]\n",
      " [-0.02479493]] , b: [ 0.06814493] , loss: 0.250141\n",
      "w: [[-0.08862933]\n",
      " [-0.02476986]] , b: [ 0.06774977] , loss: 0.250139\n",
      "w: [[-0.08799823]\n",
      " [-0.02474377]] , b: [ 0.06735684] , loss: 0.250137\n",
      "w: [[-0.08737189]\n",
      " [-0.02471669]] , b: [ 0.06696615] , loss: 0.250135\n",
      "w: [[-0.08675029]\n",
      " [-0.02468863]] , b: [ 0.06657767] , loss: 0.250134\n",
      "w: [[-0.08613338]\n",
      " [-0.02465961]] , b: [ 0.06619139] , loss: 0.250132\n",
      "w: [[-0.08552112]\n",
      " [-0.02462964]] , b: [ 0.06580731] , loss: 0.25013\n",
      "w: [[-0.08491348]\n",
      " [-0.02459875]] , b: [ 0.06542541] , loss: 0.250128\n",
      "w: [[-0.08431043]\n",
      " [-0.02456694]] , b: [ 0.06504568] , loss: 0.250127\n",
      "w: [[-0.08371193]\n",
      " [-0.02453424]] , b: [ 0.0646681] , loss: 0.250125\n",
      "w: [[-0.08311793]\n",
      " [-0.02450066]] , b: [ 0.06429266] , loss: 0.250123\n",
      "w: [[-0.0825284 ]\n",
      " [-0.02446621]] , b: [ 0.06391937] , loss: 0.250122\n",
      "w: [[-0.08194331]\n",
      " [-0.02443091]] , b: [ 0.06354819] , loss: 0.25012\n",
      "w: [[-0.08136262]\n",
      " [-0.02439477]] , b: [ 0.06317913] , loss: 0.250119\n",
      "w: [[-0.0807863 ]\n",
      " [-0.02435782]] , b: [ 0.06281216] , loss: 0.250117\n",
      "w: [[-0.0802143 ]\n",
      " [-0.02432006]] , b: [ 0.06244729] , loss: 0.250116\n",
      "w: [[-0.07964659]\n",
      " [-0.0242815 ]] , b: [ 0.0620845] , loss: 0.250114\n",
      "w: [[-0.07908315]\n",
      " [-0.02424218]] , b: [ 0.06172378] , loss: 0.250113\n",
      "w: [[-0.07852393]\n",
      " [-0.02420209]] , b: [ 0.0613651] , loss: 0.250111\n",
      "w: [[-0.07796891]\n",
      " [-0.02416125]] , b: [ 0.06100847] , loss: 0.25011\n",
      "w: [[-0.07741804]\n",
      " [-0.02411967]] , b: [ 0.06065388] , loss: 0.250108\n",
      "w: [[-0.07687131]\n",
      " [-0.02407738]] , b: [ 0.0603013] , loss: 0.250107\n",
      "w: [[-0.07632866]\n",
      " [-0.02403438]] , b: [ 0.05995075] , loss: 0.250105\n",
      "w: [[-0.07579008]\n",
      " [-0.02399068]] , b: [ 0.05960218] , loss: 0.250104\n",
      "w: [[-0.07525553]\n",
      " [-0.02394631]] , b: [ 0.0592556] , loss: 0.250103\n",
      "w: [[-0.07472496]\n",
      " [-0.02390126]] , b: [ 0.05891101] , loss: 0.250101\n",
      "w: [[-0.07419837]\n",
      " [-0.02385556]] , b: [ 0.05856839] , loss: 0.2501\n",
      "w: [[-0.07367569]\n",
      " [-0.02380921]] , b: [ 0.05822773] , loss: 0.250099\n",
      "w: [[-0.07315693]\n",
      " [-0.02376224]] , b: [ 0.05788902] , loss: 0.250097\n",
      "w: [[-0.07264204]\n",
      " [-0.02371466]] , b: [ 0.05755224] , loss: 0.250096\n",
      "w: [[-0.07213098]\n",
      " [-0.02366646]] , b: [ 0.05721738] , loss: 0.250095\n",
      "w: [[-0.07162374]\n",
      " [-0.02361768]] , b: [ 0.05688444] , loss: 0.250094\n",
      "w: [[-0.07112027]\n",
      " [-0.02356831]] , b: [ 0.0565534] , loss: 0.250093\n",
      "w: [[-0.07062055]\n",
      " [-0.02351837]] , b: [ 0.05622426] , loss: 0.250091\n",
      "w: [[-0.07012455]\n",
      " [-0.02346787]] , b: [ 0.055897] , loss: 0.25009\n",
      "w: [[-0.06963224]\n",
      " [-0.02341683]] , b: [ 0.05557162] , loss: 0.250089\n",
      "w: [[-0.06914359]\n",
      " [-0.02336525]] , b: [ 0.0552481] , loss: 0.250088\n",
      "w: [[-0.06865857]\n",
      " [-0.02331315]] , b: [ 0.05492643] , loss: 0.250087\n",
      "w: [[-0.06817715]\n",
      " [-0.02326054]] , b: [ 0.05460662] , loss: 0.250086\n",
      "w: [[-0.06769931]\n",
      " [-0.02320742]] , b: [ 0.05428863] , loss: 0.250084\n",
      "w: [[-0.06722501]\n",
      " [-0.02315382]] , b: [ 0.05397245] , loss: 0.250083\n",
      "w: [[-0.06675422]\n",
      " [-0.02309974]] , b: [ 0.0536581] , loss: 0.250082\n",
      "w: [[-0.06628692]\n",
      " [-0.02304518]] , b: [ 0.05334555] , loss: 0.250081\n",
      "w: [[-0.06582309]\n",
      " [-0.02299016]] , b: [ 0.0530348] , loss: 0.25008\n",
      "w: [[-0.06536269]\n",
      " [-0.02293469]] , b: [ 0.05272582] , loss: 0.250079\n",
      "w: [[-0.0649057 ]\n",
      " [-0.02287879]] , b: [ 0.05241863] , loss: 0.250078\n",
      "w: [[-0.06445208]\n",
      " [-0.02282245]] , b: [ 0.05211319] , loss: 0.250077\n",
      "w: [[-0.06400184]\n",
      " [-0.0227657 ]] , b: [ 0.0518095] , loss: 0.250076\n",
      "w: [[-0.06355491]\n",
      " [-0.02270853]] , b: [ 0.05150757] , loss: 0.250075\n",
      "w: [[-0.06311128]\n",
      " [-0.02265096]] , b: [ 0.05120736] , loss: 0.250074\n",
      "w: [[-0.06267093]\n",
      " [-0.02259301]] , b: [ 0.05090888] , loss: 0.250073\n",
      "w: [[-0.06223383]\n",
      " [-0.02253467]] , b: [ 0.05061213] , loss: 0.250072\n",
      "w: [[-0.06179995]\n",
      " [-0.02247596]] , b: [ 0.05031707] , loss: 0.250071\n",
      "w: [[-0.06136927]\n",
      " [-0.02241689]] , b: [ 0.05002371] , loss: 0.25007\n",
      "w: [[-0.06094177]\n",
      " [-0.02235746]] , b: [ 0.04973203] , loss: 0.25007\n",
      "w: [[-0.06051743]\n",
      " [-0.0222977 ]] , b: [ 0.04944203] , loss: 0.250069\n",
      "w: [[-0.0600962 ]\n",
      " [-0.02223759]] , b: [ 0.0491537] , loss: 0.250068\n",
      "w: [[-0.05967807]\n",
      " [-0.02217715]] , b: [ 0.04886703] , loss: 0.250067\n",
      "w: [[-0.05926301]\n",
      " [-0.0221164 ]] , b: [ 0.04858201] , loss: 0.250066\n",
      "w: [[-0.05885101]\n",
      " [-0.02205533]] , b: [ 0.04829864] , loss: 0.250065\n",
      "w: [[-0.05844203]\n",
      " [-0.02199396]] , b: [ 0.04801689] , loss: 0.250064\n",
      "w: [[-0.05803606]\n",
      " [-0.02193229]] , b: [ 0.04773678] , loss: 0.250064\n",
      "w: [[-0.05763306]\n",
      " [-0.02187034]] , b: [ 0.04745826] , loss: 0.250063\n",
      "w: [[-0.05723303]\n",
      " [-0.02180811]] , b: [ 0.04718136] , loss: 0.250062\n",
      "w: [[-0.05683593]\n",
      " [-0.02174561]] , b: [ 0.04690606] , loss: 0.250061\n",
      "w: [[-0.05644174]\n",
      " [-0.02168285]] , b: [ 0.04663233] , loss: 0.25006\n",
      "w: [[-0.05605045]\n",
      " [-0.02161983]] , b: [ 0.04636018] , loss: 0.25006\n",
      "w: [[-0.05566201]\n",
      " [-0.02155656]] , b: [ 0.04608961] , loss: 0.250059\n",
      "w: [[-0.05527642]\n",
      " [-0.02149305]] , b: [ 0.0458206] , loss: 0.250058\n",
      "w: [[-0.05489365]\n",
      " [-0.02142931]] , b: [ 0.04555314] , loss: 0.250057\n",
      "w: [[-0.05451367]\n",
      " [-0.02136534]] , b: [ 0.04528723] , loss: 0.250057\n",
      "w: [[-0.05413648]\n",
      " [-0.02130115]] , b: [ 0.04502284] , loss: 0.250056\n",
      "w: [[-0.05376205]\n",
      " [-0.02123675]] , b: [ 0.04475997] , loss: 0.250055\n",
      "w: [[-0.05339035]\n",
      " [-0.02117215]] , b: [ 0.04449864] , loss: 0.250055\n",
      "w: [[-0.05302136]\n",
      " [-0.02110734]] , b: [ 0.04423881] , loss: 0.250054\n",
      "w: [[-0.05265506]\n",
      " [-0.02104235]] , b: [ 0.04398048] , loss: 0.250053\n",
      "w: [[-0.05229144]\n",
      " [-0.02097717]] , b: [ 0.04372364] , loss: 0.250052\n",
      "w: [[-0.05193046]\n",
      " [-0.0209118 ]] , b: [ 0.04346829] , loss: 0.250052\n",
      "w: [[-0.05157212]\n",
      " [-0.02084628]] , b: [ 0.0432144] , loss: 0.250051\n",
      "w: [[-0.05121639]\n",
      " [-0.02078058]] , b: [ 0.04296198] , loss: 0.250051\n",
      "w: [[-0.05086325]\n",
      " [-0.02071473]] , b: [ 0.04271103] , loss: 0.25005\n",
      "w: [[-0.05051268]\n",
      " [-0.02064872]] , b: [ 0.04246153] , loss: 0.250049\n",
      "w: [[-0.05016465]\n",
      " [-0.02058257]] , b: [ 0.04221347] , loss: 0.250049\n",
      "w: [[-0.04981916]\n",
      " [-0.02051627]] , b: [ 0.04196684] , loss: 0.250048\n",
      "w: [[-0.04947618]\n",
      " [-0.02044984]] , b: [ 0.04172164] , loss: 0.250047\n",
      "w: [[-0.04913569]\n",
      " [-0.02038328]] , b: [ 0.04147786] , loss: 0.250047\n",
      "w: [[-0.04879767]\n",
      " [-0.0203166 ]] , b: [ 0.0412355] , loss: 0.250046\n",
      "w: [[-0.0484621 ]\n",
      " [-0.02024979]] , b: [ 0.04099453] , loss: 0.250046\n",
      "w: [[-0.04812897]\n",
      " [-0.02018288]] , b: [ 0.04075496] , loss: 0.250045\n",
      "w: [[-0.04779825]\n",
      " [-0.02011585]] , b: [ 0.04051677] , loss: 0.250044\n",
      "w: [[-0.04746994]\n",
      " [-0.02004873]] , b: [ 0.04027997] , loss: 0.250044\n",
      "w: [[-0.047144  ]\n",
      " [-0.01998151]] , b: [ 0.04004453] , loss: 0.250043\n",
      "w: [[-0.04682042]\n",
      " [-0.0199142 ]] , b: [ 0.03981045] , loss: 0.250043\n",
      "w: [[-0.04649918]\n",
      " [-0.01984681]] , b: [ 0.03957773] , loss: 0.250042\n",
      "w: [[-0.04618027]\n",
      " [-0.01977935]] , b: [ 0.03934635] , loss: 0.250042\n",
      "w: [[-0.04586366]\n",
      " [-0.0197118 ]] , b: [ 0.03911632] , loss: 0.250041\n",
      "w: [[-0.04554934]\n",
      " [-0.01964418]] , b: [ 0.03888763] , loss: 0.250041\n",
      "w: [[-0.04523729]\n",
      " [-0.0195765 ]] , b: [ 0.03866025] , loss: 0.25004\n",
      "w: [[-0.0449275 ]\n",
      " [-0.01950876]] , b: [ 0.0384342] , loss: 0.25004\n",
      "w: [[-0.04461994]\n",
      " [-0.01944095]] , b: [ 0.03820945] , loss: 0.250039\n",
      "w: [[-0.0443146]\n",
      " [-0.0193731]] , b: [ 0.03798601] , loss: 0.250039\n",
      "w: [[-0.04401146]\n",
      " [-0.01930521]] , b: [ 0.03776386] , loss: 0.250038\n",
      "w: [[-0.0437105 ]\n",
      " [-0.01923727]] , b: [ 0.037543] , loss: 0.250038\n",
      "w: [[-0.04341172]\n",
      " [-0.0191693 ]] , b: [ 0.03732342] , loss: 0.250037\n",
      "w: [[-0.04311508]\n",
      " [-0.01910129]] , b: [ 0.03710511] , loss: 0.250037\n",
      "w: [[-0.04282059]\n",
      " [-0.01903325]] , b: [ 0.03688807] , loss: 0.250036\n",
      "w: [[-0.04252821]\n",
      " [-0.01896519]] , b: [ 0.0366723] , loss: 0.250036\n",
      "w: [[-0.04223793]\n",
      " [-0.01889711]] , b: [ 0.03645777] , loss: 0.250035\n",
      "w: [[-0.04194974]\n",
      " [-0.01882901]] , b: [ 0.03624449] , loss: 0.250035\n",
      "w: [[-0.04166362]\n",
      " [-0.0187609 ]] , b: [ 0.03603245] , loss: 0.250035\n",
      "w: [[-0.04137956]\n",
      " [-0.01869279]] , b: [ 0.03582163] , loss: 0.250034\n",
      "w: [[-0.04109754]\n",
      " [-0.01862467]] , b: [ 0.03561203] , loss: 0.250034\n",
      "w: [[-0.04081755]\n",
      " [-0.01855656]] , b: [ 0.03540365] , loss: 0.250033\n",
      "w: [[-0.04053956]\n",
      " [-0.01848844]] , b: [ 0.03519649] , loss: 0.250033\n",
      "w: [[-0.04026356]\n",
      " [-0.01842033]] , b: [ 0.03499053] , loss: 0.250032\n",
      "w: [[-0.03998954]\n",
      " [-0.01835224]] , b: [ 0.03478577] , loss: 0.250032\n",
      "w: [[-0.03971749]\n",
      " [-0.01828416]] , b: [ 0.0345822] , loss: 0.250032\n",
      "w: [[-0.03944738]\n",
      " [-0.01821609]] , b: [ 0.03437981] , loss: 0.250031\n",
      "w: [[-0.03917921]\n",
      " [-0.01814806]] , b: [ 0.03417859] , loss: 0.250031\n",
      "w: [[-0.03891296]\n",
      " [-0.01808004]] , b: [ 0.03397854] , loss: 0.250031\n",
      "w: [[-0.03864861]\n",
      " [-0.01801206]] , b: [ 0.03377966] , loss: 0.25003\n",
      "w: [[-0.03838615]\n",
      " [-0.0179441 ]] , b: [ 0.03358193] , loss: 0.25003\n",
      "w: [[-0.03812556]\n",
      " [-0.01787619]] , b: [ 0.03338535] , loss: 0.250029\n",
      "w: [[-0.03786684]\n",
      " [-0.01780832]] , b: [ 0.03318991] , loss: 0.250029\n",
      "w: [[-0.03760996]\n",
      " [-0.01774048]] , b: [ 0.0329956] , loss: 0.250029\n",
      "w: [[-0.03735492]\n",
      " [-0.0176727 ]] , b: [ 0.03280243] , loss: 0.250028\n",
      "w: [[-0.03710169]\n",
      " [-0.01760496]] , b: [ 0.03261038] , loss: 0.250028\n",
      "w: [[-0.03685027]\n",
      " [-0.01753728]] , b: [ 0.03241945] , loss: 0.250028\n",
      "w: [[-0.03660065]\n",
      " [-0.01746965]] , b: [ 0.03222962] , loss: 0.250027\n",
      "w: [[-0.03635281]\n",
      " [-0.01740208]] , b: [ 0.0320409] , loss: 0.250027\n",
      "w: [[-0.03610672]\n",
      " [-0.01733456]] , b: [ 0.03185328] , loss: 0.250027\n",
      "w: [[-0.03586239]\n",
      " [-0.01726711]] , b: [ 0.03166676] , loss: 0.250026\n",
      "w: [[-0.0356198 ]\n",
      " [-0.01719973]] , b: [ 0.03148131] , loss: 0.250026\n",
      "w: [[-0.03537894]\n",
      " [-0.01713242]] , b: [ 0.03129694] , loss: 0.250026\n",
      "w: [[-0.03513978]\n",
      " [-0.01706518]] , b: [ 0.03111365] , loss: 0.250025\n",
      "w: [[-0.03490233]\n",
      " [-0.01699802]] , b: [ 0.03093142] , loss: 0.250025\n",
      "w: [[-0.03466656]\n",
      " [-0.01693093]] , b: [ 0.03075025] , loss: 0.250025\n",
      "w: [[-0.03443247]\n",
      " [-0.01686392]] , b: [ 0.03057014] , loss: 0.250024\n",
      "w: [[-0.03420003]\n",
      " [-0.016797  ]] , b: [ 0.03039108] , loss: 0.250024\n",
      "w: [[-0.03396925]\n",
      " [-0.01673016]] , b: [ 0.03021306] , loss: 0.250024\n",
      "w: [[-0.0337401]\n",
      " [-0.0166634]] , b: [ 0.03003608] , loss: 0.250023\n",
      "w: [[-0.03351258]\n",
      " [-0.01659674]] , b: [ 0.02986013] , loss: 0.250023\n",
      "w: [[-0.03328667]\n",
      " [-0.01653017]] , b: [ 0.0296852] , loss: 0.250023\n",
      "w: [[-0.03306235]\n",
      " [-0.0164637 ]] , b: [ 0.02951129] , loss: 0.250023\n",
      "w: [[-0.03283963]\n",
      " [-0.01639731]] , b: [ 0.0293384] , loss: 0.250022\n",
      "w: [[-0.03261848]\n",
      " [-0.01633104]] , b: [ 0.0291665] , loss: 0.250022\n",
      "w: [[-0.0323989 ]\n",
      " [-0.01626486]] , b: [ 0.02899561] , loss: 0.250022\n",
      "w: [[-0.03218087]\n",
      " [-0.01619878]] , b: [ 0.02882573] , loss: 0.250022\n",
      "w: [[-0.03196438]\n",
      " [-0.01613281]] , b: [ 0.02865682] , loss: 0.250021\n",
      "w: [[-0.03174943]\n",
      " [-0.01606696]] , b: [ 0.0284889] , loss: 0.250021\n",
      "w: [[-0.03153599]\n",
      " [-0.01600121]] , b: [ 0.02832196] , loss: 0.250021\n",
      "w: [[-0.03132405]\n",
      " [-0.01593557]] , b: [ 0.028156] , loss: 0.250021\n",
      "w: [[-0.03111362]\n",
      " [-0.01587005]] , b: [ 0.02799099] , loss: 0.25002\n",
      "w: [[-0.03090466]\n",
      " [-0.01580464]] , b: [ 0.02782695] , loss: 0.25002\n",
      "w: [[-0.03069718]\n",
      " [-0.01573936]] , b: [ 0.02766387] , loss: 0.25002\n",
      "w: [[-0.03049116]\n",
      " [-0.01567418]] , b: [ 0.02750174] , loss: 0.25002\n",
      "w: [[-0.0302866 ]\n",
      " [-0.01560913]] , b: [ 0.02734056] , loss: 0.250019\n",
      "w: [[-0.03008347]\n",
      " [-0.01554421]] , b: [ 0.02718031] , loss: 0.250019\n",
      "w: [[-0.02988178]\n",
      " [-0.01547941]] , b: [ 0.027021] , loss: 0.250019\n",
      "w: [[-0.0296815 ]\n",
      " [-0.01541473]] , b: [ 0.02686261] , loss: 0.250019\n",
      "w: [[-0.02948263]\n",
      " [-0.01535019]] , b: [ 0.02670516] , loss: 0.250018\n",
      "w: [[-0.02928516]\n",
      " [-0.01528577]] , b: [ 0.02654862] , loss: 0.250018\n",
      "w: [[-0.02908908]\n",
      " [-0.01522149]] , b: [ 0.02639298] , loss: 0.250018\n",
      "w: [[-0.02889438]\n",
      " [-0.01515734]] , b: [ 0.02623826] , loss: 0.250018\n",
      "w: [[-0.02870104]\n",
      " [-0.01509333]] , b: [ 0.02608444] , loss: 0.250017\n",
      "w: [[-0.02850907]\n",
      " [-0.01502945]] , b: [ 0.02593151] , loss: 0.250017\n",
      "w: [[-0.02831843]\n",
      " [-0.01496571]] , b: [ 0.02577948] , loss: 0.250017\n",
      "w: [[-0.02812914]\n",
      " [-0.01490211]] , b: [ 0.02562834] , loss: 0.250017\n",
      "w: [[-0.02794116]\n",
      " [-0.01483865]] , b: [ 0.02547809] , loss: 0.250017\n",
      "w: [[-0.02775451]\n",
      " [-0.01477533]] , b: [ 0.02532871] , loss: 0.250016\n",
      "w: [[-0.02756917]\n",
      " [-0.01471215]] , b: [ 0.0251802] , loss: 0.250016\n",
      "w: [[-0.02738512]\n",
      " [-0.01464912]] , b: [ 0.02503255] , loss: 0.250016\n",
      "w: [[-0.02720236]\n",
      " [-0.01458624]] , b: [ 0.02488577] , loss: 0.250016\n",
      "w: [[-0.02702088]\n",
      " [-0.0145235 ]] , b: [ 0.02473985] , loss: 0.250016\n",
      "w: [[-0.02684066]\n",
      " [-0.01446091]] , b: [ 0.02459478] , loss: 0.250015\n",
      "w: [[-0.02666171]\n",
      " [-0.01439848]] , b: [ 0.02445055] , loss: 0.250015\n",
      "w: [[-0.02648401]\n",
      " [-0.01433619]] , b: [ 0.02430717] , loss: 0.250015\n",
      "w: [[-0.02630754]\n",
      " [-0.01427405]] , b: [ 0.02416462] , loss: 0.250015\n",
      "w: [[-0.02613232]\n",
      " [-0.01421207]] , b: [ 0.02402291] , loss: 0.250015\n",
      "w: [[-0.02595831]\n",
      " [-0.01415025]] , b: [ 0.02388202] , loss: 0.250015\n",
      "w: [[-0.02578553]\n",
      " [-0.01408858]] , b: [ 0.02374195] , loss: 0.250014\n",
      "w: [[-0.02561394]\n",
      " [-0.01402707]] , b: [ 0.02360271] , loss: 0.250014\n",
      "w: [[-0.02544355]\n",
      " [-0.01396572]] , b: [ 0.02346427] , loss: 0.250014\n",
      "w: [[-0.02527435]\n",
      " [-0.01390452]] , b: [ 0.02332664] , loss: 0.250014\n",
      "w: [[-0.02510633]\n",
      " [-0.01384349]] , b: [ 0.02318982] , loss: 0.250014\n",
      "w: [[-0.02493948]\n",
      " [-0.01378261]] , b: [ 0.0230538] , loss: 0.250013\n",
      "w: [[-0.02477379]\n",
      " [-0.0137219 ]] , b: [ 0.02291857] , loss: 0.250013\n",
      "w: [[-0.02460925]\n",
      " [-0.01366136]] , b: [ 0.02278414] , loss: 0.250013\n",
      "w: [[-0.02444586]\n",
      " [-0.01360098]] , b: [ 0.02265048] , loss: 0.250013\n",
      "w: [[-0.02428361]\n",
      " [-0.01354076]] , b: [ 0.0225176] , loss: 0.250013\n",
      "w: [[-0.02412248]\n",
      " [-0.01348071]] , b: [ 0.02238551] , loss: 0.250013\n",
      "w: [[-0.02396247]\n",
      " [-0.01342082]] , b: [ 0.02225419] , loss: 0.250013\n",
      "w: [[-0.02380357]\n",
      " [-0.0133611 ]] , b: [ 0.02212363] , loss: 0.250012\n",
      "w: [[-0.02364578]\n",
      " [-0.01330154]] , b: [ 0.02199384] , loss: 0.250012\n",
      "w: [[-0.02348908]\n",
      " [-0.01324217]] , b: [ 0.0218648] , loss: 0.250012\n",
      "w: [[-0.02333347]\n",
      " [-0.01318295]] , b: [ 0.02173652] , loss: 0.250012\n",
      "w: [[-0.02317894]\n",
      " [-0.01312391]] , b: [ 0.02160899] , loss: 0.250012\n",
      "w: [[-0.02302547]\n",
      " [-0.01306503]] , b: [ 0.02148221] , loss: 0.250012\n",
      "w: [[-0.02287307]\n",
      " [-0.01300633]] , b: [ 0.02135617] , loss: 0.250012\n",
      "w: [[-0.02272172]\n",
      " [-0.01294779]] , b: [ 0.02123087] , loss: 0.250011\n",
      "w: [[-0.02257142]\n",
      " [-0.01288943]] , b: [ 0.0211063] , loss: 0.250011\n",
      "w: [[-0.02242216]\n",
      " [-0.01283125]] , b: [ 0.02098246] , loss: 0.250011\n",
      "w: [[-0.02227394]\n",
      " [-0.01277324]] , b: [ 0.02085934] , loss: 0.250011\n",
      "w: [[-0.02212674]\n",
      " [-0.01271541]] , b: [ 0.02073693] , loss: 0.250011\n",
      "w: [[-0.02198056]\n",
      " [-0.01265775]] , b: [ 0.02061525] , loss: 0.250011\n",
      "w: [[-0.02183539]\n",
      " [-0.01260026]] , b: [ 0.02049427] , loss: 0.250011\n",
      "w: [[-0.02169122]\n",
      " [-0.01254295]] , b: [ 0.020374] , loss: 0.25001\n",
      "w: [[-0.02154805]\n",
      " [-0.01248583]] , b: [ 0.02025443] , loss: 0.25001\n",
      "w: [[-0.02140587]\n",
      " [-0.01242888]] , b: [ 0.02013556] , loss: 0.25001\n",
      "w: [[-0.02126467]\n",
      " [-0.0123721 ]] , b: [ 0.02001739] , loss: 0.25001\n",
      "w: [[-0.02112444]\n",
      " [-0.01231551]] , b: [ 0.01989991] , loss: 0.25001\n",
      "w: [[-0.02098518]\n",
      " [-0.01225909]] , b: [ 0.01978312] , loss: 0.25001\n",
      "w: [[-0.02084688]\n",
      " [-0.01220285]] , b: [ 0.01966701] , loss: 0.25001\n",
      "w: [[-0.02070953]\n",
      " [-0.0121468 ]] , b: [ 0.01955158] , loss: 0.25001\n",
      "w: [[-0.02057313]\n",
      " [-0.01209091]] , b: [ 0.01943683] , loss: 0.250009\n",
      "w: [[-0.02043766]\n",
      " [-0.01203521]] , b: [ 0.01932275] , loss: 0.250009\n",
      "w: [[-0.02030313]\n",
      " [-0.0119797 ]] , b: [ 0.01920933] , loss: 0.250009\n",
      "w: [[-0.02016952]\n",
      " [-0.01192436]] , b: [ 0.01909658] , loss: 0.250009\n",
      "w: [[-0.02003683]\n",
      " [-0.01186921]] , b: [ 0.01898449] , loss: 0.250009\n",
      "w: [[-0.01990506]\n",
      " [-0.01181424]] , b: [ 0.01887305] , loss: 0.250009\n",
      "w: [[-0.01977419]\n",
      " [-0.01175945]] , b: [ 0.01876226] , loss: 0.250009\n",
      "w: [[-0.01964422]\n",
      " [-0.01170484]] , b: [ 0.01865213] , loss: 0.250009\n",
      "w: [[-0.01951514]\n",
      " [-0.01165042]] , b: [ 0.01854263] , loss: 0.250009\n",
      "w: [[-0.01938695]\n",
      " [-0.01159618]] , b: [ 0.01843379] , loss: 0.250008\n",
      "w: [[-0.01925963]\n",
      " [-0.01154211]] , b: [ 0.01832557] , loss: 0.250008\n",
      "w: [[-0.0191332 ]\n",
      " [-0.01148824]] , b: [ 0.01821799] , loss: 0.250008\n",
      "w: [[-0.01900763]\n",
      " [-0.01143454]] , b: [ 0.01811104] , loss: 0.250008\n",
      "w: [[-0.01888292]\n",
      " [-0.01138103]] , b: [ 0.01800472] , loss: 0.250008\n",
      "w: [[-0.01875906]\n",
      " [-0.0113277 ]] , b: [ 0.01789903] , loss: 0.250008\n",
      "w: [[-0.01863604]\n",
      " [-0.01127455]] , b: [ 0.01779394] , loss: 0.250008\n",
      "w: [[-0.01851388]\n",
      " [-0.0112216 ]] , b: [ 0.01768948] , loss: 0.250008\n",
      "w: [[-0.01839255]\n",
      " [-0.01116882]] , b: [ 0.01758562] , loss: 0.250008\n",
      "w: [[-0.01827204]\n",
      " [-0.01111623]] , b: [ 0.01748237] , loss: 0.250008\n",
      "w: [[-0.01815237]\n",
      " [-0.01106383]] , b: [ 0.01737974] , loss: 0.250008\n",
      "w: [[-0.01803351]\n",
      " [-0.01101161]] , b: [ 0.01727769] , loss: 0.250007\n",
      "w: [[-0.01791546]\n",
      " [-0.01095957]] , b: [ 0.01717625] , loss: 0.250007\n",
      "w: [[-0.01779822]\n",
      " [-0.01090772]] , b: [ 0.01707541] , loss: 0.250007\n",
      "w: [[-0.01768178]\n",
      " [-0.01085605]] , b: [ 0.01697515] , loss: 0.250007\n",
      "w: [[-0.01756614]\n",
      " [-0.01080457]] , b: [ 0.01687548] , loss: 0.250007\n",
      "w: [[-0.01745129]\n",
      " [-0.01075328]] , b: [ 0.01677639] , loss: 0.250007\n",
      "w: [[-0.01733722]\n",
      " [-0.01070217]] , b: [ 0.01667789] , loss: 0.250007\n",
      "w: [[-0.01722392]\n",
      " [-0.01065124]] , b: [ 0.01657996] , loss: 0.250007\n",
      "w: [[-0.0171114]\n",
      " [-0.0106005]] , b: [ 0.0164826] , loss: 0.250007\n",
      "w: [[-0.01699965]\n",
      " [-0.01054995]] , b: [ 0.0163858] , loss: 0.250007\n",
      "w: [[-0.01688866]\n",
      " [-0.01049958]] , b: [ 0.01628958] , loss: 0.250007\n",
      "w: [[-0.01677843]\n",
      " [-0.0104494 ]] , b: [ 0.01619392] , loss: 0.250006\n",
      "w: [[-0.01666894]\n",
      " [-0.0103994 ]] , b: [ 0.01609882] , loss: 0.250006\n",
      "w: [[-0.01656019]\n",
      " [-0.01034958]] , b: [ 0.01600428] , loss: 0.250006\n",
      "w: [[-0.01645219]\n",
      " [-0.01029995]] , b: [ 0.0159103] , loss: 0.250006\n",
      "w: [[-0.01634492]\n",
      " [-0.0102505 ]] , b: [ 0.01581687] , loss: 0.250006\n",
      "w: [[-0.01623838]\n",
      " [-0.01020124]] , b: [ 0.01572398] , loss: 0.250006\n",
      "w: [[-0.01613257]\n",
      " [-0.01015217]] , b: [ 0.01563163] , loss: 0.250006\n",
      "w: [[-0.01602747]\n",
      " [-0.01010328]] , b: [ 0.01553983] , loss: 0.250006\n",
      "w: [[-0.01592308]\n",
      " [-0.01005457]] , b: [ 0.01544857] , loss: 0.250006\n",
      "w: [[-0.0158194 ]\n",
      " [-0.01000604]] , b: [ 0.01535785] , loss: 0.250006\n",
      "w: [[-0.01571643]\n",
      " [-0.0099577 ]] , b: [ 0.01526765] , loss: 0.250006\n",
      "w: [[-0.01561416]\n",
      " [-0.00990955]] , b: [ 0.01517798] , loss: 0.250006\n",
      "w: [[-0.01551258]\n",
      " [-0.00986159]] , b: [ 0.01508883] , loss: 0.250006\n",
      "w: [[-0.01541169]\n",
      " [-0.00981381]] , b: [ 0.01500021] , loss: 0.250006\n",
      "w: [[-0.01531149]\n",
      " [-0.00976621]] , b: [ 0.01491211] , loss: 0.250006\n",
      "w: [[-0.01521196]\n",
      " [-0.00971879]] , b: [ 0.01482452] , loss: 0.250005\n",
      "w: [[-0.0151131 ]\n",
      " [-0.00967155]] , b: [ 0.01473745] , loss: 0.250005\n",
      "w: [[-0.01501492]\n",
      " [-0.0096245 ]] , b: [ 0.01465089] , loss: 0.250005\n",
      "w: [[-0.0149174 ]\n",
      " [-0.00957764]] , b: [ 0.01456483] , loss: 0.250005\n",
      "w: [[-0.01482054]\n",
      " [-0.00953095]] , b: [ 0.01447928] , loss: 0.250005\n",
      "w: [[-0.01472434]\n",
      " [-0.00948445]] , b: [ 0.01439423] , loss: 0.250005\n",
      "w: [[-0.01462878]\n",
      " [-0.00943813]] , b: [ 0.01430969] , loss: 0.250005\n",
      "w: [[-0.01453387]\n",
      " [-0.009392  ]] , b: [ 0.01422563] , loss: 0.250005\n",
      "w: [[-0.0144396 ]\n",
      " [-0.00934604]] , b: [ 0.01414207] , loss: 0.250005\n",
      "w: [[-0.01434597]\n",
      " [-0.00930027]] , b: [ 0.014059] , loss: 0.250005\n",
      "w: [[-0.01425297]\n",
      " [-0.00925468]] , b: [ 0.01397641] , loss: 0.250005\n",
      "w: [[-0.0141606 ]\n",
      " [-0.00920927]] , b: [ 0.0138943] , loss: 0.250005\n",
      "w: [[-0.01406885]\n",
      " [-0.00916405]] , b: [ 0.01381268] , loss: 0.250005\n",
      "w: [[-0.01397772]\n",
      " [-0.009119  ]] , b: [ 0.01373154] , loss: 0.250005\n",
      "w: [[-0.01388721]\n",
      " [-0.00907414]] , b: [ 0.01365087] , loss: 0.250005\n",
      "w: [[-0.0137973 ]\n",
      " [-0.00902945]] , b: [ 0.01357068] , loss: 0.250005\n",
      "w: [[-0.013708  ]\n",
      " [-0.00898494]] , b: [ 0.01349095] , loss: 0.250004\n",
      "w: [[-0.0136193 ]\n",
      " [-0.00894062]] , b: [ 0.0134117] , loss: 0.250004\n",
      "w: [[-0.0135312 ]\n",
      " [-0.00889648]] , b: [ 0.0133329] , loss: 0.250004\n",
      "w: [[-0.01344369]\n",
      " [-0.00885252]] , b: [ 0.01325457] , loss: 0.250004\n",
      "w: [[-0.01335677]\n",
      " [-0.00880872]] , b: [ 0.0131767] , loss: 0.250004\n",
      "w: [[-0.01327043]\n",
      " [-0.00876511]] , b: [ 0.0130993] , loss: 0.250004\n",
      "w: [[-0.01318467]\n",
      " [-0.00872168]] , b: [ 0.01302234] , loss: 0.250004\n",
      "w: [[-0.0130995 ]\n",
      " [-0.00867843]] , b: [ 0.01294583] , loss: 0.250004\n",
      "w: [[-0.01301489]\n",
      " [-0.00863536]] , b: [ 0.01286976] , loss: 0.250004\n",
      "w: [[-0.01293086]\n",
      " [-0.00859247]] , b: [ 0.01279415] , loss: 0.250004\n",
      "w: [[-0.01284739]\n",
      " [-0.00854975]] , b: [ 0.01271898] , loss: 0.250004\n",
      "w: [[-0.01276447]\n",
      " [-0.00850721]] , b: [ 0.01264424] , loss: 0.250004\n",
      "w: [[-0.01268212]\n",
      " [-0.00846485]] , b: [ 0.01256995] , loss: 0.250004\n",
      "w: [[-0.01260031]\n",
      " [-0.00842266]] , b: [ 0.0124961] , loss: 0.250004\n",
      "w: [[-0.01251906]\n",
      " [-0.00838065]] , b: [ 0.01242268] , loss: 0.250004\n",
      "w: [[-0.01243835]\n",
      " [-0.00833882]] , b: [ 0.01234968] , loss: 0.250004\n",
      "w: [[-0.01235818]\n",
      " [-0.00829716]] , b: [ 0.01227711] , loss: 0.250004\n",
      "w: [[-0.01227855]\n",
      " [-0.00825568]] , b: [ 0.01220497] , loss: 0.250004\n",
      "w: [[-0.01219945]\n",
      " [-0.00821436]] , b: [ 0.01213326] , loss: 0.250004\n",
      "w: [[-0.01212087]\n",
      " [-0.00817322]] , b: [ 0.01206197] , loss: 0.250004\n",
      "w: [[-0.01204283]\n",
      " [-0.00813225]] , b: [ 0.01199109] , loss: 0.250004\n",
      "w: [[-0.01196531]\n",
      " [-0.00809146]] , b: [ 0.01192063] , loss: 0.250003\n",
      "w: [[-0.01188831]\n",
      " [-0.00805085]] , b: [ 0.01185057] , loss: 0.250003\n",
      "w: [[-0.01181182]\n",
      " [-0.00801041]] , b: [ 0.01178094] , loss: 0.250003\n",
      "w: [[-0.01173584]\n",
      " [-0.00797013]] , b: [ 0.01171171] , loss: 0.250003\n",
      "w: [[-0.01166037]\n",
      " [-0.00793003]] , b: [ 0.01164289] , loss: 0.250003\n",
      "w: [[-0.01158541]\n",
      " [-0.0078901 ]] , b: [ 0.01157447] , loss: 0.250003\n",
      "w: [[-0.01151094]\n",
      " [-0.00785035]] , b: [ 0.01150645] , loss: 0.250003\n",
      "w: [[-0.01143697]\n",
      " [-0.00781076]] , b: [ 0.01143884] , loss: 0.250003\n",
      "w: [[-0.0113635 ]\n",
      " [-0.00777134]] , b: [ 0.01137162] , loss: 0.250003\n",
      "w: [[-0.01129051]\n",
      " [-0.00773209]] , b: [ 0.01130479] , loss: 0.250003\n",
      "w: [[-0.01121802]\n",
      " [-0.00769302]] , b: [ 0.01123835] , loss: 0.250003\n",
      "w: [[-0.011146 ]\n",
      " [-0.0076541]] , b: [ 0.01117231] , loss: 0.250003\n",
      "w: [[-0.01107446]\n",
      " [-0.00761536]] , b: [ 0.01110666] , loss: 0.250003\n",
      "w: [[-0.0110034 ]\n",
      " [-0.00757679]] , b: [ 0.01104139] , loss: 0.250003\n",
      "w: [[-0.01093281]\n",
      " [-0.00753838]] , b: [ 0.01097651] , loss: 0.250003\n",
      "w: [[-0.0108627 ]\n",
      " [-0.00750014]] , b: [ 0.010912] , loss: 0.250003\n",
      "w: [[-0.01079305]\n",
      " [-0.00746207]] , b: [ 0.01084787] , loss: 0.250003\n",
      "w: [[-0.01072386]\n",
      " [-0.00742416]] , b: [ 0.01078412] , loss: 0.250003\n",
      "w: [[-0.01065514]\n",
      " [-0.00738642]] , b: [ 0.01072074] , loss: 0.250003\n",
      "w: [[-0.01058687]\n",
      " [-0.00734886]] , b: [ 0.01065772] , loss: 0.250003\n",
      "w: [[-0.01051906]\n",
      " [-0.00731145]] , b: [ 0.01059509] , loss: 0.250003\n",
      "w: [[-0.01045169]\n",
      " [-0.0072742 ]] , b: [ 0.01053282] , loss: 0.250003\n",
      "w: [[-0.01038478]\n",
      " [-0.00723713]] , b: [ 0.01047091] , loss: 0.250003\n",
      "w: [[-0.01031831]\n",
      " [-0.00720022]] , b: [ 0.01040937] , loss: 0.250003\n",
      "w: [[-0.01025228]\n",
      " [-0.00716347]] , b: [ 0.01034819] , loss: 0.250003\n",
      "w: [[-0.01018669]\n",
      " [-0.00712688]] , b: [ 0.01028736] , loss: 0.250003\n",
      "w: [[-0.01012153]\n",
      " [-0.00709046]] , b: [ 0.0102269] , loss: 0.250003\n",
      "w: [[-0.01005681]\n",
      " [-0.00705419]] , b: [ 0.01016679] , loss: 0.250003\n",
      "w: [[-0.00999251]\n",
      " [-0.00701809]] , b: [ 0.01010704] , loss: 0.250003\n",
      "w: [[-0.00992864]\n",
      " [-0.00698215]] , b: [ 0.01004763] , loss: 0.250002\n",
      "w: [[-0.00986519]\n",
      " [-0.00694637]] , b: [ 0.00998857] , loss: 0.250002\n",
      "w: [[-0.00980216]\n",
      " [-0.00691075]] , b: [ 0.00992987] , loss: 0.250002\n",
      "w: [[-0.00973955]\n",
      " [-0.00687529]] , b: [ 0.0098715] , loss: 0.250002\n",
      "w: [[-0.00967736]\n",
      " [-0.00683999]] , b: [ 0.00981348] , loss: 0.250002\n",
      "w: [[-0.00961558]\n",
      " [-0.00680485]] , b: [ 0.00975579] , loss: 0.250002\n",
      "w: [[-0.00955421]\n",
      " [-0.00676987]] , b: [ 0.00969844] , loss: 0.250002\n",
      "w: [[-0.00949324]\n",
      " [-0.00673505]] , b: [ 0.00964144] , loss: 0.250002\n",
      "w: [[-0.00943268]\n",
      " [-0.00670039]] , b: [ 0.00958476] , loss: 0.250002\n",
      "w: [[-0.00937251]\n",
      " [-0.00666587]] , b: [ 0.00952843] , loss: 0.250002\n",
      "w: [[-0.00931274]\n",
      " [-0.00663152]] , b: [ 0.00947242] , loss: 0.250002\n",
      "w: [[-0.00925337]\n",
      " [-0.00659732]] , b: [ 0.00941675] , loss: 0.250002\n",
      "w: [[-0.00919439]\n",
      " [-0.00656328]] , b: [ 0.00936139] , loss: 0.250002\n",
      "w: [[-0.00913579]\n",
      " [-0.00652939]] , b: [ 0.00930637] , loss: 0.250002\n",
      "w: [[-0.00907759]\n",
      " [-0.00649566]] , b: [ 0.00925166] , loss: 0.250002\n",
      "w: [[-0.00901977]\n",
      " [-0.00646208]] , b: [ 0.00919727] , loss: 0.250002\n",
      "w: [[-0.00896234]\n",
      " [-0.00642866]] , b: [ 0.00914321] , loss: 0.250002\n",
      "w: [[-0.00890528]\n",
      " [-0.00639539]] , b: [ 0.00908946] , loss: 0.250002\n",
      "w: [[-0.00884859]\n",
      " [-0.00636227]] , b: [ 0.00903603] , loss: 0.250002\n",
      "w: [[-0.00879228]\n",
      " [-0.0063293 ]] , b: [ 0.00898292] , loss: 0.250002\n",
      "w: [[-0.00873635]\n",
      " [-0.00629649]] , b: [ 0.00893011] , loss: 0.250002\n",
      "w: [[-0.00868078]\n",
      " [-0.00626383]] , b: [ 0.00887762] , loss: 0.250002\n",
      "w: [[-0.00862557]\n",
      " [-0.00623132]] , b: [ 0.00882543] , loss: 0.250002\n",
      "w: [[-0.00857073]\n",
      " [-0.00619895]] , b: [ 0.00877355] , loss: 0.250002\n",
      "w: [[-0.00851625]\n",
      " [-0.00616674]] , b: [ 0.00872197] , loss: 0.250002\n",
      "w: [[-0.00846213]\n",
      " [-0.00613467]] , b: [ 0.0086707] , loss: 0.250002\n",
      "w: [[-0.00840836]\n",
      " [-0.00610276]] , b: [ 0.00861973] , loss: 0.250002\n",
      "w: [[-0.00835495]\n",
      " [-0.00607099]] , b: [ 0.00856906] , loss: 0.250002\n",
      "w: [[-0.00830188]\n",
      " [-0.00603936]] , b: [ 0.00851868] , loss: 0.250002\n",
      "w: [[-0.00824917]\n",
      " [-0.00600789]] , b: [ 0.0084686] , loss: 0.250002\n",
      "w: [[-0.00819681]\n",
      " [-0.00597657]] , b: [ 0.00841881] , loss: 0.250002\n",
      "w: [[-0.00814478]\n",
      " [-0.00594539]] , b: [ 0.00836932] , loss: 0.250002\n",
      "w: [[-0.0080931 ]\n",
      " [-0.00591435]] , b: [ 0.00832011] , loss: 0.250002\n",
      "w: [[-0.00804176]\n",
      " [-0.00588347]] , b: [ 0.0082712] , loss: 0.250002\n",
      "w: [[-0.00799076]\n",
      " [-0.00585272]] , b: [ 0.00822257] , loss: 0.250002\n",
      "w: [[-0.00794009]\n",
      " [-0.00582212]] , b: [ 0.00817423] , loss: 0.250002\n",
      "w: [[-0.00788974]\n",
      " [-0.00579166]] , b: [ 0.00812618] , loss: 0.250002\n",
      "w: [[-0.00783974]\n",
      " [-0.00576135]] , b: [ 0.0080784] , loss: 0.250002\n",
      "w: [[-0.00779006]\n",
      " [-0.00573118]] , b: [ 0.00803091] , loss: 0.250002\n",
      "w: [[-0.0077407 ]\n",
      " [-0.00570115]] , b: [ 0.0079837] , loss: 0.250002\n",
      "w: [[-0.00769166]\n",
      " [-0.00567126]] , b: [ 0.00793676] , loss: 0.250002\n",
      "w: [[-0.00764295]\n",
      " [-0.00564152]] , b: [ 0.0078901] , loss: 0.250001\n",
      "w: [[-0.00759456]\n",
      " [-0.00561191]] , b: [ 0.00784371] , loss: 0.250001\n",
      "w: [[-0.00754648]\n",
      " [-0.00558244]] , b: [ 0.00779759] , loss: 0.250001\n",
      "w: [[-0.00749873]\n",
      " [-0.00555312]] , b: [ 0.00775175] , loss: 0.250001\n",
      "w: [[-0.00745127]\n",
      " [-0.00552393]] , b: [ 0.00770617] , loss: 0.250001\n",
      "w: [[-0.00740414]\n",
      " [-0.00549489]] , b: [ 0.00766086] , loss: 0.250001\n",
      "w: [[-0.00735731]\n",
      " [-0.00546598]] , b: [ 0.00761582] , loss: 0.250001\n",
      "w: [[-0.00731079]\n",
      " [-0.00543721]] , b: [ 0.00757103] , loss: 0.250001\n",
      "w: [[-0.00726457]\n",
      " [-0.00540858]] , b: [ 0.00752652] , loss: 0.250001\n",
      "w: [[-0.00721865]\n",
      " [-0.00538008]] , b: [ 0.00748227] , loss: 0.250001\n",
      "w: [[-0.00717303]\n",
      " [-0.00535172]] , b: [ 0.00743827] , loss: 0.250001\n",
      "w: [[-0.00712772]\n",
      " [-0.0053235 ]] , b: [ 0.00739453] , loss: 0.250001\n",
      "w: [[-0.0070827 ]\n",
      " [-0.00529541]] , b: [ 0.00735105] , loss: 0.250001\n",
      "w: [[-0.00703797]\n",
      " [-0.00526745]] , b: [ 0.00730783] , loss: 0.250001\n",
      "w: [[-0.00699353]\n",
      " [-0.00523963]] , b: [ 0.00726486] , loss: 0.250001\n",
      "w: [[-0.00694938]\n",
      " [-0.00521195]] , b: [ 0.00722214] , loss: 0.250001\n",
      "w: [[-0.00690552]\n",
      " [-0.00518439]] , b: [ 0.00717968] , loss: 0.250001\n",
      "w: [[-0.00686194]\n",
      " [-0.00515696]] , b: [ 0.00713746] , loss: 0.250001\n",
      "w: [[-0.00681865]\n",
      " [-0.00512968]] , b: [ 0.0070955] , loss: 0.250001\n",
      "w: [[-0.00677564]\n",
      " [-0.00510252]] , b: [ 0.00705378] , loss: 0.250001\n",
      "w: [[-0.00673291]\n",
      " [-0.00507549]] , b: [ 0.00701231] , loss: 0.250001\n",
      "w: [[-0.00669046]\n",
      " [-0.0050486 ]] , b: [ 0.00697107] , loss: 0.250001\n",
      "w: [[-0.00664828]\n",
      " [-0.00502183]] , b: [ 0.00693007] , loss: 0.250001\n",
      "w: [[-0.00660639]\n",
      " [-0.0049952 ]] , b: [ 0.00688933] , loss: 0.250001\n",
      "w: [[-0.00656475]\n",
      " [-0.00496869]] , b: [ 0.00684882] , loss: 0.250001\n",
      "w: [[-0.0065234 ]\n",
      " [-0.00494231]] , b: [ 0.00680855] , loss: 0.250001\n",
      "w: [[-0.00648231]\n",
      " [-0.00491606]] , b: [ 0.00676851] , loss: 0.250001\n",
      "w: [[-0.00644149]\n",
      " [-0.00488994]] , b: [ 0.00672871] , loss: 0.250001\n",
      "w: [[-0.00640093]\n",
      " [-0.00486394]] , b: [ 0.00668915] , loss: 0.250001\n",
      "w: [[-0.00636063]\n",
      " [-0.00483807]] , b: [ 0.00664982] , loss: 0.250001\n",
      "w: [[-0.0063206 ]\n",
      " [-0.00481233]] , b: [ 0.00661072] , loss: 0.250001\n",
      "w: [[-0.00628083]\n",
      " [-0.00478672]] , b: [ 0.00657184] , loss: 0.250001\n",
      "w: [[-0.00624132]\n",
      " [-0.00476123]] , b: [ 0.0065332] , loss: 0.250001\n",
      "w: [[-0.00620206]\n",
      " [-0.00473586]] , b: [ 0.00649478] , loss: 0.250001\n",
      "w: [[-0.00616305]\n",
      " [-0.00471061]] , b: [ 0.00645659] , loss: 0.250001\n",
      "w: [[-0.0061243 ]\n",
      " [-0.00468549]] , b: [ 0.00641863] , loss: 0.250001\n",
      "w: [[-0.00608581]\n",
      " [-0.0046605 ]] , b: [ 0.00638087] , loss: 0.250001\n",
      "w: [[-0.00604756]\n",
      " [-0.00463562]] , b: [ 0.00634335] , loss: 0.250001\n",
      "w: [[-0.00600956]\n",
      " [-0.00461087]] , b: [ 0.00630605] , loss: 0.250001\n",
      "w: [[-0.0059718 ]\n",
      " [-0.00458624]] , b: [ 0.00626897] , loss: 0.250001\n",
      "w: [[-0.0059343 ]\n",
      " [-0.00456174]] , b: [ 0.0062321] , loss: 0.250001\n",
      "w: [[-0.00589703]\n",
      " [-0.00453735]] , b: [ 0.00619545] , loss: 0.250001\n",
      "w: [[-0.00586001]\n",
      " [-0.00451309]] , b: [ 0.00615902] , loss: 0.250001\n",
      "w: [[-0.00582322]\n",
      " [-0.00448894]] , b: [ 0.00612279] , loss: 0.250001\n",
      "w: [[-0.00578668]\n",
      " [-0.00446492]] , b: [ 0.00608679] , loss: 0.250001\n",
      "w: [[-0.00575036]\n",
      " [-0.00444101]] , b: [ 0.006051] , loss: 0.250001\n",
      "w: [[-0.00571429]\n",
      " [-0.00441722]] , b: [ 0.00601541] , loss: 0.250001\n",
      "w: [[-0.00567844]\n",
      " [-0.00439354]] , b: [ 0.00598004] , loss: 0.250001\n",
      "w: [[-0.00564283]\n",
      " [-0.00436999]] , b: [ 0.00594487] , loss: 0.250001\n",
      "w: [[-0.00560745]\n",
      " [-0.00434655]] , b: [ 0.00590991] , loss: 0.250001\n",
      "w: [[-0.0055723 ]\n",
      " [-0.00432323]] , b: [ 0.00587515] , loss: 0.250001\n",
      "w: [[-0.00553737]\n",
      " [-0.00430003]] , b: [ 0.0058406] , loss: 0.250001\n",
      "w: [[-0.00550267]\n",
      " [-0.00427694]] , b: [ 0.00580625] , loss: 0.250001\n",
      "w: [[-0.00546819]\n",
      " [-0.00425396]] , b: [ 0.0057721] , loss: 0.250001\n",
      "w: [[-0.00543394]\n",
      " [-0.00423111]] , b: [ 0.00573816] , loss: 0.250001\n",
      "w: [[-0.00539991]\n",
      " [-0.00420836]] , b: [ 0.00570441] , loss: 0.250001\n",
      "w: [[-0.00536609]\n",
      " [-0.00418573]] , b: [ 0.00567086] , loss: 0.250001\n",
      "w: [[-0.00533249]\n",
      " [-0.0041632 ]] , b: [ 0.00563752] , loss: 0.250001\n",
      "w: [[-0.00529911]\n",
      " [-0.00414079]] , b: [ 0.00560437] , loss: 0.250001\n",
      "w: [[-0.00526595]\n",
      " [-0.0041185 ]] , b: [ 0.0055714] , loss: 0.250001\n",
      "w: [[-0.00523299]\n",
      " [-0.00409631]] , b: [ 0.00553864] , loss: 0.250001\n",
      "w: [[-0.00520025]\n",
      " [-0.00407423]] , b: [ 0.00550607] , loss: 0.250001\n",
      "w: [[-0.00516772]\n",
      " [-0.00405227]] , b: [ 0.00547369] , loss: 0.250001\n",
      "w: [[-0.00513541]\n",
      " [-0.00403042]] , b: [ 0.00544149] , loss: 0.250001\n",
      "w: [[-0.00510329]\n",
      " [-0.00400868]] , b: [ 0.00540949] , loss: 0.250001\n",
      "w: [[-0.00507139]\n",
      " [-0.00398705]] , b: [ 0.00537768] , loss: 0.250001\n",
      "w: [[-0.0050397 ]\n",
      " [-0.00396553]] , b: [ 0.00534605] , loss: 0.250001\n",
      "w: [[-0.0050082 ]\n",
      " [-0.00394411]] , b: [ 0.00531461] , loss: 0.250001\n",
      "w: [[-0.0049769]\n",
      " [-0.0039228]] , b: [ 0.00528335] , loss: 0.250001\n",
      "w: [[-0.00494581]\n",
      " [-0.0039016 ]] , b: [ 0.00525228] , loss: 0.250001\n",
      "w: [[-0.00491492]\n",
      " [-0.0038805 ]] , b: [ 0.00522139] , loss: 0.250001\n",
      "w: [[-0.00488423]\n",
      " [-0.00385951]] , b: [ 0.00519068] , loss: 0.250001\n",
      "w: [[-0.00485373]\n",
      " [-0.00383863]] , b: [ 0.00516016] , loss: 0.250001\n",
      "w: [[-0.00482343]\n",
      " [-0.00381785]] , b: [ 0.00512981] , loss: 0.250001\n",
      "w: [[-0.00479333]\n",
      " [-0.00379718]] , b: [ 0.00509964] , loss: 0.250001\n",
      "w: [[-0.00476341]\n",
      " [-0.00377661]] , b: [ 0.00506965] , loss: 0.250001\n",
      "w: [[-0.00473369]\n",
      " [-0.00375615]] , b: [ 0.00503983] , loss: 0.250001\n",
      "w: [[-0.00470416]\n",
      " [-0.00373579]] , b: [ 0.00501019] , loss: 0.250001\n",
      "w: [[-0.00467482]\n",
      " [-0.00371554]] , b: [ 0.00498072] , loss: 0.250001\n",
      "w: [[-0.00464566]\n",
      " [-0.00369538]] , b: [ 0.00495143] , loss: 0.250001\n",
      "w: [[-0.0046167 ]\n",
      " [-0.00367533]] , b: [ 0.0049223] , loss: 0.250001\n",
      "w: [[-0.00458791]\n",
      " [-0.00365538]] , b: [ 0.00489335] , loss: 0.250001\n",
      "w: [[-0.00455932]\n",
      " [-0.00363554]] , b: [ 0.00486457] , loss: 0.250001\n",
      "w: [[-0.00453091]\n",
      " [-0.00361579]] , b: [ 0.00483596] , loss: 0.250001\n",
      "w: [[-0.00450267]\n",
      " [-0.00359614]] , b: [ 0.00480752] , loss: 0.250001\n",
      "w: [[-0.00447462]\n",
      " [-0.0035766 ]] , b: [ 0.00477925] , loss: 0.250001\n",
      "w: [[-0.00444675]\n",
      " [-0.00355716]] , b: [ 0.00475113] , loss: 0.250001\n",
      "w: [[-0.00441906]\n",
      " [-0.00353781]] , b: [ 0.00472319] , loss: 0.250001\n",
      "w: [[-0.00439155]\n",
      " [-0.00351856]] , b: [ 0.0046954] , loss: 0.250001\n",
      "w: [[-0.00436421]\n",
      " [-0.00349942]] , b: [ 0.00466778] , loss: 0.250001\n",
      "w: [[-0.00433705]\n",
      " [-0.00348037]] , b: [ 0.00464033] , loss: 0.250001\n",
      "w: [[-0.00431006]\n",
      " [-0.00346142]] , b: [ 0.00461303] , loss: 0.250001\n",
      "w: [[-0.00428325]\n",
      " [-0.00344257]] , b: [ 0.0045859] , loss: 0.250001\n",
      "w: [[-0.0042566 ]\n",
      " [-0.00342381]] , b: [ 0.00455892] , loss: 0.25\n",
      "w: [[-0.00423013]\n",
      " [-0.00340515]] , b: [ 0.0045321] , loss: 0.25\n",
      "w: [[-0.00420382]\n",
      " [-0.00338659]] , b: [ 0.00450545] , loss: 0.25\n",
      "w: [[-0.00417769]\n",
      " [-0.00336812]] , b: [ 0.00447895] , loss: 0.25\n",
      "w: [[-0.00415172]\n",
      " [-0.00334974]] , b: [ 0.0044526] , loss: 0.25\n",
      "w: [[-0.00412591]\n",
      " [-0.00333146]] , b: [ 0.00442642] , loss: 0.25\n",
      "w: [[-0.00410027]\n",
      " [-0.00331327]] , b: [ 0.00440038] , loss: 0.25\n",
      "w: [[-0.00407479]\n",
      " [-0.00329518]] , b: [ 0.0043745] , loss: 0.25\n",
      "w: [[-0.00404948]\n",
      " [-0.00327718]] , b: [ 0.00434877] , loss: 0.25\n",
      "w: [[-0.00402432]\n",
      " [-0.00325927]] , b: [ 0.00432319] , loss: 0.25\n",
      "w: [[-0.00399933]\n",
      " [-0.00324145]] , b: [ 0.00429776] , loss: 0.25\n",
      "w: [[-0.0039745 ]\n",
      " [-0.00322373]] , b: [ 0.00427248] , loss: 0.25\n",
      "w: [[-0.00394982]\n",
      " [-0.0032061 ]] , b: [ 0.00424735] , loss: 0.25\n",
      "w: [[-0.0039253 ]\n",
      " [-0.00318856]] , b: [ 0.00422236] , loss: 0.25\n",
      "w: [[-0.00390094]\n",
      " [-0.0031711 ]] , b: [ 0.00419753] , loss: 0.25\n",
      "w: [[-0.00387673]\n",
      " [-0.00315375]] , b: [ 0.00417284] , loss: 0.25\n",
      "w: [[-0.00385268]\n",
      " [-0.00313648]] , b: [ 0.00414829] , loss: 0.25\n",
      "w: [[-0.00382878]\n",
      " [-0.0031193 ]] , b: [ 0.00412389] , loss: 0.25\n",
      "w: [[-0.00380503]\n",
      " [-0.00310221]] , b: [ 0.00409963] , loss: 0.25\n",
      "w: [[-0.00378144]\n",
      " [-0.00308521]] , b: [ 0.00407552] , loss: 0.25\n",
      "w: [[-0.003758  ]\n",
      " [-0.00306829]] , b: [ 0.00405155] , loss: 0.25\n",
      "w: [[-0.0037347 ]\n",
      " [-0.00305147]] , b: [ 0.00402771] , loss: 0.25\n",
      "w: [[-0.00371155]\n",
      " [-0.00303473]] , b: [ 0.00400402] , loss: 0.25\n",
      "w: [[-0.00368855]\n",
      " [-0.00301807]] , b: [ 0.00398048] , loss: 0.25\n",
      "w: [[-0.00366569]\n",
      " [-0.00300151]] , b: [ 0.00395706] , loss: 0.25\n",
      "w: [[-0.00364299]\n",
      " [-0.00298503]] , b: [ 0.00393378] , loss: 0.25\n",
      "w: [[-0.00362042]\n",
      " [-0.00296864]] , b: [ 0.00391065] , loss: 0.25\n",
      "w: [[-0.003598  ]\n",
      " [-0.00295233]] , b: [ 0.00388764] , loss: 0.25\n",
      "w: [[-0.00357572]\n",
      " [-0.00293611]] , b: [ 0.00386477] , loss: 0.25\n",
      "w: [[-0.00355358]\n",
      " [-0.00291997]] , b: [ 0.00384203] , loss: 0.25\n",
      "w: [[-0.00353158]\n",
      " [-0.00290391]] , b: [ 0.00381943] , loss: 0.25\n",
      "w: [[-0.00350972]\n",
      " [-0.00288794]] , b: [ 0.00379696] , loss: 0.25\n",
      "w: [[-0.00348801]\n",
      " [-0.00287206]] , b: [ 0.00377462] , loss: 0.25\n",
      "w: [[-0.00346642]\n",
      " [-0.00285625]] , b: [ 0.00375242] , loss: 0.25\n",
      "w: [[-0.00344498]\n",
      " [-0.00284054]] , b: [ 0.00373035] , loss: 0.25\n",
      "w: [[-0.00342367]\n",
      " [-0.0028249 ]] , b: [ 0.00370841] , loss: 0.25\n",
      "w: [[-0.00340249]\n",
      " [-0.00280934]] , b: [ 0.0036866] , loss: 0.25\n",
      "w: [[-0.00338146]\n",
      " [-0.00279386]] , b: [ 0.00366492] , loss: 0.25\n",
      "w: [[-0.00336055]\n",
      " [-0.00277847]] , b: [ 0.00364336] , loss: 0.25\n",
      "w: [[-0.00333978]\n",
      " [-0.00276316]] , b: [ 0.00362192] , loss: 0.25\n",
      "w: [[-0.00331914]\n",
      " [-0.00274792]] , b: [ 0.00360062] , loss: 0.25\n",
      "w: [[-0.00329863]\n",
      " [-0.00273277]] , b: [ 0.00357944] , loss: 0.25\n",
      "w: [[-0.00327824]\n",
      " [-0.00271769]] , b: [ 0.00355839] , loss: 0.25\n",
      "w: [[-0.00325799]\n",
      " [-0.0027027 ]] , b: [ 0.00353745] , loss: 0.25\n",
      "w: [[-0.00323787]\n",
      " [-0.00268778]] , b: [ 0.00351664] , loss: 0.25\n",
      "w: [[-0.00321787]\n",
      " [-0.00267295]] , b: [ 0.00349595] , loss: 0.25\n",
      "w: [[-0.003198  ]\n",
      " [-0.00265819]] , b: [ 0.00347539] , loss: 0.25\n",
      "w: [[-0.00317826]\n",
      " [-0.00264351]] , b: [ 0.00345494] , loss: 0.25\n",
      "w: [[-0.00315864]\n",
      " [-0.0026289 ]] , b: [ 0.00343462] , loss: 0.25\n",
      "w: [[-0.00313914]\n",
      " [-0.00261438]] , b: [ 0.00341441] , loss: 0.25\n",
      "w: [[-0.00311977]\n",
      " [-0.00259993]] , b: [ 0.00339432] , loss: 0.25\n",
      "w: [[-0.00310052]\n",
      " [-0.00258556]] , b: [ 0.00337435] , loss: 0.25\n",
      "w: [[-0.00308139]\n",
      " [-0.00257125]] , b: [ 0.0033545] , loss: 0.25\n",
      "w: [[-0.00306238]\n",
      " [-0.00255703]] , b: [ 0.00333477] , loss: 0.25\n",
      "w: [[-0.00304349]\n",
      " [-0.00254288]] , b: [ 0.00331516] , loss: 0.25\n",
      "w: [[-0.00302472]\n",
      " [-0.0025288 ]] , b: [ 0.00329565] , loss: 0.25\n",
      "w: [[-0.00300607]\n",
      " [-0.0025148 ]] , b: [ 0.00327626] , loss: 0.25\n",
      "w: [[-0.00298754]\n",
      " [-0.00250088]] , b: [ 0.003257] , loss: 0.25\n",
      "w: [[-0.00296912]\n",
      " [-0.00248702]] , b: [ 0.00323783] , loss: 0.25\n",
      "w: [[-0.00295082]\n",
      " [-0.00247325]] , b: [ 0.00321879] , loss: 0.25\n",
      "w: [[-0.00293264]\n",
      " [-0.00245954]] , b: [ 0.00319985] , loss: 0.25\n",
      "w: [[-0.00291456]\n",
      " [-0.00244591]] , b: [ 0.00318103] , loss: 0.25\n",
      "w: [[-0.0028966 ]\n",
      " [-0.00243235]] , b: [ 0.00316232] , loss: 0.25\n",
      "w: [[-0.00287876]\n",
      " [-0.00241885]] , b: [ 0.00314372] , loss: 0.25\n",
      "w: [[-0.00286103]\n",
      " [-0.00240544]] , b: [ 0.00312522] , loss: 0.25\n",
      "w: [[-0.00284341]\n",
      " [-0.00239209]] , b: [ 0.00310684] , loss: 0.25\n",
      "w: [[-0.00282591]\n",
      " [-0.00237882]] , b: [ 0.00308856] , loss: 0.25\n",
      "w: [[-0.00280851]\n",
      " [-0.00236561]] , b: [ 0.00307039] , loss: 0.25\n",
      "w: [[-0.00279122]\n",
      " [-0.00235248]] , b: [ 0.00305233] , loss: 0.25\n",
      "w: [[-0.00277404]\n",
      " [-0.00233942]] , b: [ 0.00303437] , loss: 0.25\n",
      "w: [[-0.00275697]\n",
      " [-0.00232642]] , b: [ 0.00301652] , loss: 0.25\n",
      "w: [[-0.00274001]\n",
      " [-0.00231349]] , b: [ 0.00299878] , loss: 0.25\n",
      "w: [[-0.00272315]\n",
      " [-0.00230064]] , b: [ 0.00298113] , loss: 0.25\n",
      "w: [[-0.0027064 ]\n",
      " [-0.00228786]] , b: [ 0.00296359] , loss: 0.25\n",
      "w: [[-0.00268976]\n",
      " [-0.00227514]] , b: [ 0.00294616] , loss: 0.25\n",
      "w: [[-0.00267322]\n",
      " [-0.00226248]] , b: [ 0.00292883] , loss: 0.25\n",
      "w: [[-0.00265678]\n",
      " [-0.0022499 ]] , b: [ 0.0029116] , loss: 0.25\n",
      "w: [[-0.00264045]\n",
      " [-0.00223739]] , b: [ 0.00289446] , loss: 0.25\n",
      "w: [[-0.00262422]\n",
      " [-0.00222494]] , b: [ 0.00287743] , loss: 0.25\n",
      "w: [[-0.00260809]\n",
      " [-0.00221256]] , b: [ 0.0028605] , loss: 0.25\n",
      "w: [[-0.00259206]\n",
      " [-0.00220023]] , b: [ 0.00284368] , loss: 0.25\n",
      "w: [[-0.00257614]\n",
      " [-0.00218798]] , b: [ 0.00282695] , loss: 0.25\n",
      "w: [[-0.00256031]\n",
      " [-0.0021758 ]] , b: [ 0.00281032] , loss: 0.25\n",
      "w: [[-0.00254458]\n",
      " [-0.00216368]] , b: [ 0.00279378] , loss: 0.25\n",
      "w: [[-0.00252895]\n",
      " [-0.00215162]] , b: [ 0.00277735] , loss: 0.25\n",
      "w: [[-0.00251342]\n",
      " [-0.00213963]] , b: [ 0.00276101] , loss: 0.25\n",
      "w: [[-0.00249798]\n",
      " [-0.0021277 ]] , b: [ 0.00274477] , loss: 0.25\n",
      "w: [[-0.00248265]\n",
      " [-0.00211584]] , b: [ 0.00272861] , loss: 0.25\n",
      "w: [[-0.00246741]\n",
      " [-0.00210404]] , b: [ 0.00271256] , loss: 0.25\n",
      "w: [[-0.00245227]\n",
      " [-0.00209231]] , b: [ 0.0026966] , loss: 0.25\n",
      "w: [[-0.00243722]\n",
      " [-0.00208063]] , b: [ 0.00268074] , loss: 0.25\n",
      "w: [[-0.00242227]\n",
      " [-0.00206902]] , b: [ 0.00266497] , loss: 0.25\n",
      "w: [[-0.0024074 ]\n",
      " [-0.00205747]] , b: [ 0.00264929] , loss: 0.25\n",
      "w: [[-0.00239263]\n",
      " [-0.00204599]] , b: [ 0.00263371] , loss: 0.25\n",
      "w: [[-0.00237796]\n",
      " [-0.00203456]] , b: [ 0.00261821] , loss: 0.25\n",
      "w: [[-0.00236338]\n",
      " [-0.0020232 ]] , b: [ 0.00260281] , loss: 0.25\n",
      "w: [[-0.00234889]\n",
      " [-0.00201191]] , b: [ 0.00258749] , loss: 0.25\n",
      "w: [[-0.00233449]\n",
      " [-0.00200067]] , b: [ 0.00257227] , loss: 0.25\n",
      "w: [[-0.00232018]\n",
      " [-0.00198949]] , b: [ 0.00255713] , loss: 0.25\n",
      "w: [[-0.00230595]\n",
      " [-0.00197837]] , b: [ 0.00254209] , loss: 0.25\n",
      "w: [[-0.00229182]\n",
      " [-0.00196731]] , b: [ 0.00252713] , loss: 0.25\n",
      "w: [[-0.00227777]\n",
      " [-0.00195631]] , b: [ 0.00251227] , loss: 0.25\n",
      "w: [[-0.00226381]\n",
      " [-0.00194536]] , b: [ 0.00249749] , loss: 0.25\n",
      "w: [[-0.00224994]\n",
      " [-0.00193448]] , b: [ 0.00248281] , loss: 0.25\n",
      "w: [[-0.00223616]\n",
      " [-0.00192365]] , b: [ 0.0024682] , loss: 0.25\n",
      "w: [[-0.00222246]\n",
      " [-0.00191289]] , b: [ 0.00245368] , loss: 0.25\n",
      "w: [[-0.00220884]\n",
      " [-0.00190218]] , b: [ 0.00243925] , loss: 0.25\n",
      "w: [[-0.00219532]\n",
      " [-0.00189153]] , b: [ 0.0024249] , loss: 0.25\n",
      "w: [[-0.00218188]\n",
      " [-0.00188094]] , b: [ 0.00241063] , loss: 0.25\n",
      "w: [[-0.00216852]\n",
      " [-0.0018704 ]] , b: [ 0.00239645] , loss: 0.25\n",
      "w: [[-0.00215525]\n",
      " [-0.00185992]] , b: [ 0.00238235] , loss: 0.25\n",
      "w: [[-0.00214205]\n",
      " [-0.0018495 ]] , b: [ 0.00236834] , loss: 0.25\n",
      "w: [[-0.00212895]\n",
      " [-0.00183913]] , b: [ 0.0023544] , loss: 0.25\n",
      "w: [[-0.00211591]\n",
      " [-0.00182882]] , b: [ 0.00234055] , loss: 0.25\n",
      "w: [[-0.00210297]\n",
      " [-0.00181857]] , b: [ 0.00232678] , loss: 0.25\n",
      "w: [[-0.0020901 ]\n",
      " [-0.00180837]] , b: [ 0.0023131] , loss: 0.25\n",
      "w: [[-0.00207732]\n",
      " [-0.00179823]] , b: [ 0.00229949] , loss: 0.25\n",
      "w: [[-0.00206461]\n",
      " [-0.00178814]] , b: [ 0.00228596] , loss: 0.25\n",
      "w: [[-0.00205199]\n",
      " [-0.00177811]] , b: [ 0.00227251] , loss: 0.25\n",
      "w: [[-0.00203944]\n",
      " [-0.00176813]] , b: [ 0.00225913] , loss: 0.25\n",
      "w: [[-0.00202697]\n",
      " [-0.0017582 ]] , b: [ 0.00224584] , loss: 0.25\n",
      "w: [[-0.00201458]\n",
      " [-0.00174833]] , b: [ 0.00223264] , loss: 0.25\n",
      "w: [[-0.00200226]\n",
      " [-0.00173851]] , b: [ 0.00221951] , loss: 0.25\n",
      "w: [[-0.00199002]\n",
      " [-0.00172875]] , b: [ 0.00220645] , loss: 0.25\n",
      "w: [[-0.00197786]\n",
      " [-0.00171904]] , b: [ 0.00219347] , loss: 0.25\n",
      "w: [[-0.00196578]\n",
      " [-0.00170938]] , b: [ 0.00218057] , loss: 0.25\n",
      "w: [[-0.00195377]\n",
      " [-0.00169978]] , b: [ 0.00216774] , loss: 0.25\n",
      "w: [[-0.00194184]\n",
      " [-0.00169023]] , b: [ 0.00215498] , loss: 0.25\n",
      "w: [[-0.00192998]\n",
      " [-0.00168073]] , b: [ 0.0021423] , loss: 0.25\n",
      "w: [[-0.00191819]\n",
      " [-0.00167128]] , b: [ 0.0021297] , loss: 0.25\n",
      "w: [[-0.00190647]\n",
      " [-0.00166188]] , b: [ 0.00211717] , loss: 0.25\n",
      "w: [[-0.00189483]\n",
      " [-0.00165254]] , b: [ 0.00210471] , loss: 0.25\n",
      "w: [[-0.00188326]\n",
      " [-0.00164324]] , b: [ 0.00209233] , loss: 0.25\n",
      "w: [[-0.00187176]\n",
      " [-0.00163399]] , b: [ 0.00208002] , loss: 0.25\n",
      "w: [[-0.00186034]\n",
      " [-0.00162481]] , b: [ 0.00206778] , loss: 0.25\n",
      "w: [[-0.00184899]\n",
      " [-0.00161566]] , b: [ 0.00205561] , loss: 0.25\n",
      "w: [[-0.00183771]\n",
      " [-0.00160657]] , b: [ 0.00204352] , loss: 0.25\n",
      "w: [[-0.00182649]\n",
      " [-0.00159752]] , b: [ 0.00203149] , loss: 0.25\n",
      "w: [[-0.00181535]\n",
      " [-0.00158853]] , b: [ 0.00201954] , loss: 0.25\n",
      "w: [[-0.00180428]\n",
      " [-0.00157959]] , b: [ 0.00200765] , loss: 0.25\n",
      "w: [[-0.00179328]\n",
      " [-0.00157069]] , b: [ 0.00199584] , loss: 0.25\n",
      "w: [[-0.00178235]\n",
      " [-0.00156185]] , b: [ 0.00198409] , loss: 0.25\n",
      "w: [[-0.00177148]\n",
      " [-0.00155305]] , b: [ 0.00197241] , loss: 0.25\n",
      "w: [[-0.00176068]\n",
      " [-0.0015443 ]] , b: [ 0.0019608] , loss: 0.25\n",
      "w: [[-0.00174994]\n",
      " [-0.00153559]] , b: [ 0.00194927] , loss: 0.25\n",
      "w: [[-0.00173928]\n",
      " [-0.00152694]] , b: [ 0.0019378] , loss: 0.25\n",
      "w: [[-0.00172867]\n",
      " [-0.00151833]] , b: [ 0.0019264] , loss: 0.25\n",
      "w: [[-0.00171814]\n",
      " [-0.00150977]] , b: [ 0.00191506] , loss: 0.25\n",
      "w: [[-0.00170767]\n",
      " [-0.00150125]] , b: [ 0.0019038] , loss: 0.25\n",
      "w: [[-0.00169726]\n",
      " [-0.00149278]] , b: [ 0.0018926] , loss: 0.25\n",
      "w: [[-0.00168692]\n",
      " [-0.00148435]] , b: [ 0.00188147] , loss: 0.25\n",
      "w: [[-0.00167664]\n",
      " [-0.00147597]] , b: [ 0.00187041] , loss: 0.25\n",
      "w: [[-0.00166642]\n",
      " [-0.00146764]] , b: [ 0.0018594] , loss: 0.25\n",
      "w: [[-0.00165627]\n",
      " [-0.00145935]] , b: [ 0.00184846] , loss: 0.25\n",
      "w: [[-0.00164619]\n",
      " [-0.00145111]] , b: [ 0.00183759] , loss: 0.25\n",
      "w: [[-0.00163616]\n",
      " [-0.00144292]] , b: [ 0.00182678] , loss: 0.25\n",
      "w: [[-0.0016262 ]\n",
      " [-0.00143477]] , b: [ 0.00181603] , loss: 0.25\n",
      "w: [[-0.0016163 ]\n",
      " [-0.00142667]] , b: [ 0.00180534] , loss: 0.25\n",
      "w: [[-0.00160647]\n",
      " [-0.00141861]] , b: [ 0.00179472] , loss: 0.25\n",
      "w: [[-0.00159668]\n",
      " [-0.00141059]] , b: [ 0.00178416] , loss: 0.25\n",
      "w: [[-0.00158697]\n",
      " [-0.00140262]] , b: [ 0.00177366] , loss: 0.25\n",
      "w: [[-0.00157731]\n",
      " [-0.00139469]] , b: [ 0.00176322] , loss: 0.25\n",
      "w: [[-0.00156771]\n",
      " [-0.00138681]] , b: [ 0.00175285] , loss: 0.25\n",
      "w: [[-0.00155818]\n",
      " [-0.00137897]] , b: [ 0.00174253] , loss: 0.25\n",
      "w: [[-0.0015487 ]\n",
      " [-0.00137117]] , b: [ 0.00173229] , loss: 0.25\n",
      "w: [[-0.00153928]\n",
      " [-0.00136341]] , b: [ 0.00172209] , loss: 0.25\n",
      "w: [[-0.00152992]\n",
      " [-0.00135571]] , b: [ 0.00171196] , loss: 0.25\n",
      "w: [[-0.00152062]\n",
      " [-0.00134804]] , b: [ 0.00170189] , loss: 0.25\n",
      "w: [[-0.00151137]\n",
      " [-0.00134041]] , b: [ 0.00169187] , loss: 0.25\n",
      "w: [[-0.00150219]\n",
      " [-0.00133283]] , b: [ 0.00168192] , loss: 0.25\n",
      "w: [[-0.00149306]\n",
      " [-0.00132529]] , b: [ 0.00167203] , loss: 0.25\n",
      "w: [[-0.00148398]\n",
      " [-0.00131779]] , b: [ 0.00166219] , loss: 0.25\n",
      "w: [[-0.00147496]\n",
      " [-0.00131033]] , b: [ 0.00165241] , loss: 0.25\n",
      "w: [[-0.001466  ]\n",
      " [-0.00130291]] , b: [ 0.00164268] , loss: 0.25\n",
      "w: [[-0.0014571 ]\n",
      " [-0.00129553]] , b: [ 0.00163301] , loss: 0.25\n",
      "w: [[-0.00144825]\n",
      " [-0.0012882 ]] , b: [ 0.0016234] , loss: 0.25\n",
      "w: [[-0.00143945]\n",
      " [-0.0012809 ]] , b: [ 0.00161385] , loss: 0.25\n",
      "w: [[-0.0014307 ]\n",
      " [-0.00127364]] , b: [ 0.00160436] , loss: 0.25\n",
      "w: [[-0.00142201]\n",
      " [-0.00126643]] , b: [ 0.00159492] , loss: 0.25\n",
      "w: [[-0.00141337]\n",
      " [-0.00125925]] , b: [ 0.00158554] , loss: 0.25\n",
      "w: [[-0.00140479]\n",
      " [-0.00125211]] , b: [ 0.00157621] , loss: 0.25\n",
      "w: [[-0.00139626]\n",
      " [-0.00124501]] , b: [ 0.00156693] , loss: 0.25\n",
      "w: [[-0.00138779]\n",
      " [-0.00123796]] , b: [ 0.00155771] , loss: 0.25\n",
      "w: [[-0.00137936]\n",
      " [-0.00123094]] , b: [ 0.00154854] , loss: 0.25\n",
      "w: [[-0.00137098]\n",
      " [-0.00122396]] , b: [ 0.00153944] , loss: 0.25\n",
      "w: [[-0.00136266]\n",
      " [-0.00121701]] , b: [ 0.00153038] , loss: 0.25\n",
      "w: [[-0.00135438]\n",
      " [-0.00121011]] , b: [ 0.00152138] , loss: 0.25\n",
      "w: [[-0.00134616]\n",
      " [-0.00120324]] , b: [ 0.00151243] , loss: 0.25\n",
      "w: [[-0.00133799]\n",
      " [-0.00119641]] , b: [ 0.00150353] , loss: 0.25\n"
     ]
    }
   ],
   "source": [
    "#define placeholders for the data\n",
    "x = tf.placeholder(dtype=tf.float32,shape=[None,2])\n",
    "y = tf.placeholder(dtype=tf.float32,shape=[None,1])\n",
    "\n",
    "#define variables for the trainable parameters of the model\n",
    "w = tf.Variable(tf.random_normal([2,1]),name=\"weights\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "#create a tensor to calculate the model output\n",
    "y_model = 1/(1+tf.exp(-(tf.matmul(x,w) + b)))\n",
    "\n",
    "#define the loss function, create the optimizer and the training operation\n",
    "loss = tf.reduce_mean(tf.square(y_model-y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.3)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "#train the model\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(1000):\n",
    "    sess.run(train,{x:x_train,y:y_train})\n",
    "    print('w:',sess.run(w,{y:y_train,x:x_train}),', b:',sess.run(b,{y:y_train,x:x_train}),', loss:',sess.run(loss,{y:y_train,x:x_train}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the trained model parameters and the model outputs. What is the minimum found by the optimizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.50037587]\n",
      " [ 0.50007677]\n",
      " [ 0.50004137]\n",
      " [ 0.49974227]]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(y_model,{x:x_train}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron\n",
    "\n",
    "A multilayer perceptron is a feedforward network that can be thought of a model composed of multiple nested functions, for instance:\n",
    "\n",
    "$$y = f^{(3)}(f^{(2)}(f^{(1)}(x)))$$\n",
    "\n",
    "This means that the output of each function is routed as the input of the next function, and this operational and data flow is strictly one-directional (thus \"feedforward\") and may contain multiple layers of nested functions (thus \"deep\"). TensorFlow is a very suitable tool for building and training such models. Here we will consider the XOR problem once again, and build a multilayer perceptron to classify the data correctly.\n",
    "\n",
    "It was demonstrated previously that the XOR data are not linearly separable - this means that a non-linear layer (function) within the model is needed to tranform the problem to a linearly separable space. This is in fact the core of the multilayer perceptron as well as other deep learning models - nonlinear activation functions such as the logistic function, $tanh$, or ReLU. A comprehensive guide for TensorFlow supported functions can be found in: https://www.tensorflow.org/versions/r0.12/api_docs/python/nn/activation_functions_.\n",
    "\n",
    "Let us build a multilayer perceptron model where the sigmoid activation function is used for the hiddern layer. Let:\n",
    "- $f^{(1)}(x) = W^{(1)}x + b^{(1)}$\n",
    "- $f^{(2)}(x) = {1}/({1+e^{-x}})$\n",
    "- $f^{(3)}(x) = W^{(2)}x + b^{(2)}$\n",
    "\n",
    "with $W^{(1)} \\in \\mathbb{R}^{2\\times 2}$, $b^{(1)} \\in \\mathbb{R}^{2\\times 1}$, $W^{(2)} \\in \\mathbb{R}^{2\\times 1}$, and $b^{(2)} \\in \\mathbb{R}$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.750267\n",
      "loss: 0.313003\n",
      "loss: 0.258538\n",
      "loss: 0.251444\n",
      "loss: 0.250557\n",
      "loss: 0.250435\n",
      "loss: 0.250409\n",
      "loss: 0.250394\n",
      "loss: 0.250381\n",
      "loss: 0.250369\n",
      "loss: 0.250357\n",
      "loss: 0.250345\n",
      "loss: 0.250333\n",
      "loss: 0.250321\n",
      "loss: 0.250309\n",
      "loss: 0.250297\n",
      "loss: 0.250286\n",
      "loss: 0.250274\n",
      "loss: 0.250263\n",
      "loss: 0.250252\n",
      "loss: 0.250241\n",
      "loss: 0.25023\n",
      "loss: 0.250219\n",
      "loss: 0.250208\n",
      "loss: 0.250197\n",
      "loss: 0.250187\n",
      "loss: 0.250176\n",
      "loss: 0.250166\n",
      "loss: 0.250155\n",
      "loss: 0.250145\n",
      "loss: 0.250135\n",
      "loss: 0.250125\n",
      "loss: 0.250115\n",
      "loss: 0.250105\n",
      "loss: 0.250095\n",
      "loss: 0.250085\n",
      "loss: 0.250076\n",
      "loss: 0.250066\n",
      "loss: 0.250056\n",
      "loss: 0.250047\n",
      "loss: 0.250038\n",
      "loss: 0.250028\n",
      "loss: 0.250019\n",
      "loss: 0.25001\n",
      "loss: 0.250001\n",
      "loss: 0.249992\n",
      "loss: 0.249983\n",
      "loss: 0.249974\n",
      "loss: 0.249965\n",
      "loss: 0.249956\n",
      "loss: 0.249947\n",
      "loss: 0.249938\n",
      "loss: 0.24993\n",
      "loss: 0.249921\n",
      "loss: 0.249913\n",
      "loss: 0.249904\n",
      "loss: 0.249896\n",
      "loss: 0.249887\n",
      "loss: 0.249879\n",
      "loss: 0.249871\n",
      "loss: 0.249862\n",
      "loss: 0.249854\n",
      "loss: 0.249846\n",
      "loss: 0.249838\n",
      "loss: 0.24983\n",
      "loss: 0.249822\n",
      "loss: 0.249813\n",
      "loss: 0.249805\n",
      "loss: 0.249798\n",
      "loss: 0.24979\n",
      "loss: 0.249782\n",
      "loss: 0.249774\n",
      "loss: 0.249766\n",
      "loss: 0.249758\n",
      "loss: 0.249751\n",
      "loss: 0.249743\n",
      "loss: 0.249735\n",
      "loss: 0.249728\n",
      "loss: 0.24972\n",
      "loss: 0.249712\n",
      "loss: 0.249705\n",
      "loss: 0.249697\n",
      "loss: 0.24969\n",
      "loss: 0.249682\n",
      "loss: 0.249675\n",
      "loss: 0.249667\n",
      "loss: 0.24966\n",
      "loss: 0.249653\n",
      "loss: 0.249645\n",
      "loss: 0.249638\n",
      "loss: 0.24963\n",
      "loss: 0.249623\n",
      "loss: 0.249616\n",
      "loss: 0.249609\n",
      "loss: 0.249601\n",
      "loss: 0.249594\n",
      "loss: 0.249587\n",
      "loss: 0.24958\n",
      "loss: 0.249572\n",
      "loss: 0.249565\n",
      "loss: 0.249558\n",
      "loss: 0.249551\n",
      "loss: 0.249544\n",
      "loss: 0.249537\n",
      "loss: 0.249529\n",
      "loss: 0.249522\n",
      "loss: 0.249515\n",
      "loss: 0.249508\n",
      "loss: 0.249501\n",
      "loss: 0.249494\n",
      "loss: 0.249487\n",
      "loss: 0.24948\n",
      "loss: 0.249473\n",
      "loss: 0.249466\n",
      "loss: 0.249459\n",
      "loss: 0.249452\n",
      "loss: 0.249445\n",
      "loss: 0.249438\n",
      "loss: 0.24943\n",
      "loss: 0.249423\n",
      "loss: 0.249416\n",
      "loss: 0.249409\n",
      "loss: 0.249402\n",
      "loss: 0.249395\n",
      "loss: 0.249388\n",
      "loss: 0.249381\n",
      "loss: 0.249374\n",
      "loss: 0.249367\n",
      "loss: 0.24936\n",
      "loss: 0.249353\n",
      "loss: 0.249346\n",
      "loss: 0.249339\n",
      "loss: 0.249332\n",
      "loss: 0.249325\n",
      "loss: 0.249318\n",
      "loss: 0.249311\n",
      "loss: 0.249304\n",
      "loss: 0.249296\n",
      "loss: 0.249289\n",
      "loss: 0.249282\n",
      "loss: 0.249275\n",
      "loss: 0.249268\n",
      "loss: 0.249261\n",
      "loss: 0.249254\n",
      "loss: 0.249247\n",
      "loss: 0.249239\n",
      "loss: 0.249232\n",
      "loss: 0.249225\n",
      "loss: 0.249218\n",
      "loss: 0.249211\n",
      "loss: 0.249203\n",
      "loss: 0.249196\n",
      "loss: 0.249189\n",
      "loss: 0.249181\n",
      "loss: 0.249174\n",
      "loss: 0.249167\n",
      "loss: 0.24916\n",
      "loss: 0.249152\n",
      "loss: 0.249145\n",
      "loss: 0.249137\n",
      "loss: 0.24913\n",
      "loss: 0.249123\n",
      "loss: 0.249115\n",
      "loss: 0.249108\n",
      "loss: 0.2491\n",
      "loss: 0.249093\n",
      "loss: 0.249085\n",
      "loss: 0.249077\n",
      "loss: 0.24907\n",
      "loss: 0.249062\n",
      "loss: 0.249055\n",
      "loss: 0.249047\n",
      "loss: 0.249039\n",
      "loss: 0.249032\n",
      "loss: 0.249024\n",
      "loss: 0.249016\n",
      "loss: 0.249008\n",
      "loss: 0.249\n",
      "loss: 0.248993\n",
      "loss: 0.248985\n",
      "loss: 0.248977\n",
      "loss: 0.248969\n",
      "loss: 0.248961\n",
      "loss: 0.248953\n",
      "loss: 0.248945\n",
      "loss: 0.248937\n",
      "loss: 0.248928\n",
      "loss: 0.24892\n",
      "loss: 0.248912\n",
      "loss: 0.248904\n",
      "loss: 0.248896\n",
      "loss: 0.248887\n",
      "loss: 0.248879\n",
      "loss: 0.248871\n",
      "loss: 0.248862\n",
      "loss: 0.248854\n",
      "loss: 0.248845\n",
      "loss: 0.248837\n",
      "loss: 0.248828\n",
      "loss: 0.248819\n",
      "loss: 0.248811\n",
      "loss: 0.248802\n",
      "loss: 0.248793\n",
      "loss: 0.248784\n",
      "loss: 0.248776\n",
      "loss: 0.248767\n",
      "loss: 0.248758\n",
      "loss: 0.248749\n",
      "loss: 0.24874\n",
      "loss: 0.248731\n",
      "loss: 0.248722\n",
      "loss: 0.248712\n",
      "loss: 0.248703\n",
      "loss: 0.248694\n",
      "loss: 0.248684\n",
      "loss: 0.248675\n",
      "loss: 0.248666\n",
      "loss: 0.248656\n",
      "loss: 0.248646\n",
      "loss: 0.248637\n",
      "loss: 0.248627\n",
      "loss: 0.248617\n",
      "loss: 0.248608\n",
      "loss: 0.248598\n",
      "loss: 0.248588\n",
      "loss: 0.248578\n",
      "loss: 0.248568\n",
      "loss: 0.248558\n",
      "loss: 0.248547\n",
      "loss: 0.248537\n",
      "loss: 0.248527\n",
      "loss: 0.248517\n",
      "loss: 0.248506\n",
      "loss: 0.248496\n",
      "loss: 0.248485\n",
      "loss: 0.248474\n",
      "loss: 0.248464\n",
      "loss: 0.248453\n",
      "loss: 0.248442\n",
      "loss: 0.248431\n",
      "loss: 0.24842\n",
      "loss: 0.248409\n",
      "loss: 0.248398\n",
      "loss: 0.248386\n",
      "loss: 0.248375\n",
      "loss: 0.248364\n",
      "loss: 0.248352\n",
      "loss: 0.248341\n",
      "loss: 0.248329\n",
      "loss: 0.248317\n",
      "loss: 0.248305\n",
      "loss: 0.248293\n",
      "loss: 0.248281\n",
      "loss: 0.248269\n",
      "loss: 0.248257\n",
      "loss: 0.248245\n",
      "loss: 0.248233\n",
      "loss: 0.24822\n",
      "loss: 0.248208\n",
      "loss: 0.248195\n",
      "loss: 0.248182\n",
      "loss: 0.248169\n",
      "loss: 0.248156\n",
      "loss: 0.248143\n",
      "loss: 0.24813\n",
      "loss: 0.248117\n",
      "loss: 0.248103\n",
      "loss: 0.24809\n",
      "loss: 0.248076\n",
      "loss: 0.248063\n",
      "loss: 0.248049\n",
      "loss: 0.248035\n",
      "loss: 0.248021\n",
      "loss: 0.248007\n",
      "loss: 0.247993\n",
      "loss: 0.247978\n",
      "loss: 0.247964\n",
      "loss: 0.247949\n",
      "loss: 0.247934\n",
      "loss: 0.24792\n",
      "loss: 0.247905\n",
      "loss: 0.247889\n",
      "loss: 0.247874\n",
      "loss: 0.247859\n",
      "loss: 0.247843\n",
      "loss: 0.247828\n",
      "loss: 0.247812\n",
      "loss: 0.247796\n",
      "loss: 0.24778\n",
      "loss: 0.247764\n",
      "loss: 0.247747\n",
      "loss: 0.247731\n",
      "loss: 0.247714\n",
      "loss: 0.247698\n",
      "loss: 0.247681\n",
      "loss: 0.247664\n",
      "loss: 0.247646\n",
      "loss: 0.247629\n",
      "loss: 0.247612\n",
      "loss: 0.247594\n",
      "loss: 0.247576\n",
      "loss: 0.247558\n",
      "loss: 0.24754\n",
      "loss: 0.247522\n",
      "loss: 0.247503\n",
      "loss: 0.247484\n",
      "loss: 0.247466\n",
      "loss: 0.247447\n",
      "loss: 0.247427\n",
      "loss: 0.247408\n",
      "loss: 0.247388\n",
      "loss: 0.247369\n",
      "loss: 0.247349\n",
      "loss: 0.247329\n",
      "loss: 0.247308\n",
      "loss: 0.247288\n",
      "loss: 0.247267\n",
      "loss: 0.247246\n",
      "loss: 0.247225\n",
      "loss: 0.247204\n",
      "loss: 0.247182\n",
      "loss: 0.247161\n",
      "loss: 0.247139\n",
      "loss: 0.247117\n",
      "loss: 0.247094\n",
      "loss: 0.247072\n",
      "loss: 0.247049\n",
      "loss: 0.247026\n",
      "loss: 0.247003\n",
      "loss: 0.246979\n",
      "loss: 0.246956\n",
      "loss: 0.246932\n",
      "loss: 0.246907\n",
      "loss: 0.246883\n",
      "loss: 0.246858\n",
      "loss: 0.246833\n",
      "loss: 0.246808\n",
      "loss: 0.246783\n",
      "loss: 0.246757\n",
      "loss: 0.246731\n",
      "loss: 0.246705\n",
      "loss: 0.246679\n",
      "loss: 0.246652\n",
      "loss: 0.246625\n",
      "loss: 0.246598\n",
      "loss: 0.24657\n",
      "loss: 0.246542\n",
      "loss: 0.246514\n",
      "loss: 0.246486\n",
      "loss: 0.246457\n",
      "loss: 0.246428\n",
      "loss: 0.246399\n",
      "loss: 0.246369\n",
      "loss: 0.246339\n",
      "loss: 0.246309\n",
      "loss: 0.246278\n",
      "loss: 0.246248\n",
      "loss: 0.246216\n",
      "loss: 0.246185\n",
      "loss: 0.246153\n",
      "loss: 0.246121\n",
      "loss: 0.246088\n",
      "loss: 0.246055\n",
      "loss: 0.246022\n",
      "loss: 0.245989\n",
      "loss: 0.245955\n",
      "loss: 0.24592\n",
      "loss: 0.245886\n",
      "loss: 0.245851\n",
      "loss: 0.245815\n",
      "loss: 0.24578\n",
      "loss: 0.245743\n",
      "loss: 0.245707\n",
      "loss: 0.24567\n",
      "loss: 0.245633\n",
      "loss: 0.245595\n",
      "loss: 0.245557\n",
      "loss: 0.245518\n",
      "loss: 0.245479\n",
      "loss: 0.24544\n",
      "loss: 0.2454\n",
      "loss: 0.245359\n",
      "loss: 0.245319\n",
      "loss: 0.245278\n",
      "loss: 0.245236\n",
      "loss: 0.245194\n",
      "loss: 0.245151\n",
      "loss: 0.245108\n",
      "loss: 0.245065\n",
      "loss: 0.245021\n",
      "loss: 0.244977\n",
      "loss: 0.244932\n",
      "loss: 0.244886\n",
      "loss: 0.24484\n",
      "loss: 0.244794\n",
      "loss: 0.244747\n",
      "loss: 0.244699\n",
      "loss: 0.244651\n",
      "loss: 0.244603\n",
      "loss: 0.244554\n",
      "loss: 0.244504\n",
      "loss: 0.244454\n",
      "loss: 0.244403\n",
      "loss: 0.244352\n",
      "loss: 0.2443\n",
      "loss: 0.244248\n",
      "loss: 0.244195\n",
      "loss: 0.244141\n",
      "loss: 0.244087\n",
      "loss: 0.244032\n",
      "loss: 0.243977\n",
      "loss: 0.243921\n",
      "loss: 0.243864\n",
      "loss: 0.243807\n",
      "loss: 0.243749\n",
      "loss: 0.243691\n",
      "loss: 0.243631\n",
      "loss: 0.243571\n",
      "loss: 0.243511\n",
      "loss: 0.24345\n",
      "loss: 0.243388\n",
      "loss: 0.243325\n",
      "loss: 0.243262\n",
      "loss: 0.243198\n",
      "loss: 0.243133\n",
      "loss: 0.243068\n",
      "loss: 0.243002\n",
      "loss: 0.242935\n",
      "loss: 0.242867\n",
      "loss: 0.242799\n",
      "loss: 0.242729\n",
      "loss: 0.242659\n",
      "loss: 0.242589\n",
      "loss: 0.242517\n",
      "loss: 0.242445\n",
      "loss: 0.242372\n",
      "loss: 0.242298\n",
      "loss: 0.242223\n",
      "loss: 0.242147\n",
      "loss: 0.242071\n",
      "loss: 0.241993\n",
      "loss: 0.241915\n",
      "loss: 0.241836\n",
      "loss: 0.241756\n",
      "loss: 0.241675\n",
      "loss: 0.241594\n",
      "loss: 0.241511\n",
      "loss: 0.241427\n",
      "loss: 0.241343\n",
      "loss: 0.241258\n",
      "loss: 0.241171\n",
      "loss: 0.241084\n",
      "loss: 0.240996\n",
      "loss: 0.240906\n",
      "loss: 0.240816\n",
      "loss: 0.240725\n",
      "loss: 0.240633\n",
      "loss: 0.240539\n",
      "loss: 0.240445\n",
      "loss: 0.24035\n",
      "loss: 0.240253\n",
      "loss: 0.240156\n",
      "loss: 0.240057\n",
      "loss: 0.239958\n",
      "loss: 0.239857\n",
      "loss: 0.239755\n",
      "loss: 0.239652\n",
      "loss: 0.239549\n",
      "loss: 0.239443\n",
      "loss: 0.239337\n",
      "loss: 0.23923\n",
      "loss: 0.239121\n",
      "loss: 0.239011\n",
      "loss: 0.2389\n",
      "loss: 0.238788\n",
      "loss: 0.238675\n",
      "loss: 0.238561\n",
      "loss: 0.238445\n",
      "loss: 0.238328\n",
      "loss: 0.238209\n",
      "loss: 0.23809\n",
      "loss: 0.237969\n",
      "loss: 0.237847\n",
      "loss: 0.237724\n",
      "loss: 0.237599\n",
      "loss: 0.237473\n",
      "loss: 0.237346\n",
      "loss: 0.237217\n",
      "loss: 0.237087\n",
      "loss: 0.236956\n",
      "loss: 0.236823\n",
      "loss: 0.236689\n",
      "loss: 0.236553\n",
      "loss: 0.236416\n",
      "loss: 0.236278\n",
      "loss: 0.236138\n",
      "loss: 0.235997\n",
      "loss: 0.235854\n",
      "loss: 0.23571\n",
      "loss: 0.235565\n",
      "loss: 0.235418\n",
      "loss: 0.235269\n",
      "loss: 0.235119\n",
      "loss: 0.234967\n",
      "loss: 0.234814\n",
      "loss: 0.23466\n",
      "loss: 0.234503\n",
      "loss: 0.234345\n",
      "loss: 0.234186\n",
      "loss: 0.234025\n",
      "loss: 0.233863\n",
      "loss: 0.233698\n",
      "loss: 0.233533\n",
      "loss: 0.233365\n",
      "loss: 0.233196\n",
      "loss: 0.233025\n",
      "loss: 0.232853\n",
      "loss: 0.232679\n",
      "loss: 0.232503\n",
      "loss: 0.232326\n",
      "loss: 0.232147\n",
      "loss: 0.231966\n",
      "loss: 0.231783\n",
      "loss: 0.231599\n",
      "loss: 0.231413\n",
      "loss: 0.231225\n",
      "loss: 0.231035\n",
      "loss: 0.230844\n",
      "loss: 0.23065\n",
      "loss: 0.230455\n",
      "loss: 0.230258\n",
      "loss: 0.23006\n",
      "loss: 0.229859\n",
      "loss: 0.229657\n",
      "loss: 0.229453\n",
      "loss: 0.229247\n",
      "loss: 0.229039\n",
      "loss: 0.228829\n",
      "loss: 0.228617\n",
      "loss: 0.228403\n",
      "loss: 0.228188\n",
      "loss: 0.22797\n",
      "loss: 0.227751\n",
      "loss: 0.22753\n",
      "loss: 0.227306\n",
      "loss: 0.227081\n",
      "loss: 0.226854\n",
      "loss: 0.226624\n",
      "loss: 0.226393\n",
      "loss: 0.22616\n",
      "loss: 0.225925\n",
      "loss: 0.225687\n",
      "loss: 0.225448\n",
      "loss: 0.225207\n",
      "loss: 0.224964\n",
      "loss: 0.224718\n",
      "loss: 0.224471\n",
      "loss: 0.224221\n",
      "loss: 0.223969\n",
      "loss: 0.223716\n",
      "loss: 0.22346\n",
      "loss: 0.223202\n",
      "loss: 0.222942\n",
      "loss: 0.22268\n",
      "loss: 0.222416\n",
      "loss: 0.222149\n",
      "loss: 0.221881\n",
      "loss: 0.22161\n",
      "loss: 0.221337\n",
      "loss: 0.221062\n",
      "loss: 0.220785\n",
      "loss: 0.220506\n",
      "loss: 0.220224\n",
      "loss: 0.219941\n",
      "loss: 0.219655\n",
      "loss: 0.219367\n",
      "loss: 0.219076\n",
      "loss: 0.218784\n",
      "loss: 0.218489\n",
      "loss: 0.218192\n",
      "loss: 0.217893\n",
      "loss: 0.217591\n",
      "loss: 0.217288\n",
      "loss: 0.216982\n",
      "loss: 0.216673\n",
      "loss: 0.216363\n",
      "loss: 0.21605\n",
      "loss: 0.215735\n",
      "loss: 0.215418\n",
      "loss: 0.215098\n",
      "loss: 0.214776\n",
      "loss: 0.214452\n",
      "loss: 0.214125\n",
      "loss: 0.213796\n",
      "loss: 0.213465\n",
      "loss: 0.213131\n",
      "loss: 0.212795\n",
      "loss: 0.212457\n",
      "loss: 0.212116\n",
      "loss: 0.211773\n",
      "loss: 0.211428\n",
      "loss: 0.21108\n",
      "loss: 0.21073\n",
      "loss: 0.210377\n",
      "loss: 0.210022\n",
      "loss: 0.209665\n",
      "loss: 0.209305\n",
      "loss: 0.208943\n",
      "loss: 0.208578\n",
      "loss: 0.208211\n",
      "loss: 0.207841\n",
      "loss: 0.207469\n",
      "loss: 0.207095\n",
      "loss: 0.206718\n",
      "loss: 0.206338\n",
      "loss: 0.205957\n",
      "loss: 0.205572\n",
      "loss: 0.205185\n",
      "loss: 0.204796\n",
      "loss: 0.204404\n",
      "loss: 0.204009\n",
      "loss: 0.203612\n",
      "loss: 0.203213\n",
      "loss: 0.20281\n",
      "loss: 0.202406\n",
      "loss: 0.201998\n",
      "loss: 0.201588\n",
      "loss: 0.201176\n",
      "loss: 0.200761\n",
      "loss: 0.200343\n",
      "loss: 0.199922\n",
      "loss: 0.199499\n",
      "loss: 0.199073\n",
      "loss: 0.198645\n",
      "loss: 0.198214\n",
      "loss: 0.19778\n",
      "loss: 0.197343\n",
      "loss: 0.196904\n",
      "loss: 0.196462\n",
      "loss: 0.196017\n",
      "loss: 0.195569\n",
      "loss: 0.195119\n",
      "loss: 0.194666\n",
      "loss: 0.19421\n",
      "loss: 0.193751\n",
      "loss: 0.193289\n",
      "loss: 0.192824\n",
      "loss: 0.192357\n",
      "loss: 0.191886\n",
      "loss: 0.191413\n",
      "loss: 0.190936\n",
      "loss: 0.190457\n",
      "loss: 0.189975\n",
      "loss: 0.189489\n",
      "loss: 0.189001\n",
      "loss: 0.18851\n",
      "loss: 0.188015\n",
      "loss: 0.187518\n",
      "loss: 0.187017\n",
      "loss: 0.186513\n",
      "loss: 0.186006\n",
      "loss: 0.185496\n",
      "loss: 0.184983\n",
      "loss: 0.184466\n",
      "loss: 0.183946\n",
      "loss: 0.183423\n",
      "loss: 0.182896\n",
      "loss: 0.182367\n",
      "loss: 0.181833\n",
      "loss: 0.181297\n",
      "loss: 0.180757\n",
      "loss: 0.180214\n",
      "loss: 0.179667\n",
      "loss: 0.179116\n",
      "loss: 0.178563\n",
      "loss: 0.178005\n",
      "loss: 0.177444\n",
      "loss: 0.17688\n",
      "loss: 0.176311\n",
      "loss: 0.17574\n",
      "loss: 0.175164\n",
      "loss: 0.174585\n",
      "loss: 0.174002\n",
      "loss: 0.173415\n",
      "loss: 0.172824\n",
      "loss: 0.17223\n",
      "loss: 0.171632\n",
      "loss: 0.171029\n",
      "loss: 0.170423\n",
      "loss: 0.169813\n",
      "loss: 0.169199\n",
      "loss: 0.168581\n",
      "loss: 0.167959\n",
      "loss: 0.167333\n",
      "loss: 0.166702\n",
      "loss: 0.166068\n",
      "loss: 0.165429\n",
      "loss: 0.164786\n",
      "loss: 0.164139\n",
      "loss: 0.163487\n",
      "loss: 0.162832\n",
      "loss: 0.162171\n",
      "loss: 0.161507\n",
      "loss: 0.160838\n",
      "loss: 0.160165\n",
      "loss: 0.159487\n",
      "loss: 0.158805\n",
      "loss: 0.158118\n",
      "loss: 0.157427\n",
      "loss: 0.156731\n",
      "loss: 0.156031\n",
      "loss: 0.155326\n",
      "loss: 0.154616\n",
      "loss: 0.153902\n",
      "loss: 0.153183\n",
      "loss: 0.152459\n",
      "loss: 0.151731\n",
      "loss: 0.150998\n",
      "loss: 0.15026\n",
      "loss: 0.149517\n",
      "loss: 0.14877\n",
      "loss: 0.148018\n",
      "loss: 0.147261\n",
      "loss: 0.146499\n",
      "loss: 0.145732\n",
      "loss: 0.14496\n",
      "loss: 0.144184\n",
      "loss: 0.143402\n",
      "loss: 0.142616\n",
      "loss: 0.141825\n",
      "loss: 0.141029\n",
      "loss: 0.140228\n",
      "loss: 0.139422\n",
      "loss: 0.138612\n",
      "loss: 0.137796\n",
      "loss: 0.136976\n",
      "loss: 0.136151\n",
      "loss: 0.13532\n",
      "loss: 0.134486\n",
      "loss: 0.133646\n",
      "loss: 0.132801\n",
      "loss: 0.131952\n",
      "loss: 0.131098\n",
      "loss: 0.130239\n",
      "loss: 0.129375\n",
      "loss: 0.128507\n",
      "loss: 0.127634\n",
      "loss: 0.126757\n",
      "loss: 0.125875\n",
      "loss: 0.124988\n",
      "loss: 0.124097\n",
      "loss: 0.123202\n",
      "loss: 0.122302\n",
      "loss: 0.121398\n",
      "loss: 0.120489\n",
      "loss: 0.119576\n",
      "loss: 0.11866\n",
      "loss: 0.117739\n",
      "loss: 0.116814\n",
      "loss: 0.115885\n",
      "loss: 0.114952\n",
      "loss: 0.114015\n",
      "loss: 0.113075\n",
      "loss: 0.112131\n",
      "loss: 0.111183\n",
      "loss: 0.110233\n",
      "loss: 0.109278\n",
      "loss: 0.10832\n",
      "loss: 0.10736\n",
      "loss: 0.106396\n",
      "loss: 0.105429\n",
      "loss: 0.104459\n",
      "loss: 0.103487\n",
      "loss: 0.102512\n",
      "loss: 0.101534\n",
      "loss: 0.100554\n",
      "loss: 0.0995717\n",
      "loss: 0.0985873\n",
      "loss: 0.0976008\n",
      "loss: 0.0966126\n",
      "loss: 0.0956226\n",
      "loss: 0.0946311\n",
      "loss: 0.093638\n",
      "loss: 0.0926437\n",
      "loss: 0.0916482\n",
      "loss: 0.0906518\n",
      "loss: 0.0896546\n",
      "loss: 0.0886566\n",
      "loss: 0.0876583\n",
      "loss: 0.0866595\n",
      "loss: 0.0856605\n",
      "loss: 0.0846615\n",
      "loss: 0.0836627\n",
      "loss: 0.0826644\n",
      "loss: 0.0816664\n",
      "loss: 0.0806692\n",
      "loss: 0.0796728\n",
      "loss: 0.0786775\n",
      "loss: 0.0776835\n",
      "loss: 0.0766909\n",
      "loss: 0.0756998\n",
      "loss: 0.0747107\n",
      "loss: 0.0737235\n",
      "loss: 0.0727384\n",
      "loss: 0.0717558\n",
      "loss: 0.0707757\n",
      "loss: 0.0697984\n",
      "loss: 0.068824\n",
      "loss: 0.0678528\n",
      "loss: 0.066885\n",
      "loss: 0.0659206\n",
      "loss: 0.06496\n",
      "loss: 0.0640032\n",
      "loss: 0.0630506\n",
      "loss: 0.0621023\n",
      "loss: 0.0611585\n",
      "loss: 0.0602192\n",
      "loss: 0.0592849\n",
      "loss: 0.0583557\n",
      "loss: 0.0574316\n",
      "loss: 0.0565129\n",
      "loss: 0.0555998\n",
      "loss: 0.0546925\n",
      "loss: 0.0537911\n",
      "loss: 0.0528958\n",
      "loss: 0.0520067\n",
      "loss: 0.0511242\n",
      "loss: 0.0502481\n",
      "loss: 0.0493789\n",
      "loss: 0.0485165\n",
      "loss: 0.0476611\n",
      "loss: 0.046813\n",
      "loss: 0.0459723\n",
      "loss: 0.0451389\n",
      "loss: 0.0443132\n",
      "loss: 0.0434953\n",
      "loss: 0.0426852\n",
      "loss: 0.0418831\n",
      "loss: 0.0410891\n",
      "loss: 0.0403034\n",
      "loss: 0.0395259\n",
      "loss: 0.038757\n",
      "loss: 0.0379965\n",
      "loss: 0.0372446\n",
      "loss: 0.0365014\n",
      "loss: 0.035767\n",
      "loss: 0.0350415\n",
      "loss: 0.0343248\n",
      "loss: 0.0336173\n",
      "loss: 0.0329187\n",
      "loss: 0.0322292\n",
      "loss: 0.0315489\n",
      "loss: 0.0308779\n",
      "loss: 0.0302159\n",
      "loss: 0.0295634\n",
      "loss: 0.0289201\n",
      "loss: 0.0282861\n",
      "loss: 0.0276616\n",
      "loss: 0.0270463\n",
      "loss: 0.0264403\n",
      "loss: 0.0258438\n",
      "loss: 0.0252566\n",
      "loss: 0.0246787\n",
      "loss: 0.0241102\n",
      "loss: 0.023551\n",
      "loss: 0.0230012\n",
      "loss: 0.0224605\n",
      "loss: 0.0219292\n",
      "loss: 0.021407\n",
      "loss: 0.020894\n",
      "loss: 0.0203901\n",
      "loss: 0.0198953\n",
      "loss: 0.0194096\n",
      "loss: 0.0189328\n",
      "loss: 0.0184649\n",
      "loss: 0.0180058\n",
      "loss: 0.0175556\n",
      "loss: 0.017114\n",
      "loss: 0.0166811\n",
      "loss: 0.0162567\n",
      "loss: 0.0158408\n",
      "loss: 0.0154334\n",
      "loss: 0.0150342\n",
      "loss: 0.0146434\n",
      "loss: 0.0142606\n",
      "loss: 0.0138859\n",
      "loss: 0.0135191\n",
      "loss: 0.0131603\n",
      "loss: 0.0128092\n",
      "loss: 0.0124657\n",
      "loss: 0.0121299\n",
      "loss: 0.0118015\n",
      "loss: 0.0114805\n",
      "loss: 0.0111668\n",
      "loss: 0.0108602\n",
      "loss: 0.0105607\n",
      "loss: 0.0102681\n",
      "loss: 0.00998243\n",
      "loss: 0.00970345\n",
      "loss: 0.00943111\n",
      "loss: 0.00916528\n",
      "loss: 0.00890592\n",
      "loss: 0.00865278\n",
      "loss: 0.00840588\n",
      "loss: 0.00816505\n",
      "loss: 0.00793023\n",
      "loss: 0.00770123\n",
      "loss: 0.00747802\n",
      "loss: 0.00726047\n",
      "loss: 0.00704846\n",
      "loss: 0.0068419\n",
      "loss: 0.00664064\n",
      "loss: 0.00644463\n",
      "loss: 0.00625373\n",
      "loss: 0.00606785\n",
      "loss: 0.00588689\n",
      "loss: 0.00571076\n",
      "loss: 0.00553929\n",
      "loss: 0.00537248\n",
      "loss: 0.00521013\n",
      "loss: 0.00505225\n",
      "loss: 0.00489863\n",
      "loss: 0.00474926\n",
      "loss: 0.00460402\n",
      "loss: 0.00446279\n",
      "loss: 0.00432549\n",
      "loss: 0.00419203\n",
      "loss: 0.00406235\n",
      "loss: 0.00393632\n",
      "loss: 0.00381385\n",
      "loss: 0.00369489\n",
      "loss: 0.00357934\n",
      "loss: 0.00346714\n",
      "loss: 0.00335814\n",
      "loss: 0.00325233\n",
      "loss: 0.00314957\n",
      "loss: 0.00304985\n",
      "loss: 0.00295303\n",
      "loss: 0.00285908\n",
      "loss: 0.00276791\n",
      "loss: 0.00267943\n",
      "loss: 0.00259359\n",
      "loss: 0.00251033\n",
      "loss: 0.00242955\n",
      "loss: 0.00235123\n",
      "loss: 0.00227525\n",
      "loss: 0.00220159\n",
      "loss: 0.00213017\n",
      "loss: 0.00206093\n",
      "loss: 0.00199381\n",
      "loss: 0.00192874\n",
      "loss: 0.00186568\n",
      "loss: 0.00180458\n",
      "loss: 0.00174536\n",
      "loss: 0.00168799\n",
      "loss: 0.0016324\n",
      "loss: 0.00157856\n",
      "loss: 0.00152639\n",
      "loss: 0.00147588\n",
      "loss: 0.00142694\n",
      "loss: 0.00137955\n",
      "loss: 0.00133365\n",
      "loss: 0.00128923\n",
      "loss: 0.00124622\n",
      "loss: 0.00120457\n",
      "loss: 0.00116427\n",
      "loss: 0.00112524\n",
      "loss: 0.00108748\n",
      "loss: 0.00105093\n",
      "loss: 0.00101556\n",
      "loss: 0.000981326\n",
      "loss: 0.000948217\n",
      "loss: 0.000916171\n",
      "loss: 0.000885175\n",
      "loss: 0.000855178\n",
      "loss: 0.000826171\n",
      "loss: 0.000798119\n",
      "loss: 0.000770982\n",
      "loss: 0.000744731\n",
      "loss: 0.000719352\n",
      "loss: 0.000694797\n",
      "loss: 0.000671066\n",
      "loss: 0.000648114\n",
      "loss: 0.000625925\n",
      "loss: 0.000604468\n",
      "loss: 0.000583735\n",
      "loss: 0.000563687\n",
      "loss: 0.00054431\n",
      "loss: 0.000525578\n",
      "loss: 0.000507473\n",
      "loss: 0.000489975\n",
      "loss: 0.000473061\n",
      "loss: 0.000456723\n",
      "loss: 0.000440935\n",
      "loss: 0.000425673\n",
      "loss: 0.000410926\n",
      "loss: 0.000396678\n",
      "loss: 0.000382916\n",
      "loss: 0.000369618\n",
      "loss: 0.000356776\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "x_train = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y_train = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "X = tf.placeholder(tf.float32,[None,2])\n",
    "y = tf.placeholder(tf.float32,[None,1])\n",
    "\n",
    "W1 = tf.Variable(tf.random_uniform([2,2]),name=\"weights1\")\n",
    "b1 = tf.Variable(tf.random_uniform([2]), name=\"bias1\")\n",
    "\n",
    "W2 = tf.Variable(tf.random_uniform([2,1]),name=\"weights2\")\n",
    "b2 = tf.Variable(tf.random_uniform([1]), name=\"bias2\")\n",
    "\n",
    "f1 = tf.matmul(X,W1)+b1\n",
    "f2 = tf.nn.sigmoid(f1)\n",
    "y_model = tf.matmul(f2,W2)+b2\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(y_model-y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.35)\n",
    "#optimizer = tf.train.AdamOptimizer(0.1)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(1000):\n",
    "    sess.run(train, feed_dict={X: x_train, y: y_train})\n",
    "    print(\"loss:\", sess.run(loss,{X: x_train, y: y_train}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer $f^{(1)}(x) = W^{(1)}x + b^{(1)}$ is a linear transformation of the input, and thus cannot transform the XOR problem to a linearly separable space. Let us inspect the trained parameters $W^{(1)}$ and $b^{(1)}$, and the output of the first layer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.45533264  3.05270791]\n",
      " [ 1.44410062  3.06721687]] \n",
      "\n",
      "[ 0.67456263  0.49640512] \n",
      "\n",
      "[[ 0.67456263  0.49640512]\n",
      " [ 2.11866331  3.563622  ]\n",
      " [ 2.12989521  3.54911304]\n",
      " [ 3.57399583  6.61632967]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1218e3e80>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAEACAYAAAB8nvebAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e25bc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(sess.run(W1),'\\n')\n",
    "print(sess.run(b1),'\\n')\n",
    "\n",
    "f1_out = sess.run(f1,{X: x_train, y: y_train})\n",
    "print(f1_out,'\\n')\n",
    "\n",
    "plt.scatter(f1_out[t,0],f1_out[t,1],c='b',marker='x',s=70)\n",
    "plt.scatter(f1_out[f,0],f1_out[f,1],c='r',marker='o',s=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next layer $f^{(2)}(x)$ is the sigmoid function, which is a nonlinear transformation of the input, thus providing the possibility of transforming the problem to a new space where the outputs could be linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.66252404  0.6216141 ]\n",
      " [ 0.89270395  0.97244477]\n",
      " [ 0.89377505  0.97205329]\n",
      " [ 0.97272134  0.99866343]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1218a1b38>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEACAYAAAC+gnFaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5yAwM8u5/wfVu6TSvxxuxwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121777fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f2_out = sess.run(f2,{X: x_train, y: y_train})\n",
    "print(f2_out)\n",
    "\n",
    "plt.scatter(f2_out[t,0],f2_out[t,1],c='b',marker='x',s=70)\n",
    "plt.scatter(f2_out[f,0],f2_out[f,1],c='r',marker='o',s=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final layer is the model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:  [[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]] \n",
      "\n",
      "model:  [[ 0.08845341]\n",
      " [ 0.63328826]\n",
      " [ 0.63199496]\n",
      " [ 0.64158225]]\n"
     ]
    }
   ],
   "source": [
    "print(\"y: \",sess.run(y,{X: x_train, y: y_train}),\"\\n\")\n",
    "print(\"model: \",sess.run(y_model,{X: x_train, y: y_train}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The network seems to have learned to classify the XOR problem correctly, thanks to the multi-layered structure and the non-linear activation function in the hidden layer. This example embodies the some of the primary reasons for employing deep learning models, especially for highly non-linear problems where traditional linear approaches fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
