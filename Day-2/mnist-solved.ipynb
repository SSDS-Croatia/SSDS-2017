{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a basic TensorFlow tutorial on image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.set_random_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST dataset\n",
    "\n",
    "Contains images of handwritten digits (0, 1, 2, ..., 9).\n",
    "\n",
    "Download the dataset using TF's built-in method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every MNIST sample has two parts:\n",
    "an image (vectorized, raster-scanned) of a handwritten digit and a corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_sample(index):\n",
    "    image = mnist.train.images[index].reshape(28, 28) # 784 -> 28x28\n",
    "    label = mnist.train.labels[index]\n",
    "\n",
    "    plt.imshow(image, cmap='Greys')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "    print('label[%d]: %s' % (index, str(label)))\n",
    "\n",
    "show_sample(10)\n",
    "print('------------------------------------------------------------')\n",
    "show_sample(24)\n",
    "print('------------------------------------------------------------')\n",
    "show_sample(12)\n",
    "print('------------------------------------------------------------')\n",
    "show_sample(11)\n",
    "print('------------------------------------------------------------')\n",
    "show_sample(18)\n",
    "print('------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "\n",
    "Here we specify placeholders for the TF computational graph. Basicall, this determines how to input images, $x$, and labels, $y$, into the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs():\n",
    "    # `x` is a batch of input images (each reshaped into a vector: 28x28 -> 784)\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    # `y` is a batch of labels\n",
    "    y = tf.placeholder(tf.float32, [None, 10])\n",
    "    # return for further processing\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Our first classification model: softmax regression\n",
    "\n",
    "We're going to train a model to look at images and predict what digits they are.\n",
    "\n",
    "A function $M: \\mathbb{R}^{28\\times 28}\\rightarrow \\mathbb{R}^{10}$ outputs a classification score for each input digit.\n",
    "In other words, $M(\\text{image})=\\text{a vector of per-class scores}$.\n",
    "We want that a higher score for class $c$ translates to higher confidence that $c$ is the correct class.\n",
    "\n",
    "For example, if $M$ outputs\n",
    "$$\n",
    "    (0.05, 0.03, 0.82, 0.02, 0.01, 0.02, 0.01, 0.02, 0.01, 0.1)\n",
    "$$\n",
    "for an input image, it classifies that image as a $2$.\n",
    "\n",
    "Let us choose a very simple classification model first:\n",
    "$$\n",
    "    M(\\mathbf{x})=\n",
    "    \\mathbf{x}\\cdot\\mathbf{W} + \\mathbf{b}\n",
    "    ,\n",
    "$$\n",
    "where $\\mathbf{x}\\in\\mathbb{R}^{784}$ is a vectorized input image, and $\\mathbf{W}\\in\\mathbb{R}^{784\\times 10}$ and $\\mathbf{b}\\in\\mathbb{R}^{10}$ are the model parameters. The elements of $M(\\mathbf{x})$ are sometimes called **logits**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_affine(x):\n",
    "    W = tf.Variable(tf.random_normal([784, 10], mean=0.0, stddev=0.01))\n",
    "    b = tf.Variable(tf.zeros([10]))\n",
    "    logits = tf.matmul(x, W) + b\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the model parameters from data\n",
    "\n",
    "Initially, $\\mathbf{W}$ and $\\mathbf{b}$ contain random values that will not produce correct classification results.\n",
    "\n",
    "We have to tune these tensors by minimizing an appropirate loss function that will \"measure\" the quality of classification.\n",
    "\n",
    "We will use the **cross entropy criterion**:\n",
    "$$\n",
    "    L(\\mathbf{x}, c)=\n",
    "    -\\log p_c(\\mathbf{x})\n",
    "    ,\n",
    "$$\n",
    "where $p_c(\\mathbf{x})$ is the **probability** assigned by the model that $\\mathbf{x}$ belongs to class $c$,\n",
    "$$\n",
    "    p_c=\n",
    "    \\frac{e^{l_c}}{\\sum_{j=1}^{10} e^{l_j}}\n",
    "    ,\n",
    "$$\n",
    "and $(l_0, l_1, \\ldots, l_9)=M(\\mathbf{x})$ are the logits output by the model.\n",
    "\n",
    "The derivatives can now be computed by TensorFlow and the model can be tuned with **stochastic gradient descent** ($k=0, 1, 2, \\ldots$):\n",
    "$$\n",
    "    \\mathbf{W}_{k+1}=\n",
    "    \\mathbf{W}_k - \\eta\\frac{\\partial L}{\\partial\\mathbf{W}_k}\n",
    "$$\n",
    "$$\n",
    "    \\mathbf{b}_{k+1}=\n",
    "    \\mathbf{b}_k - \\eta\\frac{\\partial L}{\\partial\\mathbf{b}_k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, y):\n",
    "    # labels `y` are one-hot encoded class indicators (`c` in the text above)\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss $L$ is usually approximated on a batch of images.\n",
    "The code above can handle this case as well.\n",
    "We set the batch size to $100$ is our experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the test set accuracy\n",
    "\n",
    "We measure the quality of the model on a separate testing dataset by counting the number of images that it has correctly classified:\n",
    "$$\n",
    "    \\text{accuracy}=\n",
    "    \\frac{\\text{number of correctly classified samples}}{\\text{total number of samples}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_accuracy(logits, y):\n",
    "    # tests whether the positions of max elements in `logits` and `y` are the same\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    # finds the mean of the resulting vector of 0s and 1s\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    # return for future processing\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainig/testing loop\n",
    "\n",
    "The code for traing and validating the model follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_training_loop(step, accuracy, batchsize, niters):\n",
    "    #\n",
    "    for k in range(niters):\n",
    "        X, Y = mnist.train.next_batch(batchsize)\n",
    "        sess.run(step, feed_dict={x: X, y: Y})\n",
    "        if k % 500 == 0:\n",
    "            acc = 100*sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "            print('* iter %d: test set accuracy=%.2f %%' % (k, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs and model outputs\n",
    "x, y = build_inputs()\n",
    "logits = build_affine(x)\n",
    "\n",
    "# loss-computation grah\n",
    "loss = build_loss(logits, y)\n",
    "\n",
    "# testing-accuracy graph\n",
    "accuracy = build_accuracy(logits, y)\n",
    "\n",
    "# we use SGD to gradually tune the model parameters\n",
    "step = tf.train.GradientDescentOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "# final preparations for learning\n",
    "sess = tf.Session()\n",
    "tf.global_variables_initializer().run(session=sess)\n",
    "\n",
    "# start the learning process: batch size=100, number of iterations=10000\n",
    "run_training_loop(step, accuracy, 100, 30001) \n",
    "\n",
    "# clear the current session (so we can start another one later)\n",
    "sess.close()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obtained classification accuracy on the test set should be between 91 and 93 percent.\n",
    "This is a pretty bad result.\n",
    "Can we do better that that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a convolutional model for digit classification\n",
    "\n",
    "Convolutional networks are **significantly** better for image classification than traditional machine-learning methods.\n",
    "\n",
    "Basic components:\n",
    "\n",
    "* convolutional layers;\n",
    "* rectified linear units (ReLUs);\n",
    "* dense layers (affine projection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_convnet(x):\n",
    "    # reshape input to image\n",
    "    x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    # first conv block\n",
    "    x = tf.layers.conv2d(x, 32, 5, padding='SAME')\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.layers.max_pooling2d(x, 2, 2)\n",
    "    # second conv block\n",
    "    x = tf.layers.conv2d(x, 64, 5, padding='SAME')\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.layers.max_pooling2d(x, 2, 2)\n",
    "    # reshape the resulting tensor into a vector and reduce its dimension to 10\n",
    "    x = tf.reshape(x, [-1, 7*7*64])\n",
    "    x = tf.layers.dense(x, 128)\n",
    "    x = tf.nn.relu(x)\n",
    "    logits = tf.layers.dense(x, 10)\n",
    "    # return for future processing\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs and model outputs\n",
    "x, y = build_inputs()\n",
    "logits = build_convnet(x)\n",
    "\n",
    "# loss-computation grah\n",
    "loss = build_loss(logits, y)\n",
    "\n",
    "# testing-accuracy graph\n",
    "accuracy = build_accuracy(logits, y)\n",
    "\n",
    "# we use RMSProp to gradually tune the model parameters (similar to SGD, but better in most cases)\n",
    "step = tf.train.RMSPropOptimizer(1e-3).minimize(loss)\n",
    "\n",
    "# final preparations for learning\n",
    "sess = tf.Session()\n",
    "tf.global_variables_initializer().run(session=sess)\n",
    "\n",
    "# start the learning process: batch size=100, number of iterations=5000\n",
    "run_training_loop(step, accuracy, 100, 5001) \n",
    "\n",
    "# clear the current session (so we can start another one later)\n",
    "sess.close()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obtained classification accuracy should be well over 99%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
