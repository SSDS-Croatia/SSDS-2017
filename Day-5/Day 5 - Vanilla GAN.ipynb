{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science Summer School - Split '17\n",
    "\n",
    "## 5. Generating images of digits with Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals:\n",
    "\n",
    "1. Implement the model from \"[Generative Adversarial Networks](http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)\" by Goodfellow et al. (1284 citations since 2014.)\n",
    "\n",
    "2. **Understand** how the model learns to generate realistic images\n",
    "\n",
    "In ~two hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Downloading the datasets and previewing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data'; dataset = 'mnist'  # the folder in which the dataset is going to be stored\n",
    "\n",
    "download_folder = util.download_mnist(data_folder, dataset)\n",
    "images, labels = util.load_mnist(download_folder)\n",
    "\n",
    "print(\"Folder:\", download_folder)\n",
    "print(\"Image shape:\", images.shape) # greyscale, so the last dimension (color channel) = 1\n",
    "print(\"Label shape:\", labels.shape) # one-hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_n_images = 25\n",
    "sample_images, mode = util.get_sample_images(images, n=show_n_images)\n",
    "mnist_sample = util.images_square_grid(sample_images, mode)\n",
    "plt.imshow(mnist_sample, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = images[3]*50 # \n",
    "sample = sample.reshape((28, 28))\n",
    "print(np.array2string(sample.astype(int), max_line_width=100, separator=',', precision=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sample, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are we going to do with the data?\n",
    "\n",
    "- We **have** $70000$ images of hand-written digits generated from some distribution $X \\sim P_{real}$\n",
    "- We **have** $70000$ labels $y_i \\in \\{0,..., 9\\}$ indicating which digit is written on the image $x_i$\n",
    "\n",
    "**Problem:** Imagine that the number of images we have **is not enough** - a common issue in computer vision and machine learning.\n",
    "\n",
    "1. We can pay experts to create new images\n",
    "  * **Expensive**\n",
    "  * Slow\n",
    "  * Realiable\n",
    "2. We can generate new images ourselves\n",
    "  * **Cheap**\n",
    "  * **Fast**\n",
    "  * Unreliable?\n",
    "\n",
    "**Problem:** Not every image that we generate is going to be perfect (or even close to perfect). Therefore, we need some method to determine which images are realistic. \n",
    "\n",
    "1. We can pay experts to determine which images are good enough\n",
    "  * Expensive\n",
    "  * Slow\n",
    "  * **Reliable**\n",
    "2. We can train a model to determine which images are good enough\n",
    "  * **Cheap**\n",
    "  * **Fast**\n",
    "  * Unreliable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formalization\n",
    "\n",
    "* $X \\sim P_{real}$ : existing images of shape $s$\n",
    "* $Z \\sim P_z$ : a $k$-dimensional random vector\n",
    "* $G(z; \\theta_G): Z \\to \\hat{X}$ : the **generator**, a function that transforms the random vector $z$ into an image of shape $s$\n",
    "* $D(x, \\theta_D): X \\to (Real, Fake)$ : the **discriminator** a function that given an image of shape $s$ decides if the image is real or fake\n",
    "\n",
    "### Details\n",
    "\n",
    "The existing images $X$ in our setup are images from the mnist dataset. We will arbitrarily decide that vectors $z$ will be sampled from a uniform distribution, and $G$ and $D$ will both be *'deep'* neural networks. \n",
    "\n",
    "For simplicity, and since we are using the mnist dataset, both $G$ and $D$ will be multi-layer perceptrons (and not deep convolutional networks) with one hidden layer. The generated images $G(z) \\sim P_{fake}$ as well as real images $x \\sim P_{real}$ will be passed on to the discriminator, which will classify them into $(Real, Fake)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"data/img/gan_general_layout.png\">\n",
    "<strong>Figure 1. </strong> General adversarial network architecture\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "** The goal ** of the discriminator is to successfully recognize which image is sampled from the true distribution, and which image is sampled from the generator. \n",
    "\n",
    "<center>\n",
    "<img src=\"data/img/discriminator.png\">\n",
    "<strong>Figure 2.</strong> Discriminator network sketch\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "** The goal ** of the generator is that the discriminator *missclassifies* the images that the generator generated as if they were generated by the true distribution.\n",
    "\n",
    "<center>\n",
    "<img src=\"data/img/generator.png\">\n",
    "<strong>Figure 3.</strong> Generator network sketch\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Data transformation\n",
    "\n",
    "Since we are going to use a fully connected network (we are not going to use local convolutional filters), we are going to flatten the input images for simplicity. Also, the pixel values are scaled to the interval $[0,1]$ (this was already done beforehand). \n",
    "\n",
    "We will also use a pre-made `Dataset` class to iterate over the dataset in batches. The class is defined in `util.py`, and only consists of a constructor and a method `next_batch`.\n",
    "\n",
    "**Question:** Having seen the architecture of the network, why are we the pixels scaled to $[0,1]$ and not, for example, $[-1, 1]$, or left at $[0, 255]$?\n",
    "\n",
    "**Answer:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 The generator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    \"\"\"The generator network\n",
    "    \n",
    "    the generator network takes as input a vector z of dimension input_dim, and transforms it \n",
    "    to a vector of size output_dim. The network has one hidden layer of size hidden_dim.\n",
    "    \n",
    "    We will define the following methods: \n",
    "    \n",
    "    __init__: initializes all variables by using tf.get_variable(...) \n",
    "                and stores them to the class, as well a list in self.theta\n",
    "    forward: defines the forward pass of the network - how do the variables\n",
    "                interact with respect to the inputs\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"Constructor for the generator network. In the constructor, we will\n",
    "        just initialize all the variables in the network.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: The dimension of the input data vector (z).\n",
    "            hidden_dim: The dimension of the hidden layer of the neural network (h)\n",
    "            output_dim: The dimension of the output layer (equivalent to the size of the image)            \n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        with tf.variable_scope(\"generator\"):\n",
    "            pass\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\"The forward pass of the network -- here we will define the logic of how we combine\n",
    "        the variables through multiplication and activation functions in order to get the\n",
    "        output.\n",
    "        \n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 The basic network for the discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "    \"\"\"The discriminator network\n",
    "    \n",
    "    the discriminator network takes as input a vector x of dimension input_dim, and transforms it \n",
    "    to a vector of size output_dim. The network has one hidden layer of size hidden_dim.\n",
    "    \n",
    "    You will define the following methods: \n",
    "    \n",
    "    __init__: initializes all variables by using tf.get_variable(...) \n",
    "                and stores them to the class, as well a list in self.theta\n",
    "    forward: defines the forward pass of the network - how do the variables\n",
    "                interact with respect to the inputs\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        with tf.variable_scope(\"discriminator\"):\n",
    "            pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"The forward pass of the network -- here we will define the logic of how we combine\n",
    "        the variables through multiplication and activation functions in order to get the\n",
    "        output.\n",
    "        \n",
    "        Along with the probabilities, also return the unnormalized probabilities\n",
    "        (the values in the output layer before being passed through the sigmoid function)\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermezzo: Xavier initialization of weights\n",
    "\n",
    "Glorot, X., & Bengio, Y. (2010, March). [Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf). In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (pp. 249-256).\n",
    "\n",
    "Implemented in tensorflow, as part of the standard library: https://www.tensorflow.org/api_docs/python/tf/contrib/layers/xavier_initializer\n",
    "\n",
    "#### 1. Idea:\n",
    "\n",
    "- If the weights in a network are initialized to too small values, then the signal shrinks as it passes through each layer until it’s too tiny to be useful.\n",
    "- If the weights in a network are initialized to too large, then the signal grows as it passes through each layer until it’s too massive to be useful.\n",
    "\n",
    "#### 2. Goal: \n",
    "\n",
    "- We need initial weight values that are *just right* for the signal not to explode or vanish during the forward pass\n",
    "\n",
    "#### 3. Math\n",
    "\n",
    "- Trivial\n",
    "\n",
    "#### 4. Solution\n",
    "\n",
    "- $v = \\frac{2}{n_{in} + n_{out}}$\n",
    "\n",
    "In the case of a Gaussian distribution, we set the **variance** to $v$.\n",
    "\n",
    "In the case of a uniform distribution, we set the **interval ** to $\\pm v$ (the default distr. in tensorflow is the uniform).\n",
    "\n",
    "<sub>http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization</sub>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Define the model parameters\n",
    "\n",
    "We will take a brief break to set the values for the parameters of the model. Since we know the dataset we are working with, as well as the shape of the generator and discriminator networks, your task is to fill in the values of the following variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_dim = # The dimension of the input image vector to the discrminator\n",
    "discriminator_hidden_dim = # The dimension of the hidden layer of the discriminator\n",
    "discriminator_output_dim = # The dimension of the output layer of the discriminator \n",
    "\n",
    "random_sample_dim =  # The dimension of the random noise vector z\n",
    "generator_hidden_dim = # The dimension of the hidden layer of the generator\n",
    "generator_output_dim = # The dimension of the output layer of the generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Check the implementation of the classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Discriminator(image_dim, discriminator_hidden_dim, discriminator_output_dim)\n",
    "for param in d.theta:\n",
    "    print (param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Generator(random_sample_dim, generator_hidden_dim, generator_output_dim)\n",
    "for param in g.theta:\n",
    "    print (param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing samples from the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_Z(m, n):\n",
    "    pass\n",
    "\n",
    "plt.imshow(sample_Z(16, 100), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Define the model loss -- Vanilla GAN\n",
    "\n",
    "The objective for the vanilla version of the GAN was defined as follows:\n",
    "\n",
    "<center>\n",
    "$\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{real}} [log(D(x))] + \\mathbb{E}_{z \\sim p_{z}} [log(1 -D(G(z)))]$\n",
    "</center>\n",
    "\n",
    "The function contains a *minimax* formulation, and cannot be directly optimized. However, if we freeze $D$, we can derive the loss for $G$ and vice versa.\n",
    "\n",
    "**Discriminator loss:**\n",
    "<center>\n",
    "$p_{fake} = G(p_z)$\n",
    "</center>\n",
    "\n",
    "<center>\n",
    "$D_{loss} = \\mathbb{E}_{x \\sim p_{real}} [log(D(x))] + \\mathbb{E}_{\\hat{x} \\sim p_{fake}} [log(1 -D(\\hat{x}))]$\n",
    "</center>\n",
    "\n",
    "We estimate the expectation over each minibatch and arrive to the following formulation:\n",
    "<center>\n",
    "$D_{loss} = \\frac{1}{m}\\sum_{i=0}^{m} log(D(x_i)) + \\frac{1}{m}\\sum_{i=0}^{m} log(1 -D(\\hat{x_i}))$\n",
    "</center>\n",
    "\n",
    "\n",
    "**Generator loss:**\n",
    "<center>\n",
    "$G_{loss} = - \\mathbb{E}_{z \\sim p_{z}} [log(1 -D(G(z)))]$\n",
    "</center>\n",
    "\n",
    "<center>\n",
    "$G_{loss} = \\frac{1}{m}\\sum_{i=0}^{m} [log(D(G(z)))]$\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model loss, translated from math\n",
    "\n",
    "The **discriminator** wants to:\n",
    "- **maximize** the (log) probability of a **real** image being classified as **real**,\n",
    "- **minimize** the (log) probability of a **fake** image being classified as **real**.\n",
    "\n",
    "The **generator** wants to:\n",
    "- **maximize** the (log) probability of a **fake** image being classified as **real**.\n",
    "\n",
    "### Model loss, translated to practical machine learning\n",
    "\n",
    "The output of the discriminator is a scalar, $p$, which we interpret as the probability that an input image is **real** ($1-p$ is the probability that the image is fake).\n",
    "\n",
    "The **discriminator** takes as input:\n",
    "\n",
    "- a minibatch of images from our training set with a vector of **ones** for class labels: $D_{loss\\_real}$. \n",
    "- a minibatch of images from the generator with a vector of **zeros** for class labels: $D_{loss\\_fake}$.  \n",
    "- a minibatch of images from the generator with a vector of **ones** for class labels: $G_{loss}$.\n",
    "\n",
    "The **generator** takes as input:\n",
    "\n",
    "- a minibatch of vectors sampled from the latent space and transforms them to a minibatch of generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermezzo: sigmoid cross entropy with logits\n",
    "\n",
    "We defined the loss of the model as the log of the probability, but we are not using a $log$ function or the model probablities anywhere?\n",
    "\n",
    "Enter sigmoid cross entropy with logits: https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits\n",
    "\n",
    "<center>\n",
    "<img src=\"data/img/logitce.png\">\n",
    "From the tensorflow documentation\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, name=\"input\", shape=[None, image_dim])\n",
    "Z = tf.placeholder(tf.float32, name=\"latent_sample\", shape=[None, random_sample_dim])\n",
    "\n",
    "G_sample, D_loss, G_loss = gan_model_loss(X, Z, d, g)\n",
    "\n",
    "with tf.variable_scope('optim'):\n",
    "    D_solver = tf.train.AdamOptimizer(name='discriminator').minimize(D_loss, var_list=d.theta)\n",
    "    G_solver = tf.train.AdamOptimizer(name='generator').minimize(G_loss, var_list=g.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "# Some runtime parameters predefined for you\n",
    "minibatch_size = 128 # The size of the minibatch\n",
    "\n",
    "num_epoch = 500 # For how many epochs do we run the training\n",
    "plot_every_epochs = 5 # After this many epochs we will save & display samples of generated images \n",
    "print_every_batches = 1000 # After this many minibatches we will print the losses\n",
    "\n",
    "restore = True\n",
    "checkpoint = 'fc_2layer_e100_2.170.ckpt'\n",
    "model = 'gan'\n",
    "model_save_folder = os.path.join('data', 'chkp', model)\n",
    "print (\"Model checkpoints will be saved to:\", model_save_folder)\n",
    "image_save_folder = os.path.join('data', 'model_output', model)\n",
    "print (\"Image samples will be saved to:\", image_save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_counter = 0\n",
    "epoch_counter = 0\n",
    "\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "\n",
    "with tf.device(\"/gpu:0\"), tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    if restore:\n",
    "        saver.restore(sess, os.path.join(model_save_folder, checkpoint))\n",
    "        print(\"Restored model:\", checkpoint, \"from:\", model_save_folder)\n",
    "                      \n",
    "    while epoch_counter < num_epoch:\n",
    "            \n",
    "        new_epoch, X_mb = mnist.next_batch(minibatch_size)\n",
    "\n",
    "        _, D_loss_curr = sess.run([D_solver, D_loss], \n",
    "                                  feed_dict={\n",
    "                                      X: X_mb, \n",
    "                                      Z: sample_Z(minibatch_size, random_sample_dim)\n",
    "                                    })\n",
    "                      \n",
    "        _, G_loss_curr = sess.run([G_solver, G_loss], \n",
    "                                  feed_dict={\n",
    "                                      Z: sample_Z(minibatch_size, random_sample_dim)\n",
    "                                  })\n",
    "\n",
    "        # Plotting and saving images and the model\n",
    "        if new_epoch and epoch_counter % plot_every_epochs == 0:\n",
    "            samples = sess.run(G_sample, feed_dict={Z: sample_Z(16, random_sample_dim)})\n",
    "\n",
    "            fig = util.plot(samples)\n",
    "            figname = '{}.png'.format(str(minibatch_counter).zfill(3))\n",
    "            plt.savefig(os.path.join(image_save_folder, figname), bbox_inches='tight')\n",
    "            plt.show()\n",
    "            plt.close(fig)\n",
    "            \n",
    "            im = util.plot_single(samples[0], epoch_counter)\n",
    "            plt.savefig(os.path.join(image_save_folder, 'single_' + figname), bbox_inches='tight')\n",
    "            plt.show()\n",
    "            \n",
    "            chkpname = \"fc_2layer_e{}_{:.3f}.ckpt\".format(epoch_counter, G_loss_curr)\n",
    "            saver.save(sess, os.path.join(model_save_folder, chkpname))\n",
    "\n",
    "        # Printing runtime statistics\n",
    "        if minibatch_counter % print_every_batches == 0:\n",
    "            print('Epoch: {}/{}'.format(epoch_counter, num_epoch))\n",
    "            print('Iter: {}/{}'.format(mnist.position_in_epoch, mnist.n))\n",
    "            print('Discriminator loss: {:.4}'. format(D_loss_curr))\n",
    "            print('Generator loss: {:.4}'.format(G_loss_curr))\n",
    "            print()\n",
    "        \n",
    "        # Bookkeeping\n",
    "        minibatch_counter += 1\n",
    "        if new_epoch:\n",
    "            epoch_counter += 1\n",
    "        \n",
    "        d_losses.append(D_loss_curr)\n",
    "        g_losses.append(G_loss_curr)\n",
    "        \n",
    "    # Save the final model\n",
    "    chkpname = \"fc_2layer_e{}_{:.3f}.ckpt\".format(epoch_counter, G_loss_curr)\n",
    "    saver.save(sess, os.path.join(model_save_folder, chkpname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_line, = plt.plot(range(len(d_losses[:10000])), d_losses[:10000], c='b', label=\"Discriminator loss\")\n",
    "gen_line, = plt.plot(range(len(d_losses[:10000])), g_losses[:10000], c='r', label=\"Generator loss\")\n",
    "plt.legend([disc_line, gen_line], [\"Discriminator loss\", \"Generator loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode collapse\n",
    "\n",
    "<center>\n",
    "<img src=\"data/img/mode_collapse.png\">\n",
    "<img src=\"data/img/mode_collapse_2.png\">\n",
    "\n",
    "Example of mode collapse in a GAN\n",
    "</center>\n",
    "\n",
    "Second image is from: Reed, S., van den Oord, A., Kalchbrenner, N., Bapst, V., Botvinick, M., & de Freitas, N. (2016). [Generating interpretable images with controllable structure](https://pdfs.semanticscholar.org/0365/da7fabc8fcc453432a94237814db3da04af2.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further reading\n",
    "\n",
    "- https://github.com/hwalsuklee/tensorflow-generative-model-collections\n",
    "\n",
    "- https://github.com/wiseodd/generative-models/\n",
    "\n",
    "- https://github.com/znxlwm/pytorch-generative-model-collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
