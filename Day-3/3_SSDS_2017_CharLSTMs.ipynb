{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Summer School - Split '17\n",
    "\n",
    "**Prerequisites**: Please download the following [zip archive](https://www.dropbox.com/s/2eaunmgmvc10n6q/trump_tb_20_i3880_l512_1.327.ckpt.zip?dl=0) which contains checkpoint you will need in this exercise and put it in `assets\\checkpoints\\ssds\\` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Character-wise language modeling with multi-layer LSTMs\n",
    "\n",
    "This hands-on session is based on two tutorial notebooks [*Intro to Recurrent Networks (Character-wise RNN)*](https://github.com/udacity/deep-learning/tree/master/intro-to-rnns) and [*Tensorboard*](https://github.com/udacity/deep-learning/tree/master/tensorboard) from Udacity's [Deep Learning Nanodegree Foundation](https://www.udacity.com/course/deep-learning-nanodegree-foundation--nd101) program.\n",
    "\n",
    "This notebook implements a multi-layer LSTMs network for training/sampling from character-level language models. The model takes a text file as input and trains the network that learns to predict the next character in a sequence. The network can then be used to generate text character by character that will look like the original training data. This network is based on Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), which became standard example for explaining peculiarities behind RNN models.\n",
    "\n",
    "Good description of LSTM architecture can be found in the article [*Understanding LSTM Networks*](http://colah.github.io/posts/2015-08-Understanding-LSTMs/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Character-wise RNN language model](assets/images/CharRNN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.1 Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and encoding text\n",
    "\n",
    "We will train our language model on a complete collection of Donald Trump's tweets obtained from [Trump Twitter Archive](http://www.trumptwitterarchive.com/), which we already downloaded and made available in `PATH-TO-REPOSITORY/Day-3/assets/data/trump_tweets_ascii.txt`. First, we will load the text file and encode its characters as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('assets/data/trump_tweets_ascii.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "\n",
    "# get set of characters contained in the loaded text file\n",
    "vocab = sorted(set(text))\n",
    "\n",
    "# encoding characters as integers\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "encoded_chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)\n",
    "\n",
    "# make dict for decoding intergers to corresponding characters\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "\n",
    "print('Text size: {}'.format(len(encoded_chars)))\n",
    "print('Vocabulary size: {}'.format(len(vocab)))\n",
    "print('*******************************')\n",
    "print('Number of tweets: {}'.format(len(text.split('\\n'))))\n",
    "print('Median size of a tweet: {}'.format(np.percentile([len(t) for t in text.split('\\n')], 50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output, we can see that `trump_tweets_ascii.txt` contains in total 2 167 951 characters. Tweets contain 92 unique characters which will form a vocabulary for a language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see first 300 characters of the provided text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 300 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And see how they are encoded as integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 300 encoded characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making training and validation mini-batches\n",
    "\n",
    "Neural networks are trained by approximating the gradient of loss function with respect to the neuron weights, by looking at only a small subset of the data, also known as a mini-batch. Here is where we will make our mini-batches for training and validation. Now we need to split up the data into batches, as well as into training and validation sets. \n",
    "\n",
    "For the test we will observe how the network generates new text, thus we will not be using test set. We will feed a character into the network and sample a next one from the distribution over characters likely to come next. We feed the sampled character right back to get next character. Repeating this process character by character will generate new text, hopefully indistinguishable from [Donald Trump's](https://twitter.com/realdonaldtrump/status/881281755017355264) Twitter [tweets](https://twitter.com/realdonaldtrump/status/869858333477523458).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visualization of batches](assets/images/batches-visualization.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(arr, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split data into batches and training and validation sets.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    arr: Array of encoded characters as integers \n",
    "    batch_size: Number of sequences per batch\n",
    "    num_steps: Length of the sequence in a batch\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(arr) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = arr[: n_batches*slice_size]\n",
    "    \n",
    "    # The targets are the same as the inputs, except shifted one character over.\n",
    "    # number of batches covers full size of arr (no characters dropped)\n",
    "    if(len(arr) == n_batches*slice_size):\n",
    "        # for the last target character use first input character\n",
    "        y = np.roll(x, -1)\n",
    "    else:\n",
    "        # for the last target characher use first dropped character\n",
    "        y = arr[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices and then stack slices \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x (n_batches x num_steps)\n",
    "    \n",
    "    # Split into training and validation sets, keep the first split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Generate example integer array. Use function `split_data` to split `example_arr` into train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# example array generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "num_steps = 3\n",
    "split_frac = 0.9 \n",
    "\n",
    "# split example array into train and validation sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a generator function to get batches from the arrays made by `split_data`. This will provide us with the functionality to iterate over batches, which we can feed to our network model. The arrays are of dimension (`batch_size, n_batches*num_steps`). Each batch is a sliding window on these arrays with size `batch_size X num_steps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Use the `for` loop to iterate through all train and validation batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through all train and validation batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Building the model\n",
    "\n",
    "After having our data prepared and convenience functions `split_data` and `get_batch` for handling the data during the training of our model, we can finally start building the model using the TensorFlow library. We will break the model building into five parts:\n",
    "* building input placeholders for x, y and dropout \n",
    "* building multi-layer RNN with stacked LSTM cells\n",
    "* building softmax output layer\n",
    "* computation for training loss\n",
    "* building the optimizer for the model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs\n",
    "First, we will create our input placeholders for Tensorflow computational graph of the model. As we are building supervised learning model, we need to declare placeholders for inputs (x) and targets (y). We also need to one-hot encode the input and target tokens, remember we are getting them as encoded characters. Here, we will also declare scalar placeholder `keep_prob` for output keep probablity for dropout.  \n",
    "\n",
    "New functions used here:\n",
    "- [`tf.name_scope`](https://www.tensorflow.org/api_docs/python/tf/Graph#name_scope)\n",
    "- [`tf.one_hot`](https://www.tensorflow.org/api_docs/python/tf/one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Define placeholders for inputs and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps, num_classes):\n",
    "    ''' Define placeholders for inputs, targets, and dropout. \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size: Batch size, number of sequences per batch\n",
    "        num_steps: Number of sequence steps in a batch\n",
    "        num_classes: Number of classes (target values)\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    with tf.name_scope('inputs'):\n",
    "        # EXERCISE: Declare placeholder for inputs and one-hot encode inputs\n",
    "        \n",
    "        \n",
    "    with tf.name_scope('targets'):\n",
    "        # EXERCISE: Declare placeholder for targets and one-hot encode targets\n",
    "      \n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs, x_one_hot, targets, y_one_hot, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer LSTM Cell\n",
    "We first implement `build_cell` function where we create the LSTM cell we will use in the hidden layer. We will use this cell as a building block for the multi-layer RNN. Afterwards, we implement the `build_lstm` function to create multiple LSTM cells stacked on each other using `build_cell` function. We can stack up the LSTM cells into layers with [`tf.contrib.rnn.MultiRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Fill in `build_cell` function for building LSTM cell using:\n",
    "\n",
    "- [`tf.contrib.rnn.BasicLSTMCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell)\n",
    "- [`contrib.rnn.DropoutWrapper`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/DropoutWrapper). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_cell(lstm_size, keep_prob):\n",
    "    ''' Build LSTM cell.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        keep_prob: Dropout keep probability\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # EXERCISE: Use a basic LSTM cell\n",
    "    \n",
    "    # EXERCISE: Add dropout to the cell\n",
    "    \n",
    "    \n",
    "    return drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Fill in `build_lstm` function by stacking layers using [`tf.contrib.rnn.MultiRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' Build Multi-RNN cell.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        num_layers: Number of LSTM layers\n",
    "        batch_size: Batch size\n",
    "        keep_prob: Dropout keep probability\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # EXERCISE: Stack up multiple LSTM layers\n",
    "    \n",
    "    \n",
    "    with tf.name_scope(\"RNN_init_state\"):\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building RNN Output Layer\n",
    "Here we will create the output layer. We need to connect the output of the RNN cells to a fully connected layer with a softmax output. The softmax output gives us a probability distribution we can use to predict the next character. The output 3D tensor with size $(batch\\_size \\times num\\_steps \\times lstm\\_size)$ has to be reshaped to $((batch\\_size \\times num\\_steps) \\times  lstm\\_size)$, so we can do the matrix multiplication with the softmax weights.\n",
    "\n",
    "The output is calculated using softmax function\n",
    "$$\n",
    "    P(y=c\\text{ } | \\text{ }\\mathbf{x}) = \\frac{e^{\\mathbf{x}^T\\mathbf{w}_c+b_c}}{\\sum_{k=1}^{|C|}e^{\\mathbf{x}^T\\mathbf{w}_k+b_k}}\n",
    "    ,\\\\\n",
    "$$\n",
    "where $\\mathbf{x}\\in\\mathbb{R}^{512}$ is output of the last hidden layer, and $\\mathbf{W}\\in\\mathbb{R}^{512\\times 92}$ and $\\mathbf{b}\\in\\mathbb{R}^{92}$ are the model parameters. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Fill in `build_output` function by defining logits and softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' Build a softmax layer, return the softmax output and logits.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        lstm_output: Output tensor of previous layer\n",
    "        in_size: Size of the input tensor\n",
    "        out_size: Size of the softmax layer\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Reshape output so it is a bunch of rows, one row for each step for each sequence.\n",
    "    # That is, the shape should be batch_size*num_steps rows by lstm_size columns.\n",
    "    with tf.name_scope('sequence_reshape'):\n",
    "        seq_output = tf.concat(lstm_output, axis=1, name='seq_output')\n",
    "        x = tf.reshape(seq_output, [-1, in_size], name='graph_output')\n",
    "    \n",
    "    # Connect the RNN outputs to a softmax layer\n",
    "    with tf.name_scope('logits'):\n",
    "\n",
    "        # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "        # of rows of logit outputs, one for each step and sequence\n",
    "        \n",
    "        # EXERCISE: Define W and b and multiply inputs with weights and add bias\n",
    "        \n",
    "        \n",
    "        # Tensorboard\n",
    "        tf.summary.histogram('h_softmax_w', softmax_w)\n",
    "        tf.summary.histogram('h_softmax_b', softmax_b)\n",
    "    \n",
    "    with tf.name_scope('predictions'):\n",
    "        \n",
    "        # EXERCISE: Use softmax to get the probabilities for predicted characters\n",
    "        \n",
    "        \n",
    "        # Tensorboard\n",
    "        tf.summary.histogram('h_predictions', predictions)\n",
    "    \n",
    "    return predictions, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loss\n",
    "\n",
    "Next we need to calculate the training loss. We get the logits and targets and calculate the softmax cross-entropy loss. First, we need to reshape the one-hot targets so it is a 2D tensor with size $((batch\\_size \\times num\\_steps) \\times  num\\_classes)$, which match logits. Remember that we reshaped the LSTM outputs and ran them through a fully connected layer with $num\\_classes$ units. Then we run the logits and targets through [`tf.nn.softmax_cross_entropy_with_logits`](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits) and find the mean to get the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Fill in build loss function: \n",
    "\n",
    "- Reshape one-hot encoded targets to match logits \n",
    "- Define loss and cost function using [`tf.nn.softmax_cross_entropy_with_logits`](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits) and [`tf.reduce_mean`](https://www.tensorflow.org/api_docs/python/tf/reduce_mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, y_one_hot, lstm_size):\n",
    "    ''' Calculate the loss from the logits and the targets.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        logits: Logits from final fully connected layer\n",
    "        y_one_hot: one hot encoding of target\n",
    "        lstm_size: Number of LSTM hidden units        \n",
    "    '''\n",
    "    \n",
    "    # Softmax cross entropy loss\n",
    "    with tf.name_scope('loss'):\n",
    "\n",
    "        # EXERCISE: Reshape one-hot encoded targets to match logits (one row per batch_size per step)\n",
    "        # then define loss and cost function\n",
    "        \n",
    "        \n",
    "        # Tensorboard\n",
    "        tf.summary.scalar('s_cost', cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "Here we build the optimizer. Traditional RNNs face vanishing gradient problem. LSTMs fix the vanishing problem, but the gradients can still grow without bound. To fix this we can clip the gradients larger than some threshold. That is, if a gradient is larger than the prespecified threshold, we set it to the threshold value. This will ensure the gradients never grow too large. Then we use an [AdamOptimizer](https://arxiv.org/abs/1412.6980) for the learning step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Fill in the function `build_optimizer`:\n",
    "\n",
    "- Calculate and clip gradients using functions [`tf.gradients`](https://www.tensorflow.org/api_docs/python/tf/gradients) and [`tf.clip_by_global_norm`](https://www.tensorflow.org/api_docs/python/tf/clip_by_global_norm)\n",
    "- Define Adam optimizer using [`tf.train.AdamOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer)\n",
    "- Apply gradients to trainable variables using function [`tf.train.Optimizer.apply_gradients`](https://www.tensorflow.org/api_docs/python/tf/train/Optimizer#apply_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' Build optmizer for training, using gradient clipping.\n",
    "    \n",
    "        Arguments:\n",
    "        loss: Network loss\n",
    "        learning_rate: Learning rate for optimizer\n",
    "        grad_clip: Clipping ratio\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    with tf.name_scope('optimizer'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        \n",
    "        # EXERCISE: Calculate and clip gradients\n",
    "        \n",
    "        \n",
    "        # EXERCISE: Use Adam optimizer\n",
    "        \n",
    "        \n",
    "        # EXERCISE: Apply gradients to trainable variables\n",
    "        \n",
    "        \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the network\n",
    "\n",
    "Now we can put all the pieces together and build a class for the network. To actually run data through the LSTM cells, we will use [`tf.nn.dynamic_rnn`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/nn/dynamic_rnn). This function will pass the hidden and cell states across LSTM cells appropriately for us. It returns the outputs for each LSTM cell at each step for each sequence in the mini-batch. It also gives us the final LSTM state. We want to save this state as `final_state` so we can pass it to the first LSTM cell in the the next mini-batch run. For `tf.nn.dynamic_rnn`, we pass in the cell and initial state we get from `build_lstm`, as well as our input sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Fill in `CharRNN` class to run each sequence step through the RNN and collect the outputs using [`tf.nn.dynamic_rnn`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/nn/dynamic_rnn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "    \n",
    "        if sampling == True:\n",
    "            # When we will use the network for sampling later, we will pass in one character at a time\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build the input placeholder tensors, and one-hot encode the input and target tokens\n",
    "        self.inputs, x_one_hot, self.targets, y_one_hot, self.keep_prob = \\\n",
    "        build_inputs(batch_size, num_steps, num_classes)\n",
    "        \n",
    "        # Build the LSTM cell\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    " \n",
    "        with tf.name_scope(\"RNN_forward\"):\n",
    "            \n",
    "            # EXERCISE: Run each sequence step through the RNN and collect the outputs\n",
    "            \n",
    "            \n",
    "            \n",
    "        self.final_state = state\n",
    "        \n",
    "        # Get softmax predictions and logits\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # Loss and optimizer (with gradient clipping)\n",
    "        self.loss = build_loss(self.logits, y_one_hot, lstm_size)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)\n",
    "        \n",
    "        self.summary_merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Here we declare the hyperparameters for the network. \n",
    "\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `num_steps` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lstm_size` - The number of units in the hidden layers.\n",
    "* `num_layers` - Number of hidden LSTM layers to use\n",
    "* `learning_rate` - Learning rate for training\n",
    "* `keep_prob` - The dropout keep probability when training. If you're network is overfitting, try decreasing this.\n",
    "\n",
    "Here's some good advice from Andrej Karpathy on training the network [https://github.com/karpathy/char-rnn#tips-and-tricks](https://github.com/karpathy/char-rnn#tips-and-tricks).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100        # Sequences per batch\n",
    "num_steps = 100         # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.001   # Learning rate\n",
    "keep_prob = 0.5         # Dropout keep probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Create new instance of `CharRNN` class using parameters defined above. Print trainable variables in the default graph using tensorflow function [`trainable_variables`](https://www.tensorflow.org/api_docs/python/tf/trainable_variables). Does the number of parameters correspond to what we expect? **Hint**: Number of parameters in first hidden layer of LSTM is equal to:\n",
    "\n",
    "$4 \\times \\big[N_{units} \\times (N_{inputs}+1) + N_{units}^{2}\\big]$,\n",
    "\n",
    "where $N_{units}$ is the number of units in hidden layer (`lstm_size`) and $N_{inputs}$ is the length of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instance of CharRNN class\n",
    "\n",
    "# print trainable variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out the graph for TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    file_writer = tf.summary.FileWriter('assets/logs/1', sess.graph)\n",
    "    \n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Run tensorboard from command line by issuing command (e.g. from root repository directory):\n",
    "\n",
    "```\n",
    "tensorboard --logdir=Day-3/assets/logs/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Training model\n",
    "\n",
    "This is typical training code, passing inputs and targets into the network, then running the optimizer. Here we also get back the final LSTM state for the mini-batch. Then, we pass that state back into the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) we calculate the validation loss and save a checkpoint.\n",
    "\n",
    "Please download provided [`trump_tb_20_i3880_l512_1.327.ckpt`](https://www.dropbox.com/s/2eaunmgmvc10n6q/trump_tb_20_i3880_l512_1.327.ckpt.zip?dl=0)\n",
    "checkpoint and place it in `assets/checkpoints/ssds` direcory in the repository.\n",
    "\n",
    "**Exercise**: Fill in the code below:\n",
    "\n",
    "- Through all train batches run session and save loss. \n",
    "- Through all validation batches run session and append validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1 #20\n",
    "save_every_n = 10 #200\n",
    "train_x, train_y, val_x, val_y = split_data(encoded_chars, batch_size, num_steps)\n",
    "\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Tensorboard\n",
    "    train_writer = tf.summary.FileWriter('assets/logs/2/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('assets/logs/2/test')\n",
    "    \n",
    "    #############################################################\n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    saver.restore(sess, 'assets/checkpoints/ssds/trump_tb_20_i3880_l512_1.327.ckpt')\n",
    "    #############################################################\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    \n",
    "    # Train network\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        \n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            start = time.time()\n",
    "            \n",
    "            # EXERCISE: Run session and save loss for train batches\n",
    "            \n",
    "            \n",
    "            end = time.time()\n",
    "            iteration = e*n_batches + b\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "            \n",
    "            # Tensorboard\n",
    "            train_writer.add_summary(summary, iteration)\n",
    "        \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                \n",
    "                # EXERCISE: Same as above, run session and append validation loss\n",
    "                \n",
    "                \n",
    "                # Tensorboard\n",
    "                test_writer.add_summary(summary, iteration)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"assets/checkpoints/trump/trump_new_i{}_l{}_{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saved checkpoints\n",
    "\n",
    "Read up on saving and loading checkpoints here: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('assets/checkpoints/trump')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Testing model - sampling from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_model(checkpoint, n_samples, lstm_size, vocab_size, num_layers=2, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, num_layers=num_layers, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    states = []\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "         \n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "      \n",
    "            states.append(new_state)\n",
    "    \n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "        states.append(new_state)\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "            states.append(new_state)\n",
    "        \n",
    "    return (''.join(samples), states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Load the latest checkpoint from `assets/checkpoints/trump` folder and generate text using `sample_model` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the latest checkpoint from assets/checkpoints/trump\n",
    "\n",
    "# generate text using sample_model function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Train the model again starting from the initial state for a few iterations and save a checkpoint, then load it and generate text using `sample_model` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load generated checkpoint\n",
    "\n",
    "# generate text using sample_model function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Visualization of memory cell activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "from utils import save_lstm_vis, make_colored_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Load checkpoint `trump_tb_20_i3880_l512_1.327.ckpt` from `assets/checkpoints/ssds/` and generate some sample text using function `sample_model`. The use utility function `make_colored_text` to color each character by cell activations in certain layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'assets/checkpoints/ssds/trump_tb_20_i3880_l512_1.327.ckpt'\n",
    "\n",
    "# sample model from the loaded checkpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Use utility funtion `make_colored_text` and Jupyter widget `HTML` to visualize cell activations for the text above. Here are some examples of interesting visualizations.\n",
    "\n",
    "`layer_id = 0`\n",
    "\n",
    "`cell_id`:\n",
    "- position in tweet - 4*\n",
    "- short urls - 10, 50*, 130, 160, 163, 164, 183, 218, 230\n",
    "- separate fixed and variable part of short url - 80, 152\n",
    "- just variable part of short url - 75, 84, 118, 273, 380\n",
    "- position in short url - 22*, 112, 206, 386\n",
    "- urls and references - 115, 403, 483\n",
    "\n",
    "`layer_id = 1`\n",
    "\n",
    "`cell_id`:\n",
    "- just variable part of short url - 21, 107, 250, 300, 420\n",
    "- beginning of a word - 22*, 112\n",
    "- urls and references - 51, 273, 438\n",
    "- position in short url - 202, 326\n",
    "- quotation marks - 252*\n",
    "- position in a sentence - 413"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position in a tweet\n",
    "HTML(make_colored_text(samp, states, cell_id=4, layer_id=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beggining of a word\n",
    "HTML(make_colored_text(samp, states, cell_id=22, layer_id=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code below to generate html file that contains colorings of the text above from all 512 cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_lstm_vis(\"assets/html/CA_trump_tb_20_i3880_l512_1.327\", samp, states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guessing game\n",
    "\n",
    "In this section you will play a short game of guessing whether the tweet you are shown is real or generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('assets/data/trump_tweets_ascii.txt') as f:\n",
    "    tweets_real = f.readlines()\n",
    "\n",
    "with open('assets/data/trump_tweets_fake.txt') as f:\n",
    "    tweets_fake = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 0\n",
    "N = 10\n",
    "for i in range(N):\n",
    "    tweet_label = True\n",
    "    if random.random() <= 0.5:\n",
    "        tweet_text = random.choice(tweets_real)\n",
    "    else:\n",
    "        tweet_text = random.choice(tweets_fake)\n",
    "        tweet_label = False\n",
    "    print(\"\\nTweet \" + str(i+1) + \"/\" + str(N) + \": \" + tweet_text)\n",
    "    answer = bool(int(input(\"true (1) or fake (0): \")))\n",
    "    if answer^tweet_label:\n",
    "        print(\"WRONG!\")\n",
    "    else:\n",
    "        print(\"RIGHT!\")\n",
    "        score = score + 1\n",
    "\n",
    "print(\"\\nYour score: \" + str(score) + \"/\" + str(N))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
